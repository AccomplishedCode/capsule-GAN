{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15880288978476585256\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1693306880\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 3910100126469382704\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 660, pci bus id: 0000:01:00.0, compute capability: 3.0\"\n",
      "]\n",
      "Default GPU Device: /device:GPU:0\n",
      "Modules imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print('Devices:', device_lib.list_local_devices())\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "print('Modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 28\n",
    "height = 28\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator structure\n",
    "def build_discriminator():\n",
    "\n",
    "        img_shape = (width, height, channels)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator structure\n",
    "def build_generator():\n",
    "\n",
    "        noise_shape = (100,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_shape=noise_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod((28, 28, 1)), activation='tanh'))\n",
    "        model.add(Reshape((28, 28, 1)))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build and compile the generator\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding noise to generator\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the combined model we will only train the generator\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to discriminate generated images\n",
    "valid = discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * batch_size)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, valid_y)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "        directory = \"images\"\n",
    "        \n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.618875, acc.: 67.97%] [G loss: 0.802113]\n",
      "1 [D loss: 0.639175, acc.: 64.06%] [G loss: 0.823674]\n",
      "2 [D loss: 0.629066, acc.: 66.41%] [G loss: 0.825397]\n",
      "3 [D loss: 0.651026, acc.: 56.25%] [G loss: 0.807527]\n",
      "4 [D loss: 0.641097, acc.: 60.16%] [G loss: 0.827198]\n",
      "5 [D loss: 0.631091, acc.: 63.28%] [G loss: 0.852882]\n",
      "6 [D loss: 0.609378, acc.: 71.88%] [G loss: 0.877632]\n",
      "7 [D loss: 0.625812, acc.: 69.53%] [G loss: 0.887825]\n",
      "8 [D loss: 0.619630, acc.: 71.88%] [G loss: 0.877363]\n",
      "9 [D loss: 0.624530, acc.: 67.97%] [G loss: 0.859427]\n",
      "10 [D loss: 0.623899, acc.: 68.75%] [G loss: 0.849812]\n",
      "11 [D loss: 0.626899, acc.: 66.41%] [G loss: 0.872907]\n",
      "12 [D loss: 0.621005, acc.: 69.53%] [G loss: 0.842928]\n",
      "13 [D loss: 0.613719, acc.: 69.53%] [G loss: 0.834394]\n",
      "14 [D loss: 0.606841, acc.: 72.66%] [G loss: 0.849864]\n",
      "15 [D loss: 0.603702, acc.: 68.75%] [G loss: 0.856017]\n",
      "16 [D loss: 0.593130, acc.: 74.22%] [G loss: 0.850864]\n",
      "17 [D loss: 0.607741, acc.: 66.41%] [G loss: 0.834288]\n",
      "18 [D loss: 0.611515, acc.: 66.41%] [G loss: 0.836764]\n",
      "19 [D loss: 0.604529, acc.: 75.78%] [G loss: 0.846361]\n",
      "20 [D loss: 0.621148, acc.: 66.41%] [G loss: 0.841688]\n",
      "21 [D loss: 0.616187, acc.: 71.88%] [G loss: 0.842744]\n",
      "22 [D loss: 0.590595, acc.: 72.66%] [G loss: 0.849717]\n",
      "23 [D loss: 0.589413, acc.: 73.44%] [G loss: 0.836898]\n",
      "24 [D loss: 0.617348, acc.: 71.88%] [G loss: 0.848801]\n",
      "25 [D loss: 0.586499, acc.: 72.66%] [G loss: 0.853223]\n",
      "26 [D loss: 0.607401, acc.: 67.19%] [G loss: 0.835528]\n",
      "27 [D loss: 0.597025, acc.: 71.09%] [G loss: 0.842001]\n",
      "28 [D loss: 0.626262, acc.: 64.84%] [G loss: 0.805438]\n",
      "29 [D loss: 0.596390, acc.: 67.97%] [G loss: 0.805326]\n",
      "30 [D loss: 0.614826, acc.: 65.62%] [G loss: 0.831729]\n",
      "31 [D loss: 0.634234, acc.: 62.50%] [G loss: 0.843189]\n",
      "32 [D loss: 0.595658, acc.: 71.09%] [G loss: 0.837743]\n",
      "33 [D loss: 0.623460, acc.: 71.88%] [G loss: 0.813191]\n",
      "34 [D loss: 0.634887, acc.: 64.06%] [G loss: 0.810988]\n",
      "35 [D loss: 0.643266, acc.: 63.28%] [G loss: 0.820042]\n",
      "36 [D loss: 0.624302, acc.: 69.53%] [G loss: 0.846684]\n",
      "37 [D loss: 0.617907, acc.: 65.62%] [G loss: 0.866327]\n",
      "38 [D loss: 0.618127, acc.: 71.88%] [G loss: 0.893652]\n",
      "39 [D loss: 0.616281, acc.: 70.31%] [G loss: 0.862536]\n",
      "40 [D loss: 0.611563, acc.: 75.78%] [G loss: 0.884487]\n",
      "41 [D loss: 0.614756, acc.: 71.09%] [G loss: 0.839455]\n",
      "42 [D loss: 0.597472, acc.: 71.88%] [G loss: 0.867743]\n",
      "43 [D loss: 0.592765, acc.: 71.09%] [G loss: 0.876080]\n",
      "44 [D loss: 0.613368, acc.: 70.31%] [G loss: 0.879485]\n",
      "45 [D loss: 0.592520, acc.: 75.00%] [G loss: 0.907566]\n",
      "46 [D loss: 0.601094, acc.: 73.44%] [G loss: 0.900377]\n",
      "47 [D loss: 0.638521, acc.: 59.38%] [G loss: 0.864332]\n",
      "48 [D loss: 0.617778, acc.: 64.84%] [G loss: 0.832576]\n",
      "49 [D loss: 0.615474, acc.: 71.88%] [G loss: 0.863926]\n",
      "50 [D loss: 0.629423, acc.: 67.97%] [G loss: 0.876414]\n",
      "51 [D loss: 0.624641, acc.: 69.53%] [G loss: 0.857359]\n",
      "52 [D loss: 0.624672, acc.: 68.75%] [G loss: 0.848729]\n",
      "53 [D loss: 0.617369, acc.: 65.62%] [G loss: 0.875839]\n",
      "54 [D loss: 0.625135, acc.: 63.28%] [G loss: 0.866485]\n",
      "55 [D loss: 0.619289, acc.: 68.75%] [G loss: 0.847002]\n",
      "56 [D loss: 0.610231, acc.: 67.97%] [G loss: 0.870439]\n",
      "57 [D loss: 0.581861, acc.: 75.78%] [G loss: 0.860954]\n",
      "58 [D loss: 0.615270, acc.: 66.41%] [G loss: 0.854406]\n",
      "59 [D loss: 0.629136, acc.: 65.62%] [G loss: 0.853544]\n",
      "60 [D loss: 0.613468, acc.: 69.53%] [G loss: 0.850238]\n",
      "61 [D loss: 0.621553, acc.: 71.09%] [G loss: 0.854066]\n",
      "62 [D loss: 0.619994, acc.: 72.66%] [G loss: 0.850841]\n",
      "63 [D loss: 0.614118, acc.: 68.75%] [G loss: 0.877665]\n",
      "64 [D loss: 0.597904, acc.: 77.34%] [G loss: 0.885369]\n",
      "65 [D loss: 0.608962, acc.: 74.22%] [G loss: 0.875274]\n",
      "66 [D loss: 0.631447, acc.: 64.06%] [G loss: 0.876632]\n",
      "67 [D loss: 0.607449, acc.: 69.53%] [G loss: 0.860927]\n",
      "68 [D loss: 0.628811, acc.: 63.28%] [G loss: 0.849873]\n",
      "69 [D loss: 0.603735, acc.: 75.78%] [G loss: 0.854002]\n",
      "70 [D loss: 0.595205, acc.: 71.09%] [G loss: 0.867744]\n",
      "71 [D loss: 0.634253, acc.: 66.41%] [G loss: 0.832210]\n",
      "72 [D loss: 0.610849, acc.: 69.53%] [G loss: 0.852261]\n",
      "73 [D loss: 0.604136, acc.: 71.88%] [G loss: 0.847080]\n",
      "74 [D loss: 0.610985, acc.: 71.09%] [G loss: 0.842988]\n",
      "75 [D loss: 0.637825, acc.: 69.53%] [G loss: 0.860382]\n",
      "76 [D loss: 0.635319, acc.: 63.28%] [G loss: 0.866792]\n",
      "77 [D loss: 0.624247, acc.: 68.75%] [G loss: 0.870561]\n",
      "78 [D loss: 0.639618, acc.: 70.31%] [G loss: 0.837838]\n",
      "79 [D loss: 0.627807, acc.: 67.19%] [G loss: 0.828838]\n",
      "80 [D loss: 0.639918, acc.: 59.38%] [G loss: 0.844227]\n",
      "81 [D loss: 0.600737, acc.: 70.31%] [G loss: 0.863016]\n",
      "82 [D loss: 0.612870, acc.: 67.97%] [G loss: 0.876635]\n",
      "83 [D loss: 0.653730, acc.: 64.84%] [G loss: 0.858188]\n",
      "84 [D loss: 0.639810, acc.: 60.94%] [G loss: 0.862431]\n",
      "85 [D loss: 0.615784, acc.: 73.44%] [G loss: 0.865285]\n",
      "86 [D loss: 0.652403, acc.: 59.38%] [G loss: 0.865316]\n",
      "87 [D loss: 0.612511, acc.: 68.75%] [G loss: 0.848714]\n",
      "88 [D loss: 0.595451, acc.: 75.00%] [G loss: 0.861461]\n",
      "89 [D loss: 0.611808, acc.: 72.66%] [G loss: 0.863921]\n",
      "90 [D loss: 0.600703, acc.: 75.78%] [G loss: 0.880721]\n",
      "91 [D loss: 0.597331, acc.: 75.78%] [G loss: 0.890197]\n",
      "92 [D loss: 0.581802, acc.: 81.25%] [G loss: 0.900862]\n",
      "93 [D loss: 0.595550, acc.: 76.56%] [G loss: 0.872430]\n",
      "94 [D loss: 0.606256, acc.: 71.09%] [G loss: 0.874906]\n",
      "95 [D loss: 0.616019, acc.: 75.00%] [G loss: 0.848498]\n",
      "96 [D loss: 0.614226, acc.: 65.62%] [G loss: 0.854753]\n",
      "97 [D loss: 0.568402, acc.: 76.56%] [G loss: 0.870041]\n",
      "98 [D loss: 0.614548, acc.: 71.88%] [G loss: 0.868688]\n",
      "99 [D loss: 0.588409, acc.: 74.22%] [G loss: 0.890643]\n",
      "100 [D loss: 0.626867, acc.: 65.62%] [G loss: 0.878988]\n",
      "101 [D loss: 0.578542, acc.: 74.22%] [G loss: 0.894091]\n",
      "102 [D loss: 0.599592, acc.: 73.44%] [G loss: 0.901684]\n",
      "103 [D loss: 0.596656, acc.: 73.44%] [G loss: 0.908173]\n",
      "104 [D loss: 0.618579, acc.: 64.84%] [G loss: 0.884763]\n",
      "105 [D loss: 0.549777, acc.: 80.47%] [G loss: 0.916106]\n",
      "106 [D loss: 0.595508, acc.: 78.91%] [G loss: 0.902538]\n",
      "107 [D loss: 0.572428, acc.: 73.44%] [G loss: 0.891287]\n",
      "108 [D loss: 0.562812, acc.: 80.47%] [G loss: 0.871452]\n",
      "109 [D loss: 0.606751, acc.: 65.62%] [G loss: 0.846945]\n",
      "110 [D loss: 0.619795, acc.: 61.72%] [G loss: 0.861900]\n",
      "111 [D loss: 0.592306, acc.: 72.66%] [G loss: 0.881236]\n",
      "112 [D loss: 0.607775, acc.: 68.75%] [G loss: 0.863485]\n",
      "113 [D loss: 0.604242, acc.: 67.19%] [G loss: 0.849919]\n",
      "114 [D loss: 0.601327, acc.: 73.44%] [G loss: 0.815024]\n",
      "115 [D loss: 0.596905, acc.: 71.09%] [G loss: 0.826294]\n",
      "116 [D loss: 0.571654, acc.: 75.78%] [G loss: 0.832221]\n",
      "117 [D loss: 0.598428, acc.: 75.00%] [G loss: 0.852336]\n",
      "118 [D loss: 0.619632, acc.: 70.31%] [G loss: 0.834751]\n",
      "119 [D loss: 0.581733, acc.: 71.88%] [G loss: 0.834080]\n",
      "120 [D loss: 0.607278, acc.: 70.31%] [G loss: 0.859141]\n",
      "121 [D loss: 0.590491, acc.: 73.44%] [G loss: 0.860042]\n",
      "122 [D loss: 0.584217, acc.: 75.78%] [G loss: 0.902737]\n",
      "123 [D loss: 0.587969, acc.: 75.00%] [G loss: 0.908189]\n",
      "124 [D loss: 0.613024, acc.: 67.19%] [G loss: 0.903276]\n",
      "125 [D loss: 0.592189, acc.: 71.09%] [G loss: 0.892294]\n",
      "126 [D loss: 0.607186, acc.: 75.78%] [G loss: 0.863572]\n",
      "127 [D loss: 0.640780, acc.: 62.50%] [G loss: 0.865884]\n",
      "128 [D loss: 0.611117, acc.: 67.19%] [G loss: 0.875471]\n",
      "129 [D loss: 0.586747, acc.: 72.66%] [G loss: 0.900911]\n",
      "130 [D loss: 0.621478, acc.: 71.09%] [G loss: 0.871103]\n",
      "131 [D loss: 0.601767, acc.: 69.53%] [G loss: 0.864088]\n",
      "132 [D loss: 0.629834, acc.: 64.84%] [G loss: 0.844213]\n",
      "133 [D loss: 0.608838, acc.: 60.94%] [G loss: 0.866284]\n",
      "134 [D loss: 0.614263, acc.: 64.06%] [G loss: 0.834964]\n",
      "135 [D loss: 0.635045, acc.: 65.62%] [G loss: 0.830720]\n",
      "136 [D loss: 0.643870, acc.: 67.19%] [G loss: 0.811439]\n",
      "137 [D loss: 0.605334, acc.: 71.88%] [G loss: 0.864532]\n",
      "138 [D loss: 0.626448, acc.: 73.44%] [G loss: 0.899292]\n",
      "139 [D loss: 0.631667, acc.: 64.06%] [G loss: 0.897637]\n",
      "140 [D loss: 0.611653, acc.: 64.84%] [G loss: 0.853263]\n",
      "141 [D loss: 0.618289, acc.: 69.53%] [G loss: 0.819365]\n",
      "142 [D loss: 0.648458, acc.: 59.38%] [G loss: 0.828672]\n",
      "143 [D loss: 0.611420, acc.: 69.53%] [G loss: 0.866884]\n",
      "144 [D loss: 0.626071, acc.: 67.19%] [G loss: 0.904815]\n",
      "145 [D loss: 0.615583, acc.: 72.66%] [G loss: 0.891882]\n",
      "146 [D loss: 0.614485, acc.: 71.09%] [G loss: 0.901112]\n",
      "147 [D loss: 0.629468, acc.: 67.97%] [G loss: 0.901872]\n",
      "148 [D loss: 0.635700, acc.: 65.62%] [G loss: 0.929442]\n",
      "149 [D loss: 0.625669, acc.: 72.66%] [G loss: 0.890302]\n",
      "150 [D loss: 0.591460, acc.: 77.34%] [G loss: 0.882348]\n",
      "151 [D loss: 0.617826, acc.: 66.41%] [G loss: 0.877995]\n",
      "152 [D loss: 0.599712, acc.: 75.78%] [G loss: 0.858464]\n",
      "153 [D loss: 0.597568, acc.: 73.44%] [G loss: 0.866369]\n",
      "154 [D loss: 0.619940, acc.: 67.19%] [G loss: 0.877142]\n",
      "155 [D loss: 0.600336, acc.: 72.66%] [G loss: 0.890748]\n",
      "156 [D loss: 0.609203, acc.: 71.88%] [G loss: 0.896076]\n",
      "157 [D loss: 0.595253, acc.: 79.69%] [G loss: 0.891997]\n",
      "158 [D loss: 0.574743, acc.: 79.69%] [G loss: 0.899365]\n",
      "159 [D loss: 0.619664, acc.: 69.53%] [G loss: 0.855594]\n",
      "160 [D loss: 0.604511, acc.: 72.66%] [G loss: 0.877109]\n",
      "161 [D loss: 0.606676, acc.: 76.56%] [G loss: 0.854476]\n",
      "162 [D loss: 0.615005, acc.: 68.75%] [G loss: 0.831932]\n",
      "163 [D loss: 0.558898, acc.: 80.47%] [G loss: 0.839083]\n",
      "164 [D loss: 0.611040, acc.: 69.53%] [G loss: 0.849945]\n",
      "165 [D loss: 0.584013, acc.: 75.78%] [G loss: 0.862616]\n",
      "166 [D loss: 0.591853, acc.: 71.09%] [G loss: 0.892242]\n",
      "167 [D loss: 0.610773, acc.: 64.06%] [G loss: 0.904836]\n",
      "168 [D loss: 0.642825, acc.: 60.94%] [G loss: 0.884149]\n",
      "169 [D loss: 0.592936, acc.: 74.22%] [G loss: 0.900531]\n",
      "170 [D loss: 0.646082, acc.: 64.84%] [G loss: 0.875556]\n",
      "171 [D loss: 0.595313, acc.: 66.41%] [G loss: 0.932906]\n",
      "172 [D loss: 0.592110, acc.: 78.12%] [G loss: 0.919146]\n",
      "173 [D loss: 0.640931, acc.: 63.28%] [G loss: 0.851365]\n",
      "174 [D loss: 0.626040, acc.: 67.97%] [G loss: 0.849467]\n",
      "175 [D loss: 0.616219, acc.: 71.09%] [G loss: 0.871233]\n",
      "176 [D loss: 0.640419, acc.: 60.94%] [G loss: 0.882585]\n",
      "177 [D loss: 0.612841, acc.: 72.66%] [G loss: 0.881324]\n",
      "178 [D loss: 0.602241, acc.: 75.00%] [G loss: 0.880195]\n",
      "179 [D loss: 0.602793, acc.: 67.19%] [G loss: 0.868405]\n",
      "180 [D loss: 0.624888, acc.: 65.62%] [G loss: 0.894039]\n",
      "181 [D loss: 0.594474, acc.: 69.53%] [G loss: 0.901760]\n",
      "182 [D loss: 0.594141, acc.: 72.66%] [G loss: 0.869250]\n",
      "183 [D loss: 0.632663, acc.: 62.50%] [G loss: 0.839729]\n",
      "184 [D loss: 0.621435, acc.: 70.31%] [G loss: 0.844037]\n",
      "185 [D loss: 0.618547, acc.: 72.66%] [G loss: 0.844987]\n",
      "186 [D loss: 0.622709, acc.: 67.19%] [G loss: 0.883260]\n",
      "187 [D loss: 0.636976, acc.: 63.28%] [G loss: 0.866312]\n",
      "188 [D loss: 0.566988, acc.: 75.00%] [G loss: 0.912275]\n",
      "189 [D loss: 0.611864, acc.: 69.53%] [G loss: 0.881960]\n",
      "190 [D loss: 0.601147, acc.: 74.22%] [G loss: 0.865712]\n",
      "191 [D loss: 0.593529, acc.: 71.88%] [G loss: 0.869185]\n",
      "192 [D loss: 0.627184, acc.: 62.50%] [G loss: 0.880378]\n",
      "193 [D loss: 0.604558, acc.: 71.88%] [G loss: 0.872700]\n",
      "194 [D loss: 0.603864, acc.: 71.09%] [G loss: 0.879402]\n",
      "195 [D loss: 0.617963, acc.: 72.66%] [G loss: 0.898652]\n",
      "196 [D loss: 0.580136, acc.: 78.12%] [G loss: 0.896232]\n",
      "197 [D loss: 0.607287, acc.: 75.00%] [G loss: 0.876815]\n",
      "198 [D loss: 0.592007, acc.: 78.12%] [G loss: 0.866674]\n",
      "199 [D loss: 0.588454, acc.: 73.44%] [G loss: 0.865260]\n",
      "200 [D loss: 0.608410, acc.: 68.75%] [G loss: 0.889601]\n",
      "201 [D loss: 0.627363, acc.: 65.62%] [G loss: 0.913186]\n",
      "202 [D loss: 0.599130, acc.: 78.12%] [G loss: 0.938870]\n",
      "203 [D loss: 0.655861, acc.: 64.06%] [G loss: 0.849054]\n",
      "204 [D loss: 0.604228, acc.: 75.78%] [G loss: 0.843076]\n",
      "205 [D loss: 0.610406, acc.: 66.41%] [G loss: 0.883520]\n",
      "206 [D loss: 0.603104, acc.: 67.97%] [G loss: 0.853278]\n",
      "207 [D loss: 0.603504, acc.: 69.53%] [G loss: 0.866118]\n",
      "208 [D loss: 0.617042, acc.: 69.53%] [G loss: 0.857423]\n",
      "209 [D loss: 0.610969, acc.: 70.31%] [G loss: 0.864964]\n",
      "210 [D loss: 0.625299, acc.: 64.84%] [G loss: 0.869886]\n",
      "211 [D loss: 0.614981, acc.: 71.88%] [G loss: 0.856444]\n",
      "212 [D loss: 0.600937, acc.: 75.78%] [G loss: 0.868969]\n",
      "213 [D loss: 0.614775, acc.: 73.44%] [G loss: 0.882474]\n",
      "214 [D loss: 0.609512, acc.: 64.84%] [G loss: 0.899744]\n",
      "215 [D loss: 0.615605, acc.: 74.22%] [G loss: 0.905730]\n",
      "216 [D loss: 0.598858, acc.: 79.69%] [G loss: 0.861298]\n",
      "217 [D loss: 0.592394, acc.: 75.78%] [G loss: 0.875915]\n",
      "218 [D loss: 0.608644, acc.: 74.22%] [G loss: 0.888120]\n",
      "219 [D loss: 0.617369, acc.: 69.53%] [G loss: 0.887635]\n",
      "220 [D loss: 0.629901, acc.: 71.09%] [G loss: 0.862361]\n",
      "221 [D loss: 0.615797, acc.: 68.75%] [G loss: 0.833947]\n",
      "222 [D loss: 0.612125, acc.: 70.31%] [G loss: 0.846468]\n",
      "223 [D loss: 0.576378, acc.: 79.69%] [G loss: 0.851705]\n",
      "224 [D loss: 0.625374, acc.: 68.75%] [G loss: 0.868760]\n",
      "225 [D loss: 0.589381, acc.: 76.56%] [G loss: 0.878976]\n",
      "226 [D loss: 0.593023, acc.: 78.12%] [G loss: 0.859306]\n",
      "227 [D loss: 0.598520, acc.: 73.44%] [G loss: 0.877901]\n",
      "228 [D loss: 0.600775, acc.: 69.53%] [G loss: 0.900192]\n",
      "229 [D loss: 0.578762, acc.: 76.56%] [G loss: 0.899837]\n",
      "230 [D loss: 0.605316, acc.: 72.66%] [G loss: 0.876516]\n",
      "231 [D loss: 0.600128, acc.: 75.00%] [G loss: 0.868723]\n",
      "232 [D loss: 0.588035, acc.: 75.78%] [G loss: 0.888103]\n",
      "233 [D loss: 0.581523, acc.: 76.56%] [G loss: 0.893753]\n",
      "234 [D loss: 0.574694, acc.: 78.12%] [G loss: 0.911031]\n",
      "235 [D loss: 0.563260, acc.: 78.12%] [G loss: 0.914490]\n",
      "236 [D loss: 0.576971, acc.: 85.16%] [G loss: 0.903064]\n",
      "237 [D loss: 0.586713, acc.: 73.44%] [G loss: 0.883681]\n",
      "238 [D loss: 0.584630, acc.: 74.22%] [G loss: 0.869730]\n",
      "239 [D loss: 0.599500, acc.: 72.66%] [G loss: 0.876293]\n",
      "240 [D loss: 0.567776, acc.: 75.78%] [G loss: 0.867044]\n",
      "241 [D loss: 0.553822, acc.: 81.25%] [G loss: 0.890967]\n",
      "242 [D loss: 0.556670, acc.: 79.69%] [G loss: 0.902922]\n",
      "243 [D loss: 0.564658, acc.: 74.22%] [G loss: 0.924400]\n",
      "244 [D loss: 0.575066, acc.: 80.47%] [G loss: 0.936938]\n",
      "245 [D loss: 0.574255, acc.: 73.44%] [G loss: 0.931315]\n",
      "246 [D loss: 0.582023, acc.: 73.44%] [G loss: 0.921652]\n",
      "247 [D loss: 0.622078, acc.: 64.84%] [G loss: 0.914165]\n",
      "248 [D loss: 0.572163, acc.: 78.12%] [G loss: 0.889642]\n",
      "249 [D loss: 0.604493, acc.: 70.31%] [G loss: 0.876707]\n",
      "250 [D loss: 0.575604, acc.: 76.56%] [G loss: 0.873832]\n",
      "251 [D loss: 0.595102, acc.: 71.09%] [G loss: 0.885452]\n",
      "252 [D loss: 0.627563, acc.: 66.41%] [G loss: 0.896825]\n",
      "253 [D loss: 0.569650, acc.: 78.91%] [G loss: 0.898326]\n",
      "254 [D loss: 0.578416, acc.: 77.34%] [G loss: 0.913525]\n",
      "255 [D loss: 0.585300, acc.: 74.22%] [G loss: 0.939035]\n",
      "256 [D loss: 0.569169, acc.: 74.22%] [G loss: 0.907387]\n",
      "257 [D loss: 0.588230, acc.: 74.22%] [G loss: 0.885984]\n",
      "258 [D loss: 0.603418, acc.: 73.44%] [G loss: 0.900217]\n",
      "259 [D loss: 0.580830, acc.: 75.00%] [G loss: 0.930827]\n",
      "260 [D loss: 0.615400, acc.: 64.06%] [G loss: 0.912432]\n",
      "261 [D loss: 0.615221, acc.: 71.09%] [G loss: 0.862041]\n",
      "262 [D loss: 0.615444, acc.: 67.97%] [G loss: 0.858787]\n",
      "263 [D loss: 0.606541, acc.: 70.31%] [G loss: 0.844969]\n",
      "264 [D loss: 0.574478, acc.: 73.44%] [G loss: 0.876381]\n",
      "265 [D loss: 0.631982, acc.: 65.62%] [G loss: 0.914797]\n",
      "266 [D loss: 0.627436, acc.: 64.06%] [G loss: 0.941017]\n",
      "267 [D loss: 0.599779, acc.: 69.53%] [G loss: 0.936019]\n",
      "268 [D loss: 0.603043, acc.: 74.22%] [G loss: 0.943049]\n",
      "269 [D loss: 0.613783, acc.: 67.97%] [G loss: 0.924681]\n",
      "270 [D loss: 0.623895, acc.: 67.19%] [G loss: 0.865321]\n",
      "271 [D loss: 0.646683, acc.: 59.38%] [G loss: 0.871586]\n",
      "272 [D loss: 0.632174, acc.: 61.72%] [G loss: 0.901970]\n",
      "273 [D loss: 0.616004, acc.: 64.84%] [G loss: 0.901565]\n",
      "274 [D loss: 0.618611, acc.: 65.62%] [G loss: 0.903525]\n",
      "275 [D loss: 0.632881, acc.: 63.28%] [G loss: 0.935294]\n",
      "276 [D loss: 0.603419, acc.: 74.22%] [G loss: 0.903178]\n",
      "277 [D loss: 0.585068, acc.: 72.66%] [G loss: 0.913160]\n",
      "278 [D loss: 0.589568, acc.: 74.22%] [G loss: 0.920449]\n",
      "279 [D loss: 0.574888, acc.: 78.91%] [G loss: 0.955400]\n",
      "280 [D loss: 0.590469, acc.: 81.25%] [G loss: 0.950408]\n",
      "281 [D loss: 0.573346, acc.: 79.69%] [G loss: 0.939994]\n",
      "282 [D loss: 0.595817, acc.: 80.47%] [G loss: 0.939462]\n",
      "283 [D loss: 0.554724, acc.: 87.50%] [G loss: 0.950271]\n",
      "284 [D loss: 0.595876, acc.: 71.09%] [G loss: 0.933758]\n",
      "285 [D loss: 0.593762, acc.: 74.22%] [G loss: 0.914405]\n",
      "286 [D loss: 0.572705, acc.: 69.53%] [G loss: 0.966078]\n",
      "287 [D loss: 0.553741, acc.: 78.91%] [G loss: 0.966476]\n",
      "288 [D loss: 0.595033, acc.: 70.31%] [G loss: 0.947445]\n",
      "289 [D loss: 0.628341, acc.: 68.75%] [G loss: 0.911125]\n",
      "290 [D loss: 0.593540, acc.: 77.34%] [G loss: 0.867347]\n",
      "291 [D loss: 0.580679, acc.: 69.53%] [G loss: 0.895488]\n",
      "292 [D loss: 0.592107, acc.: 73.44%] [G loss: 0.905656]\n",
      "293 [D loss: 0.565432, acc.: 75.78%] [G loss: 0.936507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 [D loss: 0.565179, acc.: 76.56%] [G loss: 0.959823]\n",
      "295 [D loss: 0.593797, acc.: 74.22%] [G loss: 0.935532]\n",
      "296 [D loss: 0.574642, acc.: 72.66%] [G loss: 0.961534]\n",
      "297 [D loss: 0.559794, acc.: 77.34%] [G loss: 0.986102]\n",
      "298 [D loss: 0.597754, acc.: 75.00%] [G loss: 0.932431]\n",
      "299 [D loss: 0.580030, acc.: 70.31%] [G loss: 0.924178]\n",
      "300 [D loss: 0.582159, acc.: 71.88%] [G loss: 0.910555]\n",
      "301 [D loss: 0.585752, acc.: 73.44%] [G loss: 0.928123]\n",
      "302 [D loss: 0.610191, acc.: 67.19%] [G loss: 0.917465]\n",
      "303 [D loss: 0.585693, acc.: 69.53%] [G loss: 0.908503]\n",
      "304 [D loss: 0.595971, acc.: 65.62%] [G loss: 0.893332]\n",
      "305 [D loss: 0.574341, acc.: 71.88%] [G loss: 0.901649]\n",
      "306 [D loss: 0.620162, acc.: 67.19%] [G loss: 0.949387]\n",
      "307 [D loss: 0.586688, acc.: 78.12%] [G loss: 0.902039]\n",
      "308 [D loss: 0.557538, acc.: 73.44%] [G loss: 0.911080]\n",
      "309 [D loss: 0.572605, acc.: 75.00%] [G loss: 0.961598]\n",
      "310 [D loss: 0.598426, acc.: 65.62%] [G loss: 0.911406]\n",
      "311 [D loss: 0.579988, acc.: 70.31%] [G loss: 0.905625]\n",
      "312 [D loss: 0.593971, acc.: 67.97%] [G loss: 0.917006]\n",
      "313 [D loss: 0.588419, acc.: 73.44%] [G loss: 0.955392]\n",
      "314 [D loss: 0.586889, acc.: 72.66%] [G loss: 0.933249]\n",
      "315 [D loss: 0.604472, acc.: 68.75%] [G loss: 0.911375]\n",
      "316 [D loss: 0.641730, acc.: 60.94%] [G loss: 0.878765]\n",
      "317 [D loss: 0.632942, acc.: 62.50%] [G loss: 0.923707]\n",
      "318 [D loss: 0.595479, acc.: 72.66%] [G loss: 0.937508]\n",
      "319 [D loss: 0.608576, acc.: 67.97%] [G loss: 0.921708]\n",
      "320 [D loss: 0.617383, acc.: 64.84%] [G loss: 0.943258]\n",
      "321 [D loss: 0.622752, acc.: 66.41%] [G loss: 0.938096]\n",
      "322 [D loss: 0.599194, acc.: 69.53%] [G loss: 0.917060]\n",
      "323 [D loss: 0.613882, acc.: 69.53%] [G loss: 0.928925]\n",
      "324 [D loss: 0.601496, acc.: 67.19%] [G loss: 0.886392]\n",
      "325 [D loss: 0.635178, acc.: 57.03%] [G loss: 0.923456]\n",
      "326 [D loss: 0.606237, acc.: 71.09%] [G loss: 0.953607]\n",
      "327 [D loss: 0.638874, acc.: 67.97%] [G loss: 0.958633]\n",
      "328 [D loss: 0.590343, acc.: 75.78%] [G loss: 0.975153]\n",
      "329 [D loss: 0.606636, acc.: 70.31%] [G loss: 0.982773]\n",
      "330 [D loss: 0.603745, acc.: 66.41%] [G loss: 0.981170]\n",
      "331 [D loss: 0.601369, acc.: 68.75%] [G loss: 0.972915]\n",
      "332 [D loss: 0.631910, acc.: 64.84%] [G loss: 0.953653]\n",
      "333 [D loss: 0.605764, acc.: 73.44%] [G loss: 0.938604]\n",
      "334 [D loss: 0.593612, acc.: 69.53%] [G loss: 0.953791]\n",
      "335 [D loss: 0.621562, acc.: 67.19%] [G loss: 0.895545]\n",
      "336 [D loss: 0.589538, acc.: 72.66%] [G loss: 0.900567]\n",
      "337 [D loss: 0.617221, acc.: 67.19%] [G loss: 0.921564]\n",
      "338 [D loss: 0.618922, acc.: 67.19%] [G loss: 0.922988]\n",
      "339 [D loss: 0.586664, acc.: 75.78%] [G loss: 0.911589]\n",
      "340 [D loss: 0.596657, acc.: 71.88%] [G loss: 0.964272]\n",
      "341 [D loss: 0.612373, acc.: 72.66%] [G loss: 0.943617]\n",
      "342 [D loss: 0.619408, acc.: 68.75%] [G loss: 0.938194]\n",
      "343 [D loss: 0.595368, acc.: 75.78%] [G loss: 0.935047]\n",
      "344 [D loss: 0.594404, acc.: 78.91%] [G loss: 0.939446]\n",
      "345 [D loss: 0.585300, acc.: 73.44%] [G loss: 0.929546]\n",
      "346 [D loss: 0.595257, acc.: 73.44%] [G loss: 0.940681]\n",
      "347 [D loss: 0.611374, acc.: 71.88%] [G loss: 0.934996]\n",
      "348 [D loss: 0.581579, acc.: 79.69%] [G loss: 0.937948]\n",
      "349 [D loss: 0.558541, acc.: 80.47%] [G loss: 0.962624]\n",
      "350 [D loss: 0.596716, acc.: 74.22%] [G loss: 0.942746]\n",
      "351 [D loss: 0.603987, acc.: 67.19%] [G loss: 0.984648]\n",
      "352 [D loss: 0.595029, acc.: 71.09%] [G loss: 1.013135]\n",
      "353 [D loss: 0.625230, acc.: 68.75%] [G loss: 0.962925]\n",
      "354 [D loss: 0.626512, acc.: 64.06%] [G loss: 0.908935]\n",
      "355 [D loss: 0.567456, acc.: 80.47%] [G loss: 0.913946]\n",
      "356 [D loss: 0.600653, acc.: 71.09%] [G loss: 0.935228]\n",
      "357 [D loss: 0.571947, acc.: 76.56%] [G loss: 0.901525]\n",
      "358 [D loss: 0.596212, acc.: 67.19%] [G loss: 0.914955]\n",
      "359 [D loss: 0.573842, acc.: 75.00%] [G loss: 0.942789]\n",
      "360 [D loss: 0.567433, acc.: 80.47%] [G loss: 0.968812]\n",
      "361 [D loss: 0.563261, acc.: 81.25%] [G loss: 0.958326]\n",
      "362 [D loss: 0.598341, acc.: 76.56%] [G loss: 0.908824]\n",
      "363 [D loss: 0.601780, acc.: 65.62%] [G loss: 0.932282]\n",
      "364 [D loss: 0.585682, acc.: 76.56%] [G loss: 0.962264]\n",
      "365 [D loss: 0.619328, acc.: 67.97%] [G loss: 0.932377]\n",
      "366 [D loss: 0.584917, acc.: 79.69%] [G loss: 0.942933]\n",
      "367 [D loss: 0.598996, acc.: 66.41%] [G loss: 0.947725]\n",
      "368 [D loss: 0.605835, acc.: 71.09%] [G loss: 0.964602]\n",
      "369 [D loss: 0.546237, acc.: 82.81%] [G loss: 0.965725]\n",
      "370 [D loss: 0.587072, acc.: 70.31%] [G loss: 0.952926]\n",
      "371 [D loss: 0.578894, acc.: 69.53%] [G loss: 0.964426]\n",
      "372 [D loss: 0.581314, acc.: 69.53%] [G loss: 0.953443]\n",
      "373 [D loss: 0.566350, acc.: 78.12%] [G loss: 0.962491]\n",
      "374 [D loss: 0.548581, acc.: 80.47%] [G loss: 0.971566]\n",
      "375 [D loss: 0.564603, acc.: 77.34%] [G loss: 0.924322]\n",
      "376 [D loss: 0.583022, acc.: 64.84%] [G loss: 0.932992]\n",
      "377 [D loss: 0.585148, acc.: 71.09%] [G loss: 0.945329]\n",
      "378 [D loss: 0.617767, acc.: 64.06%] [G loss: 0.956728]\n",
      "379 [D loss: 0.537090, acc.: 81.25%] [G loss: 1.035871]\n",
      "380 [D loss: 0.600992, acc.: 72.66%] [G loss: 0.979764]\n",
      "381 [D loss: 0.568029, acc.: 76.56%] [G loss: 0.971364]\n",
      "382 [D loss: 0.574965, acc.: 79.69%] [G loss: 1.008222]\n",
      "383 [D loss: 0.567342, acc.: 71.88%] [G loss: 0.950936]\n",
      "384 [D loss: 0.579342, acc.: 72.66%] [G loss: 0.947196]\n",
      "385 [D loss: 0.574974, acc.: 77.34%] [G loss: 0.939285]\n",
      "386 [D loss: 0.590601, acc.: 70.31%] [G loss: 0.911857]\n",
      "387 [D loss: 0.578391, acc.: 74.22%] [G loss: 0.954822]\n",
      "388 [D loss: 0.607317, acc.: 67.19%] [G loss: 0.963496]\n",
      "389 [D loss: 0.620428, acc.: 66.41%] [G loss: 0.947196]\n",
      "390 [D loss: 0.560617, acc.: 78.12%] [G loss: 0.983168]\n",
      "391 [D loss: 0.560264, acc.: 71.88%] [G loss: 0.991485]\n",
      "392 [D loss: 0.587763, acc.: 76.56%] [G loss: 0.969448]\n",
      "393 [D loss: 0.591192, acc.: 76.56%] [G loss: 0.948175]\n",
      "394 [D loss: 0.578838, acc.: 72.66%] [G loss: 0.927511]\n",
      "395 [D loss: 0.589868, acc.: 77.34%] [G loss: 0.924745]\n",
      "396 [D loss: 0.551828, acc.: 81.25%] [G loss: 0.953517]\n",
      "397 [D loss: 0.542963, acc.: 82.81%] [G loss: 0.980699]\n",
      "398 [D loss: 0.577864, acc.: 78.91%] [G loss: 0.951779]\n",
      "399 [D loss: 0.591213, acc.: 71.09%] [G loss: 0.920306]\n",
      "400 [D loss: 0.599386, acc.: 74.22%] [G loss: 0.916906]\n",
      "401 [D loss: 0.554380, acc.: 77.34%] [G loss: 0.987012]\n",
      "402 [D loss: 0.575387, acc.: 76.56%] [G loss: 1.000649]\n",
      "403 [D loss: 0.596194, acc.: 75.78%] [G loss: 1.007822]\n",
      "404 [D loss: 0.584697, acc.: 76.56%] [G loss: 1.006680]\n",
      "405 [D loss: 0.547254, acc.: 80.47%] [G loss: 0.976700]\n",
      "406 [D loss: 0.583979, acc.: 79.69%] [G loss: 0.937666]\n",
      "407 [D loss: 0.601046, acc.: 70.31%] [G loss: 0.930943]\n",
      "408 [D loss: 0.562982, acc.: 76.56%] [G loss: 0.912694]\n",
      "409 [D loss: 0.595414, acc.: 69.53%] [G loss: 0.911421]\n",
      "410 [D loss: 0.576052, acc.: 81.25%] [G loss: 0.908042]\n",
      "411 [D loss: 0.583501, acc.: 72.66%] [G loss: 0.929914]\n",
      "412 [D loss: 0.577399, acc.: 77.34%] [G loss: 0.938122]\n",
      "413 [D loss: 0.562098, acc.: 81.25%] [G loss: 0.933473]\n",
      "414 [D loss: 0.603520, acc.: 69.53%] [G loss: 0.928399]\n",
      "415 [D loss: 0.557801, acc.: 81.25%] [G loss: 0.949340]\n",
      "416 [D loss: 0.555658, acc.: 79.69%] [G loss: 0.988287]\n",
      "417 [D loss: 0.567315, acc.: 77.34%] [G loss: 0.986832]\n",
      "418 [D loss: 0.571953, acc.: 77.34%] [G loss: 0.919768]\n",
      "419 [D loss: 0.576190, acc.: 74.22%] [G loss: 0.940089]\n",
      "420 [D loss: 0.612416, acc.: 68.75%] [G loss: 0.962546]\n",
      "421 [D loss: 0.579686, acc.: 78.12%] [G loss: 0.951889]\n",
      "422 [D loss: 0.560354, acc.: 82.81%] [G loss: 0.947596]\n",
      "423 [D loss: 0.588392, acc.: 75.00%] [G loss: 0.936692]\n",
      "424 [D loss: 0.566316, acc.: 71.09%] [G loss: 0.951663]\n",
      "425 [D loss: 0.540450, acc.: 84.38%] [G loss: 0.989132]\n",
      "426 [D loss: 0.556861, acc.: 78.12%] [G loss: 0.963811]\n",
      "427 [D loss: 0.600547, acc.: 69.53%] [G loss: 0.925991]\n",
      "428 [D loss: 0.586910, acc.: 78.12%] [G loss: 0.947201]\n",
      "429 [D loss: 0.591599, acc.: 70.31%] [G loss: 0.932260]\n",
      "430 [D loss: 0.606226, acc.: 64.84%] [G loss: 0.964707]\n",
      "431 [D loss: 0.604144, acc.: 67.19%] [G loss: 0.941015]\n",
      "432 [D loss: 0.598255, acc.: 73.44%] [G loss: 0.967483]\n",
      "433 [D loss: 0.599310, acc.: 78.91%] [G loss: 0.941034]\n",
      "434 [D loss: 0.625499, acc.: 65.62%] [G loss: 0.930908]\n",
      "435 [D loss: 0.572951, acc.: 78.91%] [G loss: 0.938854]\n",
      "436 [D loss: 0.576567, acc.: 75.78%] [G loss: 0.926270]\n",
      "437 [D loss: 0.568734, acc.: 79.69%] [G loss: 0.937777]\n",
      "438 [D loss: 0.584753, acc.: 71.09%] [G loss: 0.952459]\n",
      "439 [D loss: 0.684741, acc.: 56.25%] [G loss: 0.974301]\n",
      "440 [D loss: 0.598285, acc.: 70.31%] [G loss: 0.998729]\n",
      "441 [D loss: 0.663104, acc.: 57.03%] [G loss: 0.921013]\n",
      "442 [D loss: 0.655653, acc.: 53.91%] [G loss: 0.895391]\n",
      "443 [D loss: 0.628836, acc.: 63.28%] [G loss: 0.874727]\n",
      "444 [D loss: 0.628933, acc.: 64.84%] [G loss: 0.898046]\n",
      "445 [D loss: 0.579591, acc.: 75.78%] [G loss: 0.946117]\n",
      "446 [D loss: 0.588432, acc.: 78.12%] [G loss: 0.963476]\n",
      "447 [D loss: 0.615228, acc.: 69.53%] [G loss: 0.940722]\n",
      "448 [D loss: 0.602011, acc.: 69.53%] [G loss: 0.952998]\n",
      "449 [D loss: 0.620363, acc.: 70.31%] [G loss: 0.925395]\n",
      "450 [D loss: 0.568741, acc.: 72.66%] [G loss: 0.985520]\n",
      "451 [D loss: 0.611898, acc.: 66.41%] [G loss: 0.997446]\n",
      "452 [D loss: 0.591850, acc.: 75.00%] [G loss: 0.983132]\n",
      "453 [D loss: 0.567469, acc.: 78.91%] [G loss: 1.021371]\n",
      "454 [D loss: 0.612047, acc.: 73.44%] [G loss: 0.994003]\n",
      "455 [D loss: 0.549033, acc.: 82.03%] [G loss: 0.981406]\n",
      "456 [D loss: 0.566765, acc.: 82.03%] [G loss: 0.970288]\n",
      "457 [D loss: 0.594070, acc.: 74.22%] [G loss: 0.931952]\n",
      "458 [D loss: 0.551185, acc.: 83.59%] [G loss: 0.948945]\n",
      "459 [D loss: 0.587524, acc.: 71.09%] [G loss: 0.940386]\n",
      "460 [D loss: 0.607919, acc.: 73.44%] [G loss: 0.923251]\n",
      "461 [D loss: 0.579525, acc.: 75.78%] [G loss: 0.969028]\n",
      "462 [D loss: 0.559644, acc.: 71.09%] [G loss: 1.006138]\n",
      "463 [D loss: 0.594292, acc.: 66.41%] [G loss: 1.003938]\n",
      "464 [D loss: 0.628058, acc.: 64.84%] [G loss: 0.943367]\n",
      "465 [D loss: 0.575099, acc.: 72.66%] [G loss: 0.928104]\n",
      "466 [D loss: 0.561549, acc.: 76.56%] [G loss: 0.957318]\n",
      "467 [D loss: 0.563206, acc.: 78.91%] [G loss: 0.974922]\n",
      "468 [D loss: 0.601005, acc.: 68.75%] [G loss: 0.951451]\n",
      "469 [D loss: 0.588637, acc.: 71.09%] [G loss: 0.965838]\n",
      "470 [D loss: 0.554239, acc.: 72.66%] [G loss: 1.009779]\n",
      "471 [D loss: 0.573428, acc.: 73.44%] [G loss: 0.969127]\n",
      "472 [D loss: 0.567289, acc.: 78.12%] [G loss: 0.970474]\n",
      "473 [D loss: 0.619438, acc.: 64.84%] [G loss: 0.937966]\n",
      "474 [D loss: 0.566872, acc.: 76.56%] [G loss: 0.934535]\n",
      "475 [D loss: 0.580304, acc.: 71.88%] [G loss: 0.950349]\n",
      "476 [D loss: 0.587002, acc.: 72.66%] [G loss: 0.944497]\n",
      "477 [D loss: 0.588368, acc.: 69.53%] [G loss: 0.958357]\n",
      "478 [D loss: 0.592693, acc.: 73.44%] [G loss: 1.007400]\n",
      "479 [D loss: 0.597772, acc.: 72.66%] [G loss: 1.003840]\n",
      "480 [D loss: 0.587157, acc.: 67.97%] [G loss: 0.961657]\n",
      "481 [D loss: 0.588653, acc.: 67.97%] [G loss: 0.939010]\n",
      "482 [D loss: 0.609220, acc.: 65.62%] [G loss: 0.994220]\n",
      "483 [D loss: 0.594939, acc.: 71.88%] [G loss: 1.021422]\n",
      "484 [D loss: 0.610191, acc.: 68.75%] [G loss: 0.994595]\n",
      "485 [D loss: 0.610891, acc.: 71.09%] [G loss: 0.979100]\n",
      "486 [D loss: 0.604674, acc.: 65.62%] [G loss: 0.973926]\n",
      "487 [D loss: 0.548301, acc.: 76.56%] [G loss: 1.014683]\n",
      "488 [D loss: 0.580867, acc.: 72.66%] [G loss: 1.022920]\n",
      "489 [D loss: 0.563758, acc.: 71.88%] [G loss: 1.040475]\n",
      "490 [D loss: 0.570030, acc.: 75.00%] [G loss: 1.046280]\n",
      "491 [D loss: 0.572901, acc.: 75.78%] [G loss: 1.025576]\n",
      "492 [D loss: 0.563539, acc.: 76.56%] [G loss: 1.054153]\n",
      "493 [D loss: 0.585331, acc.: 68.75%] [G loss: 0.992076]\n",
      "494 [D loss: 0.606117, acc.: 67.19%] [G loss: 0.998942]\n",
      "495 [D loss: 0.604726, acc.: 68.75%] [G loss: 1.014364]\n",
      "496 [D loss: 0.557665, acc.: 79.69%] [G loss: 0.945232]\n",
      "497 [D loss: 0.580763, acc.: 78.91%] [G loss: 0.953439]\n",
      "498 [D loss: 0.588527, acc.: 71.09%] [G loss: 0.922513]\n",
      "499 [D loss: 0.576884, acc.: 74.22%] [G loss: 0.928978]\n",
      "500 [D loss: 0.561543, acc.: 76.56%] [G loss: 0.953113]\n",
      "501 [D loss: 0.532255, acc.: 81.25%] [G loss: 0.996925]\n",
      "502 [D loss: 0.571314, acc.: 71.88%] [G loss: 1.004765]\n",
      "503 [D loss: 0.548460, acc.: 75.00%] [G loss: 1.001831]\n",
      "504 [D loss: 0.577914, acc.: 75.00%] [G loss: 0.984568]\n",
      "505 [D loss: 0.549760, acc.: 71.88%] [G loss: 1.037843]\n",
      "506 [D loss: 0.554779, acc.: 75.00%] [G loss: 1.010135]\n",
      "507 [D loss: 0.621899, acc.: 68.75%] [G loss: 0.993692]\n",
      "508 [D loss: 0.569353, acc.: 76.56%] [G loss: 1.016859]\n",
      "509 [D loss: 0.577284, acc.: 77.34%] [G loss: 1.009236]\n",
      "510 [D loss: 0.609386, acc.: 69.53%] [G loss: 0.983789]\n",
      "511 [D loss: 0.509507, acc.: 85.94%] [G loss: 1.048811]\n",
      "512 [D loss: 0.516862, acc.: 78.12%] [G loss: 1.071708]\n",
      "513 [D loss: 0.606373, acc.: 66.41%] [G loss: 1.047953]\n",
      "514 [D loss: 0.570911, acc.: 78.12%] [G loss: 1.008065]\n",
      "515 [D loss: 0.574466, acc.: 73.44%] [G loss: 0.989316]\n",
      "516 [D loss: 0.566789, acc.: 79.69%] [G loss: 0.965130]\n",
      "517 [D loss: 0.555014, acc.: 77.34%] [G loss: 1.011117]\n",
      "518 [D loss: 0.594634, acc.: 71.09%] [G loss: 0.953823]\n",
      "519 [D loss: 0.559026, acc.: 78.12%] [G loss: 1.015126]\n",
      "520 [D loss: 0.562471, acc.: 77.34%] [G loss: 1.029714]\n",
      "521 [D loss: 0.609186, acc.: 71.09%] [G loss: 0.993136]\n",
      "522 [D loss: 0.583653, acc.: 75.78%] [G loss: 0.979903]\n",
      "523 [D loss: 0.576785, acc.: 76.56%] [G loss: 0.965153]\n",
      "524 [D loss: 0.546706, acc.: 78.91%] [G loss: 0.980595]\n",
      "525 [D loss: 0.578661, acc.: 73.44%] [G loss: 0.975182]\n",
      "526 [D loss: 0.559215, acc.: 75.78%] [G loss: 0.941674]\n",
      "527 [D loss: 0.591902, acc.: 73.44%] [G loss: 0.958552]\n",
      "528 [D loss: 0.577523, acc.: 76.56%] [G loss: 0.995927]\n",
      "529 [D loss: 0.557385, acc.: 77.34%] [G loss: 1.021376]\n",
      "530 [D loss: 0.554350, acc.: 81.25%] [G loss: 1.044730]\n",
      "531 [D loss: 0.574854, acc.: 71.09%] [G loss: 1.040433]\n",
      "532 [D loss: 0.585565, acc.: 75.78%] [G loss: 1.002376]\n",
      "533 [D loss: 0.577054, acc.: 75.00%] [G loss: 1.014929]\n",
      "534 [D loss: 0.561459, acc.: 75.78%] [G loss: 1.062562]\n",
      "535 [D loss: 0.574048, acc.: 71.09%] [G loss: 1.015056]\n",
      "536 [D loss: 0.543962, acc.: 76.56%] [G loss: 1.010752]\n",
      "537 [D loss: 0.543569, acc.: 73.44%] [G loss: 1.068004]\n",
      "538 [D loss: 0.572761, acc.: 71.88%] [G loss: 1.068348]\n",
      "539 [D loss: 0.568228, acc.: 77.34%] [G loss: 1.037965]\n",
      "540 [D loss: 0.557548, acc.: 77.34%] [G loss: 1.032723]\n",
      "541 [D loss: 0.579748, acc.: 75.00%] [G loss: 0.997144]\n",
      "542 [D loss: 0.562837, acc.: 77.34%] [G loss: 0.998972]\n",
      "543 [D loss: 0.550060, acc.: 80.47%] [G loss: 1.001442]\n",
      "544 [D loss: 0.575172, acc.: 78.91%] [G loss: 0.999972]\n",
      "545 [D loss: 0.522587, acc.: 82.81%] [G loss: 0.986718]\n",
      "546 [D loss: 0.506933, acc.: 89.06%] [G loss: 1.011081]\n",
      "547 [D loss: 0.551379, acc.: 78.91%] [G loss: 0.995843]\n",
      "548 [D loss: 0.548295, acc.: 78.91%] [G loss: 0.977161]\n",
      "549 [D loss: 0.551374, acc.: 75.00%] [G loss: 0.971568]\n",
      "550 [D loss: 0.557667, acc.: 78.91%] [G loss: 0.980533]\n",
      "551 [D loss: 0.512865, acc.: 85.94%] [G loss: 1.002929]\n",
      "552 [D loss: 0.537791, acc.: 79.69%] [G loss: 0.951770]\n",
      "553 [D loss: 0.545047, acc.: 79.69%] [G loss: 0.956543]\n",
      "554 [D loss: 0.559822, acc.: 75.00%] [G loss: 0.978151]\n",
      "555 [D loss: 0.558456, acc.: 68.75%] [G loss: 0.997149]\n",
      "556 [D loss: 0.561444, acc.: 72.66%] [G loss: 1.020399]\n",
      "557 [D loss: 0.534374, acc.: 79.69%] [G loss: 1.028013]\n",
      "558 [D loss: 0.607462, acc.: 68.75%] [G loss: 0.991094]\n",
      "559 [D loss: 0.554528, acc.: 77.34%] [G loss: 1.005838]\n",
      "560 [D loss: 0.541372, acc.: 78.12%] [G loss: 1.031558]\n",
      "561 [D loss: 0.525662, acc.: 82.81%] [G loss: 1.015385]\n",
      "562 [D loss: 0.569681, acc.: 78.91%] [G loss: 1.031499]\n",
      "563 [D loss: 0.571008, acc.: 81.25%] [G loss: 0.996754]\n",
      "564 [D loss: 0.594605, acc.: 71.88%] [G loss: 0.979243]\n",
      "565 [D loss: 0.548734, acc.: 74.22%] [G loss: 1.039600]\n",
      "566 [D loss: 0.595437, acc.: 71.09%] [G loss: 1.023265]\n",
      "567 [D loss: 0.590997, acc.: 73.44%] [G loss: 1.013773]\n",
      "568 [D loss: 0.579115, acc.: 79.69%] [G loss: 1.021825]\n",
      "569 [D loss: 0.559594, acc.: 77.34%] [G loss: 0.966771]\n",
      "570 [D loss: 0.586563, acc.: 72.66%] [G loss: 0.989385]\n",
      "571 [D loss: 0.564885, acc.: 78.91%] [G loss: 0.988457]\n",
      "572 [D loss: 0.539741, acc.: 80.47%] [G loss: 1.014202]\n",
      "573 [D loss: 0.604897, acc.: 71.09%] [G loss: 0.951998]\n",
      "574 [D loss: 0.590530, acc.: 73.44%] [G loss: 0.947624]\n",
      "575 [D loss: 0.551989, acc.: 78.12%] [G loss: 0.963580]\n",
      "576 [D loss: 0.543515, acc.: 79.69%] [G loss: 1.025370]\n",
      "577 [D loss: 0.585245, acc.: 71.88%] [G loss: 1.017532]\n",
      "578 [D loss: 0.578710, acc.: 71.88%] [G loss: 1.003849]\n",
      "579 [D loss: 0.607578, acc.: 67.19%] [G loss: 0.975626]\n",
      "580 [D loss: 0.580253, acc.: 71.88%] [G loss: 1.000083]\n",
      "581 [D loss: 0.619508, acc.: 64.06%] [G loss: 1.001354]\n",
      "582 [D loss: 0.601056, acc.: 69.53%] [G loss: 1.027474]\n",
      "583 [D loss: 0.620165, acc.: 66.41%] [G loss: 0.953777]\n",
      "584 [D loss: 0.610164, acc.: 67.97%] [G loss: 0.979786]\n",
      "585 [D loss: 0.578792, acc.: 74.22%] [G loss: 0.953523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586 [D loss: 0.573569, acc.: 78.12%] [G loss: 1.059465]\n",
      "587 [D loss: 0.617965, acc.: 70.31%] [G loss: 1.004442]\n",
      "588 [D loss: 0.596242, acc.: 71.88%] [G loss: 1.017766]\n",
      "589 [D loss: 0.623021, acc.: 64.06%] [G loss: 1.003483]\n",
      "590 [D loss: 0.581931, acc.: 68.75%] [G loss: 1.090993]\n",
      "591 [D loss: 0.582219, acc.: 71.88%] [G loss: 1.070310]\n",
      "592 [D loss: 0.558309, acc.: 72.66%] [G loss: 1.074763]\n",
      "593 [D loss: 0.582613, acc.: 75.00%] [G loss: 1.051298]\n",
      "594 [D loss: 0.566128, acc.: 77.34%] [G loss: 1.057800]\n",
      "595 [D loss: 0.574451, acc.: 72.66%] [G loss: 1.010246]\n",
      "596 [D loss: 0.578024, acc.: 70.31%] [G loss: 0.977121]\n",
      "597 [D loss: 0.584743, acc.: 74.22%] [G loss: 0.968903]\n",
      "598 [D loss: 0.617654, acc.: 63.28%] [G loss: 1.000791]\n",
      "599 [D loss: 0.569237, acc.: 75.00%] [G loss: 0.973988]\n",
      "600 [D loss: 0.586063, acc.: 72.66%] [G loss: 0.974901]\n",
      "601 [D loss: 0.578633, acc.: 72.66%] [G loss: 0.945917]\n",
      "602 [D loss: 0.608126, acc.: 63.28%] [G loss: 0.957294]\n",
      "603 [D loss: 0.582138, acc.: 75.00%] [G loss: 0.976100]\n",
      "604 [D loss: 0.584349, acc.: 73.44%] [G loss: 1.022163]\n",
      "605 [D loss: 0.570793, acc.: 71.88%] [G loss: 0.985404]\n",
      "606 [D loss: 0.617200, acc.: 66.41%] [G loss: 0.996426]\n",
      "607 [D loss: 0.582494, acc.: 68.75%] [G loss: 1.008381]\n",
      "608 [D loss: 0.579703, acc.: 71.88%] [G loss: 1.011558]\n",
      "609 [D loss: 0.616005, acc.: 67.19%] [G loss: 0.969729]\n",
      "610 [D loss: 0.614010, acc.: 63.28%] [G loss: 0.940490]\n",
      "611 [D loss: 0.595234, acc.: 67.97%] [G loss: 0.947117]\n",
      "612 [D loss: 0.555916, acc.: 76.56%] [G loss: 0.942096]\n",
      "613 [D loss: 0.585677, acc.: 69.53%] [G loss: 0.948160]\n",
      "614 [D loss: 0.609794, acc.: 68.75%] [G loss: 0.936456]\n",
      "615 [D loss: 0.632406, acc.: 59.38%] [G loss: 0.971877]\n",
      "616 [D loss: 0.593055, acc.: 71.09%] [G loss: 1.013146]\n",
      "617 [D loss: 0.601129, acc.: 71.09%] [G loss: 1.028344]\n",
      "618 [D loss: 0.594262, acc.: 68.75%] [G loss: 0.979429]\n",
      "619 [D loss: 0.590585, acc.: 73.44%] [G loss: 0.979414]\n",
      "620 [D loss: 0.581751, acc.: 70.31%] [G loss: 1.016483]\n",
      "621 [D loss: 0.597773, acc.: 67.97%] [G loss: 1.007756]\n",
      "622 [D loss: 0.589613, acc.: 71.09%] [G loss: 0.963201]\n",
      "623 [D loss: 0.591613, acc.: 65.62%] [G loss: 0.954787]\n",
      "624 [D loss: 0.588950, acc.: 75.00%] [G loss: 0.991768]\n",
      "625 [D loss: 0.549878, acc.: 75.78%] [G loss: 1.019462]\n",
      "626 [D loss: 0.555729, acc.: 71.09%] [G loss: 1.006504]\n",
      "627 [D loss: 0.514401, acc.: 82.03%] [G loss: 1.008929]\n",
      "628 [D loss: 0.593956, acc.: 74.22%] [G loss: 0.981845]\n",
      "629 [D loss: 0.553418, acc.: 75.78%] [G loss: 0.955454]\n",
      "630 [D loss: 0.588859, acc.: 76.56%] [G loss: 0.965752]\n",
      "631 [D loss: 0.561813, acc.: 75.00%] [G loss: 1.015638]\n",
      "632 [D loss: 0.574999, acc.: 73.44%] [G loss: 1.005151]\n",
      "633 [D loss: 0.575943, acc.: 78.12%] [G loss: 0.944454]\n",
      "634 [D loss: 0.557636, acc.: 79.69%] [G loss: 0.954856]\n",
      "635 [D loss: 0.556384, acc.: 77.34%] [G loss: 0.936318]\n",
      "636 [D loss: 0.577754, acc.: 75.00%] [G loss: 0.952323]\n",
      "637 [D loss: 0.597678, acc.: 75.00%] [G loss: 1.005663]\n",
      "638 [D loss: 0.595496, acc.: 75.78%] [G loss: 0.982819]\n",
      "639 [D loss: 0.585645, acc.: 73.44%] [G loss: 0.918348]\n",
      "640 [D loss: 0.572157, acc.: 73.44%] [G loss: 0.941856]\n",
      "641 [D loss: 0.568508, acc.: 75.78%] [G loss: 0.973773]\n",
      "642 [D loss: 0.563997, acc.: 76.56%] [G loss: 1.006504]\n",
      "643 [D loss: 0.621582, acc.: 72.66%] [G loss: 1.015076]\n",
      "644 [D loss: 0.587598, acc.: 69.53%] [G loss: 0.973077]\n",
      "645 [D loss: 0.599632, acc.: 69.53%] [G loss: 0.945307]\n",
      "646 [D loss: 0.581634, acc.: 72.66%] [G loss: 1.021170]\n",
      "647 [D loss: 0.598974, acc.: 71.88%] [G loss: 1.021674]\n",
      "648 [D loss: 0.624848, acc.: 66.41%] [G loss: 0.950333]\n",
      "649 [D loss: 0.596701, acc.: 71.09%] [G loss: 0.932237]\n",
      "650 [D loss: 0.622809, acc.: 65.62%] [G loss: 0.945065]\n",
      "651 [D loss: 0.591221, acc.: 76.56%] [G loss: 1.017740]\n",
      "652 [D loss: 0.595187, acc.: 71.88%] [G loss: 1.055046]\n",
      "653 [D loss: 0.635297, acc.: 60.94%] [G loss: 1.001863]\n",
      "654 [D loss: 0.609409, acc.: 71.09%] [G loss: 0.975213]\n",
      "655 [D loss: 0.590826, acc.: 72.66%] [G loss: 0.967871]\n",
      "656 [D loss: 0.546342, acc.: 72.66%] [G loss: 1.026227]\n",
      "657 [D loss: 0.583826, acc.: 71.09%] [G loss: 1.042027]\n",
      "658 [D loss: 0.600883, acc.: 71.88%] [G loss: 0.980080]\n",
      "659 [D loss: 0.603549, acc.: 68.75%] [G loss: 0.987385]\n",
      "660 [D loss: 0.578805, acc.: 69.53%] [G loss: 0.952162]\n",
      "661 [D loss: 0.550422, acc.: 76.56%] [G loss: 0.996229]\n",
      "662 [D loss: 0.564508, acc.: 72.66%] [G loss: 1.002747]\n",
      "663 [D loss: 0.535918, acc.: 82.03%] [G loss: 1.033201]\n",
      "664 [D loss: 0.580323, acc.: 69.53%] [G loss: 1.028740]\n",
      "665 [D loss: 0.562932, acc.: 77.34%] [G loss: 1.011955]\n",
      "666 [D loss: 0.586935, acc.: 72.66%] [G loss: 0.970440]\n",
      "667 [D loss: 0.580503, acc.: 78.12%] [G loss: 1.001660]\n",
      "668 [D loss: 0.559859, acc.: 78.12%] [G loss: 0.976492]\n",
      "669 [D loss: 0.607864, acc.: 64.06%] [G loss: 0.971629]\n",
      "670 [D loss: 0.556058, acc.: 78.12%] [G loss: 0.965545]\n",
      "671 [D loss: 0.605965, acc.: 68.75%] [G loss: 0.997633]\n",
      "672 [D loss: 0.613767, acc.: 64.84%] [G loss: 0.939946]\n",
      "673 [D loss: 0.578289, acc.: 71.88%] [G loss: 1.021222]\n",
      "674 [D loss: 0.546505, acc.: 77.34%] [G loss: 1.042593]\n",
      "675 [D loss: 0.586165, acc.: 64.84%] [G loss: 1.032933]\n",
      "676 [D loss: 0.545838, acc.: 79.69%] [G loss: 1.001468]\n",
      "677 [D loss: 0.551334, acc.: 75.78%] [G loss: 1.000081]\n",
      "678 [D loss: 0.547522, acc.: 72.66%] [G loss: 1.039432]\n",
      "679 [D loss: 0.513537, acc.: 84.38%] [G loss: 1.022162]\n",
      "680 [D loss: 0.587514, acc.: 71.09%] [G loss: 0.975903]\n",
      "681 [D loss: 0.595227, acc.: 72.66%] [G loss: 1.025640]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3c5076b11a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-3576e3aa9848>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# Train the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m# Plot the progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(epochs=30000, batch_size=128, save_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capsule Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 24, 24, 8)         208       \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)   (None, 8, 8, 64)          41536     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 512, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 512, 8)            0         \n",
      "_________________________________________________________________\n",
      "digitcaps (CapsuleLayer)     (None, 10, 16)            660480    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 702,385\n",
      "Trainable params: 697,265\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    img = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=8, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(img)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=8, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "    \n",
    "    x = layers.Flatten()(digitcaps)\n",
    "    prediction = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model(img, prediction)\n",
    "\n",
    "discriminator = CapsNet([28, 28, 1], 10, 3)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs',\n",
    "                               batch_size=batch_size, histogram_freq=debug)\n",
    "    checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: lr * (0.9 ** epoch))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizers.Adam(lr=lr),\n",
    "                  loss=[margin_loss],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, batch_size, shift_fraction),\n",
    "                        steps_per_epoch=int(y_train.shape[0] / batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    # End: Training with data augmentation -----------------------------------------------------------------------#\n",
    "\n",
    "    model.save_weights(save_dir + '/trained_model_capsnet.h5')\n",
    "    print('Trained model saved to \\'%s/trained_model_capsnet.h5\\'' % save_dir)\n",
    "\n",
    "    from utils import plot_log\n",
    "    plot_log(save_dir + '/log.csv', show=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from utils import combine_images\n",
    "    from PIL import Image\n",
    "\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    from keras.datasets import mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "lam_recon = 0.392  # 784 * 0.0005, paper uses sum of SE, here uses MSE\n",
    "num_routing = 3\n",
    "shift_fraction = 0.1\n",
    "debug = 1  # debug>0 will save weights by TensorBoard\n",
    "save_dir ='./result'\n",
    "is_training = 1\n",
    "weights = None\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 24, 24, 8)         208       \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)   (None, 8, 8, 64)          41536     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 512, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 512, 8)            0         \n",
      "_________________________________________________________________\n",
      "digitcaps (CapsuleLayer)     (None, 10, 16)            660480    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 702,385\n",
      "Trainable params: 697,265\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "\n",
    "# define model\n",
    "model = CapsNet(input_shape=[28, 28, 1],\n",
    "                n_class=len(np.unique(np.argmax(y_train, 1))),\n",
    "                num_routing=num_routing)\n",
    "model.summary()\n",
    "#plot_model(model, to_file=save_dir+'/model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=optimizers.Adam(lr=lr),\n",
    "loss=[margin_loss],\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[9,9,8,64]\n\t [[Node: training_1/Adam/mul_11 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam_1/beta_1/read, training_1/Adam/Variable_2/read)]]\n\t [[Node: digitcaps_3/Reshape_2/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_369_digitcaps_3/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'training_1/Adam/mul_11', defined at:\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-0c77fd4d45a4>\", line 5, in <module>\n    history = model.train_on_batch(x_train, y_train)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\", line 1838, in train_on_batch\n    self._make_train_function()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\optimizers.py\", line 432, in get_updates\n    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 754, in _run_op\n    return getattr(ops.Tensor, operator)(a._AsTensor(), *args)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1117, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2725, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[9,9,8,64]\n\t [[Node: training_1/Adam/mul_11 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam_1/beta_1/read, training_1/Adam/Variable_2/read)]]\n\t [[Node: digitcaps_3/Reshape_2/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_369_digitcaps_3/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[9,9,8,64]\n\t [[Node: training_1/Adam/mul_11 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam_1/beta_1/read, training_1/Adam/Variable_2/read)]]\n\t [[Node: digitcaps_3/Reshape_2/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_369_digitcaps_3/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0c77fd4d45a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# as long as weights are given, will run testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[9,9,8,64]\n\t [[Node: training_1/Adam/mul_11 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam_1/beta_1/read, training_1/Adam/Variable_2/read)]]\n\t [[Node: digitcaps_3/Reshape_2/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_369_digitcaps_3/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'training_1/Adam/mul_11', defined at:\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-0c77fd4d45a4>\", line 5, in <module>\n    history = model.train_on_batch(x_train, y_train)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\", line 1838, in train_on_batch\n    self._make_train_function()\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\optimizers.py\", line 432, in get_updates\n    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 754, in _run_op\n    return getattr(ops.Tensor, operator)(a._AsTensor(), *args)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1117, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2725, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[9,9,8,64]\n\t [[Node: training_1/Adam/mul_11 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam_1/beta_1/read, training_1/Adam/Variable_2/read)]]\n\t [[Node: digitcaps_3/Reshape_2/_267 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_369_digitcaps_3/Reshape_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# train or test\n",
    "if weights is not None:  # init the model weights with provided one\n",
    "    model.load_weights(args.weights)\n",
    "if is_training:\n",
    "    history = model.train_on_batch(x_train, y_train)\n",
    "else:  # as long as weights are given, will run testing\n",
    "    if args.weights is None:\n",
    "        print('No weights are provided. Will test using random initialized weights.')\n",
    "    model.train_on_batch(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
