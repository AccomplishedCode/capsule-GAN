{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 32 \n",
    "        self.img_cols = 32\n",
    "        self.mask_height = 10\n",
    "        self.mask_width = 10\n",
    "        self.channels = 3\n",
    "        self.num_classes = 2\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], \n",
    "            loss_weights=[0.5, 0.5],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss=['binary_crossentropy'], \n",
    "            optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        masked_img = Input(shape=self.img_shape)\n",
    "        gen_img = self.generator(masked_img)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid, _ = self.discriminator(gen_img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # masked_img as input => generates images => determines validity \n",
    "        self.combined = Model(masked_img , [gen_img, valid])\n",
    "        self.combined.compile(loss=['mse', 'binary_crossentropy'],\n",
    "            loss_weights=[0.999, 0.001],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        # Encoder\n",
    "        model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        # Decoder\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation('tanh'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        masked_img = Input(shape=self.img_shape)\n",
    "        img = model(masked_img)\n",
    "\n",
    "        return Model(masked_img, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Flatten())\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        features = model(img)\n",
    "\n",
    "        valid = Dense(1, activation=\"sigmoid\")(features)\n",
    "        label = Dense(self.num_classes+1, activation=\"softmax\")(features)\n",
    "\n",
    "        return Model(img, [valid, label])\n",
    "\n",
    "    def mask_randomly(self, imgs):\n",
    "        y1 = np.random.randint(0, self.img_rows - self.mask_height, imgs.shape[0])\n",
    "        y2 = y1 + self.mask_height\n",
    "        x1 = np.random.randint(0, self.img_rows - self.mask_width, imgs.shape[0])\n",
    "        x2 = x1 + self.mask_width\n",
    "\n",
    "        masked_imgs = np.empty_like(imgs)\n",
    "        for i, img in enumerate(imgs):\n",
    "            masked_img = img.copy()\n",
    "            _y1, _y2, _x1, _x2 = y1[i], y2[i], x1[i], x2[i], \n",
    "            masked_img[_y1:_y2, _x1:_x2, :] = 0\n",
    "            masked_imgs[i] = masked_img\n",
    "\n",
    "        return masked_imgs\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "\n",
    "        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        X_train = np.vstack((X_train, X_test))\n",
    "        y_train = np.vstack((y_train, y_test))\n",
    "\n",
    "        # Extract dogs and cats\n",
    "        X_cats = X_train[(y_train == 3).flatten()]\n",
    "        y_cats = y_train[y_train == 3]\n",
    "        X_dogs = X_train[(y_train == 5).flatten()]\n",
    "        y_dogs = y_train[y_train == 5]\n",
    "        X_train = np.vstack((X_cats, X_dogs))\n",
    "        y_train = np.vstack((y_cats, y_dogs))\n",
    "\n",
    "        # Change labels to 0 and 1\n",
    "        y_train[y_train == 3] = 0\n",
    "        y_train[y_train == 5] = 1\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 255\n",
    "        X_train = 2 * X_train - 1\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        # Class weights:\n",
    "        # To balance the difference in occurences of digit class labels. \n",
    "        # 50% of labels that the discriminator trains on are 'fake'.\n",
    "        # Weight = 1 / frequency\n",
    "        cw1 = {0: 1, 1: 1}\n",
    "        cw2 = {i: self.num_classes / half_batch for i in range(self.num_classes)}\n",
    "        cw2[self.num_classes] = 1 / half_batch\n",
    "        class_weights = [cw1, cw2]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "            labels = y_train[idx]\n",
    "\n",
    "            masked_imgs = self.mask_randomly(imgs)\n",
    "            \n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(masked_imgs)\n",
    "\n",
    "            valid = np.ones((half_batch, 1))\n",
    "            fake = np.zeros((half_batch, 1))\n",
    "\n",
    "            labels = to_categorical(labels, num_classes=self.num_classes+1)\n",
    "            fake_labels = to_categorical(np.full((half_batch, 1), self.num_classes), num_classes=self.num_classes+1)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, labels], class_weight=class_weights)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, fake_labels], class_weight=class_weights)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            masked_imgs = self.mask_randomly(imgs)\n",
    "\n",
    "            # Generator wants the discriminator to label the generated images as valid\n",
    "            valid = np.ones((batch_size, 1))\n",
    "            \n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch(masked_imgs, [imgs, valid])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%, op_acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0], g_loss[1]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                # Select a random half batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], 6)\n",
    "                imgs = X_train[idx]\n",
    "                self.save_imgs(epoch, imgs)\n",
    "                self.save_model()\n",
    "\n",
    "    def save_imgs(self, epoch, imgs):\n",
    "        r, c = 3, 6\n",
    "        \n",
    "        masked_imgs = self.mask_randomly(imgs)\n",
    "        gen_imgs = self.generator.predict(masked_imgs)\n",
    "\n",
    "        imgs = 0.5 * imgs + 0.5\n",
    "        masked_imgs = 0.5 * masked_imgs + 0.5\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for i in range(c):\n",
    "            axs[0,i].imshow(imgs[i, :,:])\n",
    "            axs[0,i].axis('off')\n",
    "            axs[1,i].imshow(masked_imgs[i, :,:])\n",
    "            axs[1,i].axis('off')\n",
    "            axs[2,i].imshow(gen_imgs[i, :,:])\n",
    "            axs[2,i].axis('off')\n",
    "        fig.savefig(\"images/cifar_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"ccgan/saved_model/%s.json\" % model_name\n",
    "            weights_path = \"ccgan/saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path, \n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"ccgan_generator\")\n",
    "        save(self.discriminator, \"ccgan_discriminator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "=================================================================\n",
      "Total params: 1,126,080\n",
      "Trainable params: 1,126,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 16, 16, 64)        3136      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 8, 8, 128)         131200    \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 4, 4, 256)         524544    \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 8, 8, 128)         524416    \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 16, 64)        131136    \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 32, 32, 3)         3075      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,317,507\n",
      "Trainable params: 1,317,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.403407, acc: 0.00%, op_acc: 18.75%] [G loss: 0.283719, mse: 0.283314]\n",
      "1 [D loss: 0.379286, acc: 50.00%, op_acc: 15.62%] [G loss: 0.231170, mse: 0.230731]\n",
      "2 [D loss: 0.370387, acc: 50.00%, op_acc: 31.25%] [G loss: 0.183978, mse: 0.183545]\n",
      "3 [D loss: 0.385339, acc: 50.00%, op_acc: 25.00%] [G loss: 0.222561, mse: 0.222231]\n",
      "4 [D loss: 0.351296, acc: 50.00%, op_acc: 21.88%] [G loss: 0.211636, mse: 0.211300]\n",
      "5 [D loss: 0.383958, acc: 50.00%, op_acc: 31.25%] [G loss: 0.220926, mse: 0.220606]\n",
      "6 [D loss: 0.366975, acc: 50.00%, op_acc: 25.00%] [G loss: 0.165207, mse: 0.164823]\n",
      "7 [D loss: 0.364516, acc: 50.00%, op_acc: 15.62%] [G loss: 0.138129, mse: 0.137672]\n",
      "8 [D loss: 0.348829, acc: 50.00%, op_acc: 31.25%] [G loss: 0.135600, mse: 0.135143]\n",
      "9 [D loss: 0.347891, acc: 50.00%, op_acc: 31.25%] [G loss: 0.142456, mse: 0.142029]\n",
      "10 [D loss: 0.355096, acc: 50.00%, op_acc: 31.25%] [G loss: 0.120759, mse: 0.120301]\n",
      "11 [D loss: 0.344686, acc: 50.00%, op_acc: 21.88%] [G loss: 0.119382, mse: 0.118896]\n",
      "12 [D loss: 0.333790, acc: 50.00%, op_acc: 34.38%] [G loss: 0.123640, mse: 0.123117]\n",
      "13 [D loss: 0.339076, acc: 50.00%, op_acc: 21.88%] [G loss: 0.137722, mse: 0.137248]\n",
      "14 [D loss: 0.300133, acc: 50.00%, op_acc: 28.12%] [G loss: 0.115773, mse: 0.115219]\n",
      "15 [D loss: 0.304412, acc: 50.00%, op_acc: 34.38%] [G loss: 0.127557, mse: 0.126995]\n",
      "16 [D loss: 0.290402, acc: 59.38%, op_acc: 28.12%] [G loss: 0.108187, mse: 0.107586]\n",
      "17 [D loss: 0.291087, acc: 65.62%, op_acc: 34.38%] [G loss: 0.107581, mse: 0.106919]\n",
      "18 [D loss: 0.271735, acc: 81.25%, op_acc: 18.75%] [G loss: 0.107022, mse: 0.106271]\n",
      "19 [D loss: 0.210540, acc: 96.88%, op_acc: 25.00%] [G loss: 0.091550, mse: 0.090568]\n",
      "20 [D loss: 0.257543, acc: 75.00%, op_acc: 28.12%] [G loss: 0.084403, mse: 0.083213]\n",
      "21 [D loss: 0.251824, acc: 90.62%, op_acc: 59.38%] [G loss: 0.095196, mse: 0.094207]\n",
      "22 [D loss: 0.168028, acc: 96.88%, op_acc: 62.50%] [G loss: 0.104421, mse: 0.103201]\n",
      "23 [D loss: 0.125884, acc: 100.00%, op_acc: 87.50%] [G loss: 0.083878, mse: 0.082047]\n",
      "24 [D loss: 0.241599, acc: 87.50%, op_acc: 56.25%] [G loss: 0.108544, mse: 0.106603]\n",
      "25 [D loss: 0.066626, acc: 100.00%, op_acc: 78.12%] [G loss: 0.105420, mse: 0.102478]\n",
      "26 [D loss: 0.494489, acc: 50.00%, op_acc: 25.00%] [G loss: 0.095827, mse: 0.094732]\n",
      "27 [D loss: 0.134041, acc: 93.75%, op_acc: 62.50%] [G loss: 0.086542, mse: 0.085018]\n",
      "28 [D loss: 0.106395, acc: 100.00%, op_acc: 71.88%] [G loss: 0.083988, mse: 0.082233]\n",
      "29 [D loss: 0.104711, acc: 96.88%, op_acc: 78.12%] [G loss: 0.083010, mse: 0.081126]\n",
      "30 [D loss: 0.066581, acc: 100.00%, op_acc: 71.88%] [G loss: 0.095402, mse: 0.092770]\n",
      "31 [D loss: 0.060763, acc: 100.00%, op_acc: 68.75%] [G loss: 0.090617, mse: 0.087694]\n",
      "32 [D loss: 0.184657, acc: 93.75%, op_acc: 68.75%] [G loss: 0.083677, mse: 0.080748]\n",
      "33 [D loss: 0.108905, acc: 96.88%, op_acc: 78.12%] [G loss: 0.108009, mse: 0.104985]\n",
      "34 [D loss: 0.052009, acc: 100.00%, op_acc: 84.38%] [G loss: 0.091153, mse: 0.087818]\n",
      "35 [D loss: 0.162610, acc: 93.75%, op_acc: 68.75%] [G loss: 0.076580, mse: 0.073946]\n",
      "36 [D loss: 0.109582, acc: 96.88%, op_acc: 59.38%] [G loss: 0.085302, mse: 0.083023]\n",
      "37 [D loss: 0.099413, acc: 93.75%, op_acc: 75.00%] [G loss: 0.079389, mse: 0.075968]\n",
      "38 [D loss: 0.095603, acc: 93.75%, op_acc: 81.25%] [G loss: 0.069548, mse: 0.066198]\n",
      "39 [D loss: 0.075357, acc: 93.75%, op_acc: 84.38%] [G loss: 0.089923, mse: 0.087446]\n",
      "40 [D loss: 0.046167, acc: 100.00%, op_acc: 78.12%] [G loss: 0.083087, mse: 0.079614]\n",
      "41 [D loss: 0.027754, acc: 100.00%, op_acc: 81.25%] [G loss: 0.074490, mse: 0.069913]\n",
      "42 [D loss: 0.068068, acc: 96.88%, op_acc: 78.12%] [G loss: 0.078846, mse: 0.074057]\n",
      "43 [D loss: 0.086563, acc: 96.88%, op_acc: 75.00%] [G loss: 0.083107, mse: 0.078606]\n",
      "44 [D loss: 0.096379, acc: 96.88%, op_acc: 75.00%] [G loss: 0.075577, mse: 0.072207]\n",
      "45 [D loss: 0.030832, acc: 100.00%, op_acc: 75.00%] [G loss: 0.079237, mse: 0.074319]\n",
      "46 [D loss: 0.032432, acc: 100.00%, op_acc: 81.25%] [G loss: 0.098139, mse: 0.092816]\n",
      "47 [D loss: 0.035059, acc: 100.00%, op_acc: 87.50%] [G loss: 0.161155, mse: 0.157815]\n",
      "48 [D loss: 0.038869, acc: 100.00%, op_acc: 59.38%] [G loss: 0.126173, mse: 0.122293]\n",
      "49 [D loss: 0.034521, acc: 100.00%, op_acc: 78.12%] [G loss: 0.110961, mse: 0.105837]\n",
      "50 [D loss: 0.033678, acc: 100.00%, op_acc: 68.75%] [G loss: 0.079728, mse: 0.074329]\n",
      "51 [D loss: 0.040471, acc: 100.00%, op_acc: 65.62%] [G loss: 0.090331, mse: 0.083465]\n",
      "52 [D loss: 0.034711, acc: 100.00%, op_acc: 75.00%] [G loss: 0.088539, mse: 0.082859]\n",
      "53 [D loss: 0.031912, acc: 100.00%, op_acc: 75.00%] [G loss: 0.086335, mse: 0.081042]\n",
      "54 [D loss: 0.078219, acc: 96.88%, op_acc: 84.38%] [G loss: 0.083441, mse: 0.076212]\n",
      "55 [D loss: 0.244429, acc: 87.50%, op_acc: 68.75%] [G loss: 0.079406, mse: 0.074668]\n",
      "56 [D loss: 0.033998, acc: 100.00%, op_acc: 68.75%] [G loss: 0.076759, mse: 0.071300]\n",
      "57 [D loss: 0.118486, acc: 93.75%, op_acc: 78.12%] [G loss: 0.079604, mse: 0.076741]\n",
      "58 [D loss: 0.103972, acc: 90.62%, op_acc: 62.50%] [G loss: 0.073329, mse: 0.068449]\n",
      "59 [D loss: 0.063112, acc: 96.88%, op_acc: 78.12%] [G loss: 0.084940, mse: 0.080421]\n",
      "60 [D loss: 0.128533, acc: 96.88%, op_acc: 81.25%] [G loss: 0.071482, mse: 0.067920]\n",
      "61 [D loss: 0.068582, acc: 96.88%, op_acc: 65.62%] [G loss: 0.074074, mse: 0.071330]\n",
      "62 [D loss: 0.046290, acc: 100.00%, op_acc: 71.88%] [G loss: 0.074432, mse: 0.070195]\n",
      "63 [D loss: 0.042121, acc: 96.88%, op_acc: 75.00%] [G loss: 0.076874, mse: 0.072299]\n",
      "64 [D loss: 0.041928, acc: 96.88%, op_acc: 75.00%] [G loss: 0.068144, mse: 0.063379]\n",
      "65 [D loss: 0.028687, acc: 100.00%, op_acc: 78.12%] [G loss: 0.087525, mse: 0.082995]\n",
      "66 [D loss: 0.263524, acc: 87.50%, op_acc: 62.50%] [G loss: 0.089915, mse: 0.085829]\n",
      "67 [D loss: 0.032348, acc: 100.00%, op_acc: 71.88%] [G loss: 0.100494, mse: 0.095871]\n",
      "68 [D loss: 0.033449, acc: 100.00%, op_acc: 81.25%] [G loss: 0.077983, mse: 0.073271]\n",
      "69 [D loss: 0.064004, acc: 96.88%, op_acc: 75.00%] [G loss: 0.077486, mse: 0.073787]\n",
      "70 [D loss: 0.061216, acc: 96.88%, op_acc: 81.25%] [G loss: 0.067075, mse: 0.062674]\n",
      "71 [D loss: 0.031393, acc: 100.00%, op_acc: 71.88%] [G loss: 0.072280, mse: 0.067638]\n",
      "72 [D loss: 0.040650, acc: 96.88%, op_acc: 78.12%] [G loss: 0.068910, mse: 0.063662]\n",
      "73 [D loss: 0.055536, acc: 96.88%, op_acc: 84.38%] [G loss: 0.064291, mse: 0.060551]\n",
      "74 [D loss: 0.212242, acc: 87.50%, op_acc: 71.88%] [G loss: 0.068549, mse: 0.064373]\n",
      "75 [D loss: 0.085666, acc: 96.88%, op_acc: 71.88%] [G loss: 0.066275, mse: 0.061935]\n",
      "76 [D loss: 0.169132, acc: 93.75%, op_acc: 68.75%] [G loss: 0.068003, mse: 0.064485]\n",
      "77 [D loss: 0.134515, acc: 90.62%, op_acc: 65.62%] [G loss: 0.061351, mse: 0.058924]\n",
      "78 [D loss: 0.071857, acc: 100.00%, op_acc: 78.12%] [G loss: 0.064807, mse: 0.061627]\n",
      "79 [D loss: 0.054387, acc: 96.88%, op_acc: 78.12%] [G loss: 0.060067, mse: 0.056363]\n",
      "80 [D loss: 0.034522, acc: 100.00%, op_acc: 84.38%] [G loss: 0.067291, mse: 0.063296]\n",
      "81 [D loss: 0.086204, acc: 93.75%, op_acc: 65.62%] [G loss: 0.058265, mse: 0.054650]\n",
      "82 [D loss: 0.034582, acc: 100.00%, op_acc: 78.12%] [G loss: 0.068285, mse: 0.063996]\n",
      "83 [D loss: 0.029129, acc: 100.00%, op_acc: 59.38%] [G loss: 0.061782, mse: 0.057013]\n",
      "84 [D loss: 0.029152, acc: 100.00%, op_acc: 75.00%] [G loss: 0.075924, mse: 0.071089]\n",
      "85 [D loss: 0.051619, acc: 100.00%, op_acc: 78.12%] [G loss: 0.064473, mse: 0.059956]\n",
      "86 [D loss: 0.041144, acc: 100.00%, op_acc: 65.62%] [G loss: 0.068076, mse: 0.063266]\n",
      "87 [D loss: 0.026954, acc: 100.00%, op_acc: 75.00%] [G loss: 0.082522, mse: 0.077670]\n",
      "88 [D loss: 0.024322, acc: 100.00%, op_acc: 78.12%] [G loss: 0.079773, mse: 0.074466]\n",
      "89 [D loss: 0.023116, acc: 100.00%, op_acc: 75.00%] [G loss: 0.075639, mse: 0.069939]\n",
      "90 [D loss: 0.026943, acc: 100.00%, op_acc: 78.12%] [G loss: 0.065722, mse: 0.060320]\n",
      "91 [D loss: 0.023931, acc: 100.00%, op_acc: 75.00%] [G loss: 0.069752, mse: 0.064102]\n",
      "92 [D loss: 0.021941, acc: 100.00%, op_acc: 84.38%] [G loss: 0.064527, mse: 0.058692]\n",
      "93 [D loss: 0.027009, acc: 100.00%, op_acc: 71.88%] [G loss: 0.075781, mse: 0.069329]\n",
      "94 [D loss: 0.039184, acc: 100.00%, op_acc: 68.75%] [G loss: 0.090599, mse: 0.083249]\n",
      "95 [D loss: 0.046610, acc: 100.00%, op_acc: 75.00%] [G loss: 0.123230, mse: 0.115465]\n",
      "96 [D loss: 0.026081, acc: 100.00%, op_acc: 75.00%] [G loss: 0.081041, mse: 0.074419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 [D loss: 0.055120, acc: 96.88%, op_acc: 65.62%] [G loss: 0.075783, mse: 0.070841]\n",
      "98 [D loss: 0.030246, acc: 100.00%, op_acc: 78.12%] [G loss: 0.074588, mse: 0.069689]\n",
      "99 [D loss: 0.030445, acc: 100.00%, op_acc: 78.12%] [G loss: 0.081512, mse: 0.076047]\n",
      "100 [D loss: 0.027202, acc: 100.00%, op_acc: 71.88%] [G loss: 0.073164, mse: 0.067431]\n",
      "101 [D loss: 0.022555, acc: 100.00%, op_acc: 84.38%] [G loss: 0.065548, mse: 0.059171]\n",
      "102 [D loss: 0.029827, acc: 100.00%, op_acc: 68.75%] [G loss: 0.071756, mse: 0.064604]\n",
      "103 [D loss: 0.026669, acc: 100.00%, op_acc: 71.88%] [G loss: 0.074969, mse: 0.067980]\n",
      "104 [D loss: 0.021192, acc: 100.00%, op_acc: 81.25%] [G loss: 0.072390, mse: 0.065490]\n",
      "105 [D loss: 0.056361, acc: 96.88%, op_acc: 75.00%] [G loss: 0.068037, mse: 0.062051]\n",
      "106 [D loss: 0.021015, acc: 100.00%, op_acc: 75.00%] [G loss: 0.066955, mse: 0.060254]\n",
      "107 [D loss: 0.025359, acc: 100.00%, op_acc: 71.88%] [G loss: 0.072915, mse: 0.065748]\n",
      "108 [D loss: 0.023541, acc: 100.00%, op_acc: 75.00%] [G loss: 0.076704, mse: 0.070157]\n",
      "109 [D loss: 0.033137, acc: 100.00%, op_acc: 71.88%] [G loss: 0.079775, mse: 0.073412]\n",
      "110 [D loss: 0.025211, acc: 100.00%, op_acc: 81.25%] [G loss: 0.076351, mse: 0.069968]\n",
      "111 [D loss: 0.024707, acc: 100.00%, op_acc: 87.50%] [G loss: 0.066779, mse: 0.060034]\n",
      "112 [D loss: 0.138871, acc: 96.88%, op_acc: 78.12%] [G loss: 0.068909, mse: 0.063119]\n",
      "113 [D loss: 0.021820, acc: 100.00%, op_acc: 87.50%] [G loss: 0.076656, mse: 0.070710]\n",
      "114 [D loss: 0.038436, acc: 100.00%, op_acc: 65.62%] [G loss: 0.064613, mse: 0.058097]\n",
      "115 [D loss: 0.024651, acc: 100.00%, op_acc: 78.12%] [G loss: 0.067460, mse: 0.060510]\n",
      "116 [D loss: 0.089506, acc: 96.88%, op_acc: 65.62%] [G loss: 0.055779, mse: 0.050837]\n",
      "117 [D loss: 0.035715, acc: 100.00%, op_acc: 81.25%] [G loss: 0.064772, mse: 0.058192]\n",
      "118 [D loss: 0.030662, acc: 100.00%, op_acc: 71.88%] [G loss: 0.062026, mse: 0.055257]\n",
      "119 [D loss: 0.225367, acc: 87.50%, op_acc: 56.25%] [G loss: 0.065953, mse: 0.061071]\n",
      "120 [D loss: 0.024198, acc: 100.00%, op_acc: 75.00%] [G loss: 0.064862, mse: 0.059361]\n",
      "121 [D loss: 0.093487, acc: 93.75%, op_acc: 81.25%] [G loss: 0.058503, mse: 0.055792]\n",
      "122 [D loss: 0.166617, acc: 87.50%, op_acc: 50.00%] [G loss: 0.065233, mse: 0.059742]\n",
      "123 [D loss: 0.207263, acc: 90.62%, op_acc: 65.62%] [G loss: 0.056568, mse: 0.053473]\n",
      "124 [D loss: 0.041569, acc: 100.00%, op_acc: 78.12%] [G loss: 0.061042, mse: 0.058071]\n",
      "125 [D loss: 0.094851, acc: 93.75%, op_acc: 78.12%] [G loss: 0.070131, mse: 0.066637]\n",
      "126 [D loss: 0.098432, acc: 96.88%, op_acc: 75.00%] [G loss: 0.072212, mse: 0.068539]\n",
      "127 [D loss: 0.087135, acc: 96.88%, op_acc: 81.25%] [G loss: 0.076573, mse: 0.073377]\n",
      "128 [D loss: 0.033944, acc: 100.00%, op_acc: 78.12%] [G loss: 0.058993, mse: 0.055493]\n",
      "129 [D loss: 0.028638, acc: 100.00%, op_acc: 78.12%] [G loss: 0.059338, mse: 0.055583]\n",
      "130 [D loss: 0.035153, acc: 100.00%, op_acc: 81.25%] [G loss: 0.060661, mse: 0.056820]\n",
      "131 [D loss: 0.029664, acc: 100.00%, op_acc: 71.88%] [G loss: 0.061728, mse: 0.057617]\n",
      "132 [D loss: 0.026068, acc: 100.00%, op_acc: 71.88%] [G loss: 0.057654, mse: 0.053362]\n",
      "133 [D loss: 0.027033, acc: 100.00%, op_acc: 78.12%] [G loss: 0.059547, mse: 0.054190]\n",
      "134 [D loss: 0.024302, acc: 100.00%, op_acc: 78.12%] [G loss: 0.060395, mse: 0.054322]\n",
      "135 [D loss: 0.023353, acc: 100.00%, op_acc: 75.00%] [G loss: 0.058065, mse: 0.052054]\n",
      "136 [D loss: 0.032719, acc: 100.00%, op_acc: 68.75%] [G loss: 0.057605, mse: 0.051548]\n",
      "137 [D loss: 0.021665, acc: 100.00%, op_acc: 75.00%] [G loss: 0.064334, mse: 0.058385]\n",
      "138 [D loss: 0.037173, acc: 100.00%, op_acc: 84.38%] [G loss: 0.060699, mse: 0.056686]\n",
      "139 [D loss: 0.149682, acc: 90.62%, op_acc: 59.38%] [G loss: 0.069271, mse: 0.062041]\n",
      "140 [D loss: 0.101427, acc: 93.75%, op_acc: 68.75%] [G loss: 0.062572, mse: 0.059739]\n",
      "141 [D loss: 0.088414, acc: 96.88%, op_acc: 75.00%] [G loss: 0.068619, mse: 0.063488]\n",
      "142 [D loss: 0.026679, acc: 100.00%, op_acc: 81.25%] [G loss: 0.062570, mse: 0.057197]\n",
      "143 [D loss: 0.041766, acc: 100.00%, op_acc: 65.62%] [G loss: 0.057236, mse: 0.051250]\n",
      "144 [D loss: 0.032913, acc: 100.00%, op_acc: 75.00%] [G loss: 0.060019, mse: 0.054522]\n",
      "145 [D loss: 0.031194, acc: 100.00%, op_acc: 75.00%] [G loss: 0.062562, mse: 0.057734]\n",
      "146 [D loss: 0.027128, acc: 100.00%, op_acc: 75.00%] [G loss: 0.069303, mse: 0.064032]\n",
      "147 [D loss: 0.035726, acc: 100.00%, op_acc: 71.88%] [G loss: 0.061004, mse: 0.056199]\n",
      "148 [D loss: 0.090997, acc: 93.75%, op_acc: 75.00%] [G loss: 0.071832, mse: 0.065952]\n",
      "149 [D loss: 0.027315, acc: 100.00%, op_acc: 68.75%] [G loss: 0.060953, mse: 0.053972]\n",
      "150 [D loss: 0.331138, acc: 93.75%, op_acc: 75.00%] [G loss: 0.067546, mse: 0.064666]\n",
      "151 [D loss: 0.052808, acc: 100.00%, op_acc: 78.12%] [G loss: 0.063093, mse: 0.059557]\n",
      "152 [D loss: 0.034583, acc: 100.00%, op_acc: 81.25%] [G loss: 0.056545, mse: 0.052459]\n",
      "153 [D loss: 0.028539, acc: 100.00%, op_acc: 75.00%] [G loss: 0.064343, mse: 0.059925]\n",
      "154 [D loss: 0.031407, acc: 100.00%, op_acc: 78.12%] [G loss: 0.062021, mse: 0.057488]\n",
      "155 [D loss: 0.066043, acc: 93.75%, op_acc: 78.12%] [G loss: 0.057692, mse: 0.053474]\n",
      "156 [D loss: 0.029399, acc: 100.00%, op_acc: 78.12%] [G loss: 0.054433, mse: 0.050089]\n",
      "157 [D loss: 0.024103, acc: 100.00%, op_acc: 84.38%] [G loss: 0.067212, mse: 0.062596]\n",
      "158 [D loss: 0.051237, acc: 96.88%, op_acc: 68.75%] [G loss: 0.071586, mse: 0.067860]\n",
      "159 [D loss: 0.056594, acc: 96.88%, op_acc: 65.62%] [G loss: 0.057890, mse: 0.052984]\n",
      "160 [D loss: 0.026308, acc: 100.00%, op_acc: 78.12%] [G loss: 0.057408, mse: 0.051536]\n",
      "161 [D loss: 0.130731, acc: 93.75%, op_acc: 78.12%] [G loss: 0.055666, mse: 0.051686]\n",
      "162 [D loss: 0.030465, acc: 100.00%, op_acc: 65.62%] [G loss: 0.061617, mse: 0.057601]\n",
      "163 [D loss: 0.028655, acc: 100.00%, op_acc: 75.00%] [G loss: 0.056655, mse: 0.052017]\n",
      "164 [D loss: 0.032016, acc: 100.00%, op_acc: 71.88%] [G loss: 0.060352, mse: 0.055338]\n",
      "165 [D loss: 0.026163, acc: 100.00%, op_acc: 71.88%] [G loss: 0.059317, mse: 0.054215]\n",
      "166 [D loss: 0.037062, acc: 100.00%, op_acc: 68.75%] [G loss: 0.054733, mse: 0.050319]\n",
      "167 [D loss: 0.046273, acc: 96.88%, op_acc: 71.88%] [G loss: 0.050848, mse: 0.045729]\n",
      "168 [D loss: 0.023893, acc: 100.00%, op_acc: 81.25%] [G loss: 0.059563, mse: 0.053582]\n",
      "169 [D loss: 0.032469, acc: 100.00%, op_acc: 81.25%] [G loss: 0.060244, mse: 0.055159]\n",
      "170 [D loss: 0.100736, acc: 96.88%, op_acc: 81.25%] [G loss: 0.057552, mse: 0.053790]\n",
      "171 [D loss: 0.027374, acc: 100.00%, op_acc: 78.12%] [G loss: 0.058670, mse: 0.054742]\n",
      "172 [D loss: 0.021824, acc: 100.00%, op_acc: 90.62%] [G loss: 0.057436, mse: 0.052913]\n",
      "173 [D loss: 0.037594, acc: 96.88%, op_acc: 68.75%] [G loss: 0.057401, mse: 0.051573]\n",
      "174 [D loss: 0.032655, acc: 100.00%, op_acc: 87.50%] [G loss: 0.053752, mse: 0.048027]\n",
      "175 [D loss: 0.026950, acc: 100.00%, op_acc: 75.00%] [G loss: 0.058933, mse: 0.053248]\n",
      "176 [D loss: 0.022815, acc: 100.00%, op_acc: 78.12%] [G loss: 0.060915, mse: 0.054907]\n",
      "177 [D loss: 0.034672, acc: 100.00%, op_acc: 62.50%] [G loss: 0.047725, mse: 0.042426]\n",
      "178 [D loss: 0.032957, acc: 100.00%, op_acc: 68.75%] [G loss: 0.056378, mse: 0.050452]\n",
      "179 [D loss: 0.028286, acc: 100.00%, op_acc: 71.88%] [G loss: 0.055307, mse: 0.049435]\n",
      "180 [D loss: 0.021745, acc: 100.00%, op_acc: 84.38%] [G loss: 0.058359, mse: 0.052064]\n",
      "181 [D loss: 0.025804, acc: 100.00%, op_acc: 71.88%] [G loss: 0.064507, mse: 0.058446]\n",
      "182 [D loss: 0.092725, acc: 93.75%, op_acc: 68.75%] [G loss: 0.064486, mse: 0.059806]\n",
      "183 [D loss: 1.077688, acc: 53.12%, op_acc: 28.12%] [G loss: 0.065843, mse: 0.060738]\n",
      "184 [D loss: 0.174621, acc: 90.62%, op_acc: 78.12%] [G loss: 0.054465, mse: 0.050591]\n",
      "185 [D loss: 0.480539, acc: 71.88%, op_acc: 62.50%] [G loss: 0.049370, mse: 0.047781]\n",
      "186 [D loss: 0.194363, acc: 96.88%, op_acc: 75.00%] [G loss: 0.050888, mse: 0.049444]\n",
      "187 [D loss: 0.124631, acc: 100.00%, op_acc: 81.25%] [G loss: 0.057744, mse: 0.056162]\n",
      "188 [D loss: 0.115083, acc: 96.88%, op_acc: 62.50%] [G loss: 0.053138, mse: 0.051325]\n",
      "189 [D loss: 0.081580, acc: 100.00%, op_acc: 78.12%] [G loss: 0.056794, mse: 0.054590]\n",
      "190 [D loss: 0.063445, acc: 100.00%, op_acc: 75.00%] [G loss: 0.044109, mse: 0.041312]\n",
      "191 [D loss: 0.078214, acc: 96.88%, op_acc: 75.00%] [G loss: 0.054440, mse: 0.050915]\n",
      "192 [D loss: 0.038097, acc: 100.00%, op_acc: 87.50%] [G loss: 0.050516, mse: 0.045973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 [D loss: 0.076683, acc: 96.88%, op_acc: 71.88%] [G loss: 0.052065, mse: 0.047983]\n",
      "194 [D loss: 0.084460, acc: 93.75%, op_acc: 68.75%] [G loss: 0.049455, mse: 0.045266]\n",
      "195 [D loss: 0.039746, acc: 100.00%, op_acc: 75.00%] [G loss: 0.054676, mse: 0.049917]\n",
      "196 [D loss: 0.027963, acc: 100.00%, op_acc: 71.88%] [G loss: 0.061090, mse: 0.055905]\n",
      "197 [D loss: 0.091243, acc: 93.75%, op_acc: 84.38%] [G loss: 0.058782, mse: 0.055678]\n",
      "198 [D loss: 0.090251, acc: 90.62%, op_acc: 71.88%] [G loss: 0.063800, mse: 0.059528]\n",
      "199 [D loss: 0.025555, acc: 100.00%, op_acc: 87.50%] [G loss: 0.049518, mse: 0.044453]\n",
      "200 [D loss: 0.036531, acc: 100.00%, op_acc: 71.88%] [G loss: 0.056119, mse: 0.050894]\n",
      "201 [D loss: 0.025236, acc: 100.00%, op_acc: 81.25%] [G loss: 0.052912, mse: 0.047554]\n",
      "202 [D loss: 0.030587, acc: 100.00%, op_acc: 75.00%] [G loss: 0.050462, mse: 0.045040]\n",
      "203 [D loss: 0.025909, acc: 100.00%, op_acc: 81.25%] [G loss: 0.049798, mse: 0.044702]\n",
      "204 [D loss: 0.029167, acc: 100.00%, op_acc: 71.88%] [G loss: 0.058930, mse: 0.053708]\n",
      "205 [D loss: 0.027642, acc: 100.00%, op_acc: 78.12%] [G loss: 0.056096, mse: 0.050592]\n",
      "206 [D loss: 0.022600, acc: 100.00%, op_acc: 78.12%] [G loss: 0.065839, mse: 0.059703]\n",
      "207 [D loss: 0.024402, acc: 100.00%, op_acc: 81.25%] [G loss: 0.068361, mse: 0.062313]\n",
      "208 [D loss: 0.023241, acc: 100.00%, op_acc: 71.88%] [G loss: 0.070160, mse: 0.063970]\n",
      "209 [D loss: 0.148590, acc: 93.75%, op_acc: 71.88%] [G loss: 0.053050, mse: 0.047919]\n",
      "210 [D loss: 0.028523, acc: 100.00%, op_acc: 87.50%] [G loss: 0.047917, mse: 0.042376]\n",
      "211 [D loss: 0.038389, acc: 100.00%, op_acc: 68.75%] [G loss: 0.059884, mse: 0.054642]\n",
      "212 [D loss: 0.023639, acc: 100.00%, op_acc: 78.12%] [G loss: 0.061908, mse: 0.056170]\n",
      "213 [D loss: 0.023362, acc: 100.00%, op_acc: 75.00%] [G loss: 0.058107, mse: 0.052291]\n",
      "214 [D loss: 0.135323, acc: 96.88%, op_acc: 68.75%] [G loss: 0.054893, mse: 0.050354]\n",
      "215 [D loss: 0.040011, acc: 100.00%, op_acc: 75.00%] [G loss: 0.051465, mse: 0.046596]\n",
      "216 [D loss: 0.026422, acc: 100.00%, op_acc: 81.25%] [G loss: 0.054275, mse: 0.048664]\n",
      "217 [D loss: 0.024197, acc: 100.00%, op_acc: 75.00%] [G loss: 0.054037, mse: 0.048422]\n",
      "218 [D loss: 0.038509, acc: 100.00%, op_acc: 71.88%] [G loss: 0.054248, mse: 0.048650]\n",
      "219 [D loss: 0.086111, acc: 96.88%, op_acc: 68.75%] [G loss: 0.053032, mse: 0.049417]\n",
      "220 [D loss: 0.043753, acc: 96.88%, op_acc: 75.00%] [G loss: 0.052181, mse: 0.048129]\n",
      "221 [D loss: 0.039476, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050627, mse: 0.044908]\n",
      "222 [D loss: 0.027944, acc: 100.00%, op_acc: 71.88%] [G loss: 0.055834, mse: 0.049515]\n",
      "223 [D loss: 0.025569, acc: 100.00%, op_acc: 68.75%] [G loss: 0.058452, mse: 0.052096]\n",
      "224 [D loss: 0.068609, acc: 96.88%, op_acc: 65.62%] [G loss: 0.055646, mse: 0.051125]\n",
      "225 [D loss: 0.206177, acc: 87.50%, op_acc: 62.50%] [G loss: 0.057861, mse: 0.053328]\n",
      "226 [D loss: 0.039514, acc: 96.88%, op_acc: 84.38%] [G loss: 0.061428, mse: 0.056117]\n",
      "227 [D loss: 0.176911, acc: 96.88%, op_acc: 78.12%] [G loss: 0.056140, mse: 0.052861]\n",
      "228 [D loss: 0.038733, acc: 100.00%, op_acc: 71.88%] [G loss: 0.043465, mse: 0.039844]\n",
      "229 [D loss: 0.078604, acc: 93.75%, op_acc: 78.12%] [G loss: 0.046590, mse: 0.042435]\n",
      "230 [D loss: 0.030571, acc: 100.00%, op_acc: 65.62%] [G loss: 0.048911, mse: 0.044139]\n",
      "231 [D loss: 0.051845, acc: 96.88%, op_acc: 68.75%] [G loss: 0.050074, mse: 0.045652]\n",
      "232 [D loss: 0.034504, acc: 100.00%, op_acc: 71.88%] [G loss: 0.047910, mse: 0.043265]\n",
      "233 [D loss: 0.039379, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050034, mse: 0.045115]\n",
      "234 [D loss: 0.027123, acc: 100.00%, op_acc: 75.00%] [G loss: 0.048207, mse: 0.042869]\n",
      "235 [D loss: 0.024165, acc: 100.00%, op_acc: 71.88%] [G loss: 0.050763, mse: 0.045171]\n",
      "236 [D loss: 0.024384, acc: 100.00%, op_acc: 87.50%] [G loss: 0.057450, mse: 0.052217]\n",
      "237 [D loss: 0.152442, acc: 90.62%, op_acc: 78.12%] [G loss: 0.046466, mse: 0.042006]\n",
      "238 [D loss: 0.055040, acc: 100.00%, op_acc: 71.88%] [G loss: 0.046728, mse: 0.042681]\n",
      "239 [D loss: 0.079171, acc: 93.75%, op_acc: 75.00%] [G loss: 0.049199, mse: 0.044722]\n",
      "240 [D loss: 0.030784, acc: 100.00%, op_acc: 68.75%] [G loss: 0.054338, mse: 0.049007]\n",
      "241 [D loss: 0.024681, acc: 100.00%, op_acc: 78.12%] [G loss: 0.047995, mse: 0.042335]\n",
      "242 [D loss: 0.037220, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050054, mse: 0.045102]\n",
      "243 [D loss: 0.066835, acc: 96.88%, op_acc: 71.88%] [G loss: 0.050238, mse: 0.044802]\n",
      "244 [D loss: 0.156396, acc: 90.62%, op_acc: 68.75%] [G loss: 0.054931, mse: 0.049637]\n",
      "245 [D loss: 0.037255, acc: 100.00%, op_acc: 81.25%] [G loss: 0.058607, mse: 0.053348]\n",
      "246 [D loss: 0.107812, acc: 93.75%, op_acc: 75.00%] [G loss: 0.070101, mse: 0.065444]\n",
      "247 [D loss: 0.026241, acc: 100.00%, op_acc: 68.75%] [G loss: 0.068471, mse: 0.064048]\n",
      "248 [D loss: 0.025540, acc: 100.00%, op_acc: 75.00%] [G loss: 0.056533, mse: 0.051782]\n",
      "249 [D loss: 0.024982, acc: 100.00%, op_acc: 75.00%] [G loss: 0.047083, mse: 0.041857]\n",
      "250 [D loss: 0.070834, acc: 96.88%, op_acc: 68.75%] [G loss: 0.047904, mse: 0.044178]\n",
      "251 [D loss: 0.037882, acc: 100.00%, op_acc: 71.88%] [G loss: 0.047024, mse: 0.042688]\n",
      "252 [D loss: 0.049024, acc: 96.88%, op_acc: 65.62%] [G loss: 0.046089, mse: 0.040618]\n",
      "253 [D loss: 0.026521, acc: 100.00%, op_acc: 84.38%] [G loss: 0.052555, mse: 0.046514]\n",
      "254 [D loss: 0.031697, acc: 100.00%, op_acc: 78.12%] [G loss: 0.053667, mse: 0.047209]\n",
      "255 [D loss: 0.025241, acc: 100.00%, op_acc: 81.25%] [G loss: 0.059256, mse: 0.053134]\n",
      "256 [D loss: 0.027498, acc: 100.00%, op_acc: 78.12%] [G loss: 0.053311, mse: 0.047123]\n",
      "257 [D loss: 0.023611, acc: 100.00%, op_acc: 71.88%] [G loss: 0.051313, mse: 0.045171]\n",
      "258 [D loss: 0.030079, acc: 100.00%, op_acc: 65.62%] [G loss: 0.056186, mse: 0.050054]\n",
      "259 [D loss: 0.049283, acc: 96.88%, op_acc: 65.62%] [G loss: 0.049502, mse: 0.042784]\n",
      "260 [D loss: 0.024596, acc: 100.00%, op_acc: 75.00%] [G loss: 0.051150, mse: 0.043838]\n",
      "261 [D loss: 0.133070, acc: 93.75%, op_acc: 81.25%] [G loss: 0.047184, mse: 0.042476]\n",
      "262 [D loss: 0.089350, acc: 93.75%, op_acc: 75.00%] [G loss: 0.047141, mse: 0.041688]\n",
      "263 [D loss: 0.024996, acc: 100.00%, op_acc: 87.50%] [G loss: 0.045990, mse: 0.039655]\n",
      "264 [D loss: 0.025745, acc: 100.00%, op_acc: 78.12%] [G loss: 0.051760, mse: 0.045164]\n",
      "265 [D loss: 0.063245, acc: 93.75%, op_acc: 81.25%] [G loss: 0.045864, mse: 0.041432]\n",
      "266 [D loss: 0.066777, acc: 96.88%, op_acc: 75.00%] [G loss: 0.048638, mse: 0.044190]\n",
      "267 [D loss: 0.060983, acc: 96.88%, op_acc: 68.75%] [G loss: 0.052468, mse: 0.046536]\n",
      "268 [D loss: 0.060350, acc: 96.88%, op_acc: 78.12%] [G loss: 0.059749, mse: 0.054171]\n",
      "269 [D loss: 0.124917, acc: 93.75%, op_acc: 78.12%] [G loss: 0.063418, mse: 0.058837]\n",
      "270 [D loss: 0.075624, acc: 96.88%, op_acc: 65.62%] [G loss: 0.059253, mse: 0.055594]\n",
      "271 [D loss: 0.047256, acc: 100.00%, op_acc: 81.25%] [G loss: 0.051938, mse: 0.047420]\n",
      "272 [D loss: 0.035591, acc: 100.00%, op_acc: 75.00%] [G loss: 0.056990, mse: 0.051868]\n",
      "273 [D loss: 0.029417, acc: 100.00%, op_acc: 81.25%] [G loss: 0.046225, mse: 0.040549]\n",
      "274 [D loss: 0.187763, acc: 81.25%, op_acc: 62.50%] [G loss: 0.043133, mse: 0.039566]\n",
      "275 [D loss: 0.065178, acc: 93.75%, op_acc: 78.12%] [G loss: 0.045374, mse: 0.040632]\n",
      "276 [D loss: 0.034358, acc: 100.00%, op_acc: 71.88%] [G loss: 0.045705, mse: 0.040431]\n",
      "277 [D loss: 0.039029, acc: 100.00%, op_acc: 68.75%] [G loss: 0.047274, mse: 0.042085]\n",
      "278 [D loss: 0.108900, acc: 90.62%, op_acc: 65.62%] [G loss: 0.049353, mse: 0.044539]\n",
      "279 [D loss: 0.111125, acc: 93.75%, op_acc: 78.12%] [G loss: 0.048129, mse: 0.044923]\n",
      "280 [D loss: 0.081137, acc: 93.75%, op_acc: 62.50%] [G loss: 0.048583, mse: 0.043742]\n",
      "281 [D loss: 0.023087, acc: 100.00%, op_acc: 78.12%] [G loss: 0.054321, mse: 0.048505]\n",
      "282 [D loss: 0.041606, acc: 100.00%, op_acc: 81.25%] [G loss: 0.054298, mse: 0.048550]\n",
      "283 [D loss: 0.020217, acc: 100.00%, op_acc: 84.38%] [G loss: 0.053692, mse: 0.048248]\n",
      "284 [D loss: 0.029639, acc: 100.00%, op_acc: 65.62%] [G loss: 0.051722, mse: 0.045907]\n",
      "285 [D loss: 0.069224, acc: 96.88%, op_acc: 81.25%] [G loss: 0.060133, mse: 0.054854]\n",
      "286 [D loss: 0.071159, acc: 96.88%, op_acc: 81.25%] [G loss: 0.054921, mse: 0.048797]\n",
      "287 [D loss: 0.130006, acc: 93.75%, op_acc: 75.00%] [G loss: 0.049395, mse: 0.044823]\n",
      "288 [D loss: 0.042033, acc: 96.88%, op_acc: 78.12%] [G loss: 0.043341, mse: 0.038978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 [D loss: 0.028826, acc: 100.00%, op_acc: 90.62%] [G loss: 0.049286, mse: 0.044805]\n",
      "290 [D loss: 0.032355, acc: 100.00%, op_acc: 75.00%] [G loss: 0.044913, mse: 0.040208]\n",
      "291 [D loss: 0.026044, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050975, mse: 0.046335]\n",
      "292 [D loss: 0.037362, acc: 100.00%, op_acc: 81.25%] [G loss: 0.053847, mse: 0.047563]\n",
      "293 [D loss: 0.026894, acc: 100.00%, op_acc: 78.12%] [G loss: 0.051074, mse: 0.044217]\n",
      "294 [D loss: 0.062195, acc: 96.88%, op_acc: 78.12%] [G loss: 0.052492, mse: 0.046930]\n",
      "295 [D loss: 0.023755, acc: 100.00%, op_acc: 71.88%] [G loss: 0.046722, mse: 0.042056]\n",
      "296 [D loss: 0.027822, acc: 100.00%, op_acc: 75.00%] [G loss: 0.052077, mse: 0.046662]\n",
      "297 [D loss: 0.028266, acc: 100.00%, op_acc: 71.88%] [G loss: 0.045843, mse: 0.040063]\n",
      "298 [D loss: 0.026155, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050723, mse: 0.044287]\n",
      "299 [D loss: 0.026456, acc: 100.00%, op_acc: 75.00%] [G loss: 0.050785, mse: 0.044085]\n",
      "300 [D loss: 0.027735, acc: 100.00%, op_acc: 84.38%] [G loss: 0.053793, mse: 0.046132]\n",
      "301 [D loss: 0.042084, acc: 96.88%, op_acc: 75.00%] [G loss: 0.057905, mse: 0.051362]\n",
      "302 [D loss: 0.041004, acc: 96.88%, op_acc: 78.12%] [G loss: 0.055155, mse: 0.047801]\n",
      "303 [D loss: 0.021658, acc: 100.00%, op_acc: 81.25%] [G loss: 0.053687, mse: 0.046063]\n",
      "304 [D loss: 0.024565, acc: 100.00%, op_acc: 75.00%] [G loss: 0.054056, mse: 0.046490]\n",
      "305 [D loss: 0.127656, acc: 96.88%, op_acc: 68.75%] [G loss: 0.045897, mse: 0.040560]\n",
      "306 [D loss: 0.072659, acc: 96.88%, op_acc: 78.12%] [G loss: 0.053018, mse: 0.046847]\n",
      "307 [D loss: 0.031227, acc: 100.00%, op_acc: 68.75%] [G loss: 0.055695, mse: 0.048981]\n",
      "308 [D loss: 0.027905, acc: 100.00%, op_acc: 62.50%] [G loss: 0.055292, mse: 0.049046]\n",
      "309 [D loss: 0.147734, acc: 93.75%, op_acc: 68.75%] [G loss: 0.049960, mse: 0.046590]\n",
      "310 [D loss: 0.048899, acc: 96.88%, op_acc: 71.88%] [G loss: 0.044359, mse: 0.040344]\n",
      "311 [D loss: 0.066992, acc: 96.88%, op_acc: 75.00%] [G loss: 0.047634, mse: 0.043626]\n",
      "312 [D loss: 0.025048, acc: 100.00%, op_acc: 78.12%] [G loss: 0.051502, mse: 0.047177]\n",
      "313 [D loss: 0.110284, acc: 90.62%, op_acc: 65.62%] [G loss: 0.051252, mse: 0.048307]\n",
      "314 [D loss: 0.053347, acc: 100.00%, op_acc: 71.88%] [G loss: 0.046470, mse: 0.041479]\n",
      "315 [D loss: 0.022889, acc: 100.00%, op_acc: 84.38%] [G loss: 0.055349, mse: 0.048747]\n",
      "316 [D loss: 0.341469, acc: 75.00%, op_acc: 62.50%] [G loss: 0.056170, mse: 0.052448]\n",
      "317 [D loss: 0.107559, acc: 93.75%, op_acc: 65.62%] [G loss: 0.048411, mse: 0.045146]\n",
      "318 [D loss: 0.034818, acc: 100.00%, op_acc: 84.38%] [G loss: 0.052960, mse: 0.049580]\n",
      "319 [D loss: 0.039827, acc: 100.00%, op_acc: 81.25%] [G loss: 0.042068, mse: 0.038270]\n",
      "320 [D loss: 0.037763, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041655, mse: 0.037751]\n",
      "321 [D loss: 0.048041, acc: 96.88%, op_acc: 71.88%] [G loss: 0.043710, mse: 0.040175]\n",
      "322 [D loss: 0.038862, acc: 100.00%, op_acc: 81.25%] [G loss: 0.042196, mse: 0.037746]\n",
      "323 [D loss: 0.026246, acc: 100.00%, op_acc: 81.25%] [G loss: 0.050629, mse: 0.045573]\n",
      "324 [D loss: 0.034172, acc: 100.00%, op_acc: 68.75%] [G loss: 0.053132, mse: 0.046996]\n",
      "325 [D loss: 0.022886, acc: 100.00%, op_acc: 75.00%] [G loss: 0.049481, mse: 0.042372]\n",
      "326 [D loss: 0.072309, acc: 96.88%, op_acc: 78.12%] [G loss: 0.051198, mse: 0.048099]\n",
      "327 [D loss: 0.196843, acc: 87.50%, op_acc: 65.62%] [G loss: 0.052204, mse: 0.047688]\n",
      "328 [D loss: 0.035421, acc: 100.00%, op_acc: 68.75%] [G loss: 0.045819, mse: 0.039969]\n",
      "329 [D loss: 0.037972, acc: 96.88%, op_acc: 84.38%] [G loss: 0.045209, mse: 0.039539]\n",
      "330 [D loss: 0.032312, acc: 100.00%, op_acc: 68.75%] [G loss: 0.043976, mse: 0.038962]\n",
      "331 [D loss: 0.154962, acc: 93.75%, op_acc: 68.75%] [G loss: 0.040162, mse: 0.035679]\n",
      "332 [D loss: 0.029199, acc: 100.00%, op_acc: 65.62%] [G loss: 0.038972, mse: 0.034306]\n",
      "333 [D loss: 0.042065, acc: 100.00%, op_acc: 65.62%] [G loss: 0.047723, mse: 0.043112]\n",
      "334 [D loss: 0.026129, acc: 100.00%, op_acc: 87.50%] [G loss: 0.043076, mse: 0.038489]\n",
      "335 [D loss: 0.076506, acc: 96.88%, op_acc: 81.25%] [G loss: 0.047458, mse: 0.043993]\n",
      "336 [D loss: 0.037093, acc: 100.00%, op_acc: 78.12%] [G loss: 0.045158, mse: 0.040648]\n",
      "337 [D loss: 0.034447, acc: 100.00%, op_acc: 71.88%] [G loss: 0.043754, mse: 0.038490]\n",
      "338 [D loss: 0.036876, acc: 100.00%, op_acc: 78.12%] [G loss: 0.039186, mse: 0.033117]\n",
      "339 [D loss: 0.074340, acc: 96.88%, op_acc: 78.12%] [G loss: 0.042607, mse: 0.036196]\n",
      "340 [D loss: 0.206937, acc: 93.75%, op_acc: 84.38%] [G loss: 0.044092, mse: 0.039527]\n",
      "341 [D loss: 0.056382, acc: 96.88%, op_acc: 62.50%] [G loss: 0.042774, mse: 0.038304]\n",
      "342 [D loss: 0.027600, acc: 100.00%, op_acc: 78.12%] [G loss: 0.045867, mse: 0.040853]\n",
      "343 [D loss: 0.112321, acc: 93.75%, op_acc: 75.00%] [G loss: 0.041548, mse: 0.038567]\n",
      "344 [D loss: 0.063597, acc: 96.88%, op_acc: 84.38%] [G loss: 0.045979, mse: 0.042171]\n",
      "345 [D loss: 0.031140, acc: 100.00%, op_acc: 71.88%] [G loss: 0.042144, mse: 0.036776]\n",
      "346 [D loss: 0.099286, acc: 96.88%, op_acc: 84.38%] [G loss: 0.042983, mse: 0.038573]\n",
      "347 [D loss: 0.105656, acc: 90.62%, op_acc: 68.75%] [G loss: 0.039633, mse: 0.036809]\n",
      "348 [D loss: 0.099297, acc: 93.75%, op_acc: 59.38%] [G loss: 0.037229, mse: 0.032419]\n",
      "349 [D loss: 0.122124, acc: 93.75%, op_acc: 71.88%] [G loss: 0.047417, mse: 0.043517]\n",
      "350 [D loss: 0.029230, acc: 100.00%, op_acc: 81.25%] [G loss: 0.054719, mse: 0.049945]\n",
      "351 [D loss: 0.065955, acc: 100.00%, op_acc: 87.50%] [G loss: 0.080593, mse: 0.075966]\n",
      "352 [D loss: 0.114582, acc: 93.75%, op_acc: 71.88%] [G loss: 0.083189, mse: 0.079645]\n",
      "353 [D loss: 0.054828, acc: 93.75%, op_acc: 78.12%] [G loss: 0.052997, mse: 0.048868]\n",
      "354 [D loss: 0.030059, acc: 100.00%, op_acc: 68.75%] [G loss: 0.061713, mse: 0.057098]\n",
      "355 [D loss: 0.060660, acc: 96.88%, op_acc: 75.00%] [G loss: 0.053494, mse: 0.048895]\n",
      "356 [D loss: 0.065117, acc: 96.88%, op_acc: 71.88%] [G loss: 0.052290, mse: 0.047462]\n",
      "357 [D loss: 0.044601, acc: 96.88%, op_acc: 75.00%] [G loss: 0.051782, mse: 0.047646]\n",
      "358 [D loss: 0.032119, acc: 100.00%, op_acc: 78.12%] [G loss: 0.048535, mse: 0.044009]\n",
      "359 [D loss: 0.042566, acc: 100.00%, op_acc: 68.75%] [G loss: 0.045685, mse: 0.039720]\n",
      "360 [D loss: 0.023288, acc: 100.00%, op_acc: 81.25%] [G loss: 0.048203, mse: 0.041746]\n",
      "361 [D loss: 0.106796, acc: 90.62%, op_acc: 62.50%] [G loss: 0.046361, mse: 0.042147]\n",
      "362 [D loss: 0.063931, acc: 100.00%, op_acc: 68.75%] [G loss: 0.042689, mse: 0.036737]\n",
      "363 [D loss: 0.022887, acc: 100.00%, op_acc: 84.38%] [G loss: 0.045187, mse: 0.038330]\n",
      "364 [D loss: 0.085916, acc: 96.88%, op_acc: 81.25%] [G loss: 0.044363, mse: 0.038660]\n",
      "365 [D loss: 0.028188, acc: 100.00%, op_acc: 78.12%] [G loss: 0.042161, mse: 0.037756]\n",
      "366 [D loss: 0.030388, acc: 100.00%, op_acc: 75.00%] [G loss: 0.044702, mse: 0.039273]\n",
      "367 [D loss: 0.034604, acc: 100.00%, op_acc: 75.00%] [G loss: 0.049426, mse: 0.044081]\n",
      "368 [D loss: 0.249479, acc: 81.25%, op_acc: 65.62%] [G loss: 0.046574, mse: 0.041253]\n",
      "369 [D loss: 0.025980, acc: 100.00%, op_acc: 84.38%] [G loss: 0.044718, mse: 0.038505]\n",
      "370 [D loss: 0.093231, acc: 93.75%, op_acc: 68.75%] [G loss: 0.039153, mse: 0.034890]\n",
      "371 [D loss: 0.186140, acc: 84.38%, op_acc: 65.62%] [G loss: 0.041523, mse: 0.037979]\n",
      "372 [D loss: 0.053201, acc: 100.00%, op_acc: 75.00%] [G loss: 0.044985, mse: 0.040284]\n",
      "373 [D loss: 0.064535, acc: 96.88%, op_acc: 87.50%] [G loss: 0.046858, mse: 0.042268]\n",
      "374 [D loss: 0.039183, acc: 100.00%, op_acc: 81.25%] [G loss: 0.043433, mse: 0.038747]\n",
      "375 [D loss: 0.219581, acc: 81.25%, op_acc: 65.62%] [G loss: 0.049454, mse: 0.045092]\n",
      "376 [D loss: 0.086445, acc: 96.88%, op_acc: 78.12%] [G loss: 0.041770, mse: 0.037018]\n",
      "377 [D loss: 0.035577, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043844, mse: 0.038965]\n",
      "378 [D loss: 0.110434, acc: 96.88%, op_acc: 75.00%] [G loss: 0.041454, mse: 0.037598]\n",
      "379 [D loss: 0.041124, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043538, mse: 0.039951]\n",
      "380 [D loss: 0.108477, acc: 87.50%, op_acc: 75.00%] [G loss: 0.041962, mse: 0.038679]\n",
      "381 [D loss: 0.133506, acc: 93.75%, op_acc: 65.62%] [G loss: 0.040731, mse: 0.037652]\n",
      "382 [D loss: 0.043982, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041357, mse: 0.037484]\n",
      "383 [D loss: 0.035399, acc: 100.00%, op_acc: 75.00%] [G loss: 0.041226, mse: 0.036528]\n",
      "384 [D loss: 0.167277, acc: 87.50%, op_acc: 59.38%] [G loss: 0.041361, mse: 0.037538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 [D loss: 0.050204, acc: 96.88%, op_acc: 71.88%] [G loss: 0.040770, mse: 0.035735]\n",
      "386 [D loss: 0.079529, acc: 96.88%, op_acc: 75.00%] [G loss: 0.046321, mse: 0.041431]\n",
      "387 [D loss: 0.056313, acc: 96.88%, op_acc: 71.88%] [G loss: 0.044855, mse: 0.040403]\n",
      "388 [D loss: 0.048814, acc: 96.88%, op_acc: 78.12%] [G loss: 0.038950, mse: 0.034290]\n",
      "389 [D loss: 0.028537, acc: 100.00%, op_acc: 68.75%] [G loss: 0.044433, mse: 0.039195]\n",
      "390 [D loss: 0.139002, acc: 93.75%, op_acc: 68.75%] [G loss: 0.036710, mse: 0.033424]\n",
      "391 [D loss: 0.073109, acc: 93.75%, op_acc: 62.50%] [G loss: 0.042099, mse: 0.037892]\n",
      "392 [D loss: 0.035428, acc: 100.00%, op_acc: 84.38%] [G loss: 0.035886, mse: 0.031166]\n",
      "393 [D loss: 0.102137, acc: 93.75%, op_acc: 62.50%] [G loss: 0.040192, mse: 0.036079]\n",
      "394 [D loss: 0.108432, acc: 93.75%, op_acc: 81.25%] [G loss: 0.039404, mse: 0.035167]\n",
      "395 [D loss: 0.037579, acc: 100.00%, op_acc: 71.88%] [G loss: 0.045452, mse: 0.040021]\n",
      "396 [D loss: 0.023682, acc: 100.00%, op_acc: 81.25%] [G loss: 0.046504, mse: 0.040257]\n",
      "397 [D loss: 0.119314, acc: 93.75%, op_acc: 81.25%] [G loss: 0.052568, mse: 0.049471]\n",
      "398 [D loss: 0.045126, acc: 100.00%, op_acc: 81.25%] [G loss: 0.056969, mse: 0.052690]\n",
      "399 [D loss: 0.024292, acc: 100.00%, op_acc: 87.50%] [G loss: 0.059766, mse: 0.054442]\n",
      "400 [D loss: 0.058787, acc: 96.88%, op_acc: 75.00%] [G loss: 0.050152, mse: 0.044619]\n",
      "401 [D loss: 0.021873, acc: 100.00%, op_acc: 78.12%] [G loss: 0.045599, mse: 0.039597]\n",
      "402 [D loss: 0.037639, acc: 100.00%, op_acc: 81.25%] [G loss: 0.044875, mse: 0.038931]\n",
      "403 [D loss: 0.103094, acc: 96.88%, op_acc: 81.25%] [G loss: 0.045513, mse: 0.040800]\n",
      "404 [D loss: 0.051053, acc: 100.00%, op_acc: 78.12%] [G loss: 0.044609, mse: 0.038119]\n",
      "405 [D loss: 0.030860, acc: 100.00%, op_acc: 81.25%] [G loss: 0.049472, mse: 0.041547]\n",
      "406 [D loss: 0.162352, acc: 93.75%, op_acc: 65.62%] [G loss: 0.050485, mse: 0.047009]\n",
      "407 [D loss: 0.077758, acc: 96.88%, op_acc: 59.38%] [G loss: 0.044146, mse: 0.040064]\n",
      "408 [D loss: 0.048041, acc: 100.00%, op_acc: 81.25%] [G loss: 0.036967, mse: 0.031999]\n",
      "409 [D loss: 0.056105, acc: 96.88%, op_acc: 78.12%] [G loss: 0.044638, mse: 0.040231]\n",
      "410 [D loss: 0.098708, acc: 93.75%, op_acc: 78.12%] [G loss: 0.046755, mse: 0.042599]\n",
      "411 [D loss: 0.029945, acc: 100.00%, op_acc: 84.38%] [G loss: 0.041606, mse: 0.037184]\n",
      "412 [D loss: 0.048339, acc: 96.88%, op_acc: 84.38%] [G loss: 0.045476, mse: 0.040053]\n",
      "413 [D loss: 0.064597, acc: 96.88%, op_acc: 71.88%] [G loss: 0.043060, mse: 0.038402]\n",
      "414 [D loss: 0.042835, acc: 100.00%, op_acc: 81.25%] [G loss: 0.040607, mse: 0.035446]\n",
      "415 [D loss: 0.023879, acc: 100.00%, op_acc: 81.25%] [G loss: 0.042312, mse: 0.037537]\n",
      "416 [D loss: 0.213102, acc: 81.25%, op_acc: 65.62%] [G loss: 0.044788, mse: 0.040933]\n",
      "417 [D loss: 0.024576, acc: 100.00%, op_acc: 84.38%] [G loss: 0.052797, mse: 0.048199]\n",
      "418 [D loss: 0.033608, acc: 100.00%, op_acc: 87.50%] [G loss: 0.054911, mse: 0.050522]\n",
      "419 [D loss: 0.044628, acc: 96.88%, op_acc: 84.38%] [G loss: 0.048760, mse: 0.043844]\n",
      "420 [D loss: 0.033486, acc: 100.00%, op_acc: 81.25%] [G loss: 0.046362, mse: 0.041455]\n",
      "421 [D loss: 0.034193, acc: 100.00%, op_acc: 78.12%] [G loss: 0.046452, mse: 0.041384]\n",
      "422 [D loss: 0.028367, acc: 100.00%, op_acc: 75.00%] [G loss: 0.042699, mse: 0.037147]\n",
      "423 [D loss: 0.026234, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043104, mse: 0.037480]\n",
      "424 [D loss: 0.026356, acc: 100.00%, op_acc: 84.38%] [G loss: 0.044245, mse: 0.038069]\n",
      "425 [D loss: 0.030732, acc: 100.00%, op_acc: 68.75%] [G loss: 0.045573, mse: 0.038698]\n",
      "426 [D loss: 0.156486, acc: 93.75%, op_acc: 71.88%] [G loss: 0.043073, mse: 0.037729]\n",
      "427 [D loss: 0.029126, acc: 100.00%, op_acc: 84.38%] [G loss: 0.045743, mse: 0.040659]\n",
      "428 [D loss: 0.037304, acc: 100.00%, op_acc: 78.12%] [G loss: 0.041790, mse: 0.035610]\n",
      "429 [D loss: 0.022716, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043900, mse: 0.037740]\n",
      "430 [D loss: 0.043117, acc: 96.88%, op_acc: 71.88%] [G loss: 0.039142, mse: 0.034018]\n",
      "431 [D loss: 0.021876, acc: 100.00%, op_acc: 78.12%] [G loss: 0.042364, mse: 0.037298]\n",
      "432 [D loss: 0.026197, acc: 100.00%, op_acc: 81.25%] [G loss: 0.040497, mse: 0.035252]\n",
      "433 [D loss: 0.026041, acc: 100.00%, op_acc: 84.38%] [G loss: 0.038704, mse: 0.032747]\n",
      "434 [D loss: 0.032045, acc: 100.00%, op_acc: 78.12%] [G loss: 0.044936, mse: 0.038840]\n",
      "435 [D loss: 0.024934, acc: 100.00%, op_acc: 71.88%] [G loss: 0.045196, mse: 0.038627]\n",
      "436 [D loss: 0.064793, acc: 96.88%, op_acc: 84.38%] [G loss: 0.049422, mse: 0.042792]\n",
      "437 [D loss: 0.021676, acc: 100.00%, op_acc: 78.12%] [G loss: 0.046046, mse: 0.038903]\n",
      "438 [D loss: 0.082344, acc: 96.88%, op_acc: 78.12%] [G loss: 0.045291, mse: 0.040405]\n",
      "439 [D loss: 0.166298, acc: 90.62%, op_acc: 65.62%] [G loss: 0.048945, mse: 0.041911]\n",
      "440 [D loss: 0.085754, acc: 93.75%, op_acc: 75.00%] [G loss: 0.046506, mse: 0.041896]\n",
      "441 [D loss: 0.083868, acc: 96.88%, op_acc: 81.25%] [G loss: 0.042718, mse: 0.037544]\n",
      "442 [D loss: 0.026235, acc: 100.00%, op_acc: 68.75%] [G loss: 0.041093, mse: 0.035551]\n",
      "443 [D loss: 0.034941, acc: 100.00%, op_acc: 75.00%] [G loss: 0.044534, mse: 0.038838]\n",
      "444 [D loss: 0.083705, acc: 93.75%, op_acc: 75.00%] [G loss: 0.036713, mse: 0.033362]\n",
      "445 [D loss: 0.377862, acc: 78.12%, op_acc: 62.50%] [G loss: 0.046173, mse: 0.039007]\n",
      "446 [D loss: 0.371861, acc: 75.00%, op_acc: 65.62%] [G loss: 0.038671, mse: 0.036409]\n",
      "447 [D loss: 0.132732, acc: 90.62%, op_acc: 75.00%] [G loss: 0.040595, mse: 0.038532]\n",
      "448 [D loss: 0.058904, acc: 100.00%, op_acc: 78.12%] [G loss: 0.047703, mse: 0.044555]\n",
      "449 [D loss: 0.061577, acc: 100.00%, op_acc: 71.88%] [G loss: 0.039094, mse: 0.035441]\n",
      "450 [D loss: 0.046749, acc: 100.00%, op_acc: 71.88%] [G loss: 0.041500, mse: 0.037599]\n",
      "451 [D loss: 0.044930, acc: 100.00%, op_acc: 71.88%] [G loss: 0.037601, mse: 0.033559]\n",
      "452 [D loss: 0.037618, acc: 100.00%, op_acc: 68.75%] [G loss: 0.040566, mse: 0.036435]\n",
      "453 [D loss: 0.113889, acc: 93.75%, op_acc: 68.75%] [G loss: 0.039113, mse: 0.035554]\n",
      "454 [D loss: 0.050444, acc: 100.00%, op_acc: 68.75%] [G loss: 0.037382, mse: 0.032989]\n",
      "455 [D loss: 0.028111, acc: 100.00%, op_acc: 81.25%] [G loss: 0.040268, mse: 0.035295]\n",
      "456 [D loss: 0.028054, acc: 100.00%, op_acc: 93.75%] [G loss: 0.044046, mse: 0.038798]\n",
      "457 [D loss: 0.112109, acc: 96.88%, op_acc: 75.00%] [G loss: 0.038953, mse: 0.034465]\n",
      "458 [D loss: 0.040318, acc: 100.00%, op_acc: 65.62%] [G loss: 0.038794, mse: 0.034180]\n",
      "459 [D loss: 0.055004, acc: 100.00%, op_acc: 87.50%] [G loss: 0.039007, mse: 0.034678]\n",
      "460 [D loss: 0.028298, acc: 100.00%, op_acc: 81.25%] [G loss: 0.040957, mse: 0.036132]\n",
      "461 [D loss: 0.028564, acc: 100.00%, op_acc: 65.62%] [G loss: 0.042386, mse: 0.037506]\n",
      "462 [D loss: 0.063874, acc: 96.88%, op_acc: 87.50%] [G loss: 0.040749, mse: 0.035627]\n",
      "463 [D loss: 0.023643, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041499, mse: 0.036039]\n",
      "464 [D loss: 0.024210, acc: 100.00%, op_acc: 81.25%] [G loss: 0.049853, mse: 0.044304]\n",
      "465 [D loss: 0.203505, acc: 90.62%, op_acc: 75.00%] [G loss: 0.040726, mse: 0.037930]\n",
      "466 [D loss: 0.109105, acc: 93.75%, op_acc: 71.88%] [G loss: 0.044653, mse: 0.040585]\n",
      "467 [D loss: 0.029664, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043801, mse: 0.038477]\n",
      "468 [D loss: 0.084654, acc: 93.75%, op_acc: 81.25%] [G loss: 0.038328, mse: 0.034251]\n",
      "469 [D loss: 0.034867, acc: 100.00%, op_acc: 81.25%] [G loss: 0.039147, mse: 0.035065]\n",
      "470 [D loss: 0.052584, acc: 96.88%, op_acc: 71.88%] [G loss: 0.037808, mse: 0.033396]\n",
      "471 [D loss: 0.026231, acc: 100.00%, op_acc: 84.38%] [G loss: 0.047430, mse: 0.042736]\n",
      "472 [D loss: 0.033091, acc: 100.00%, op_acc: 78.12%] [G loss: 0.044034, mse: 0.038841]\n",
      "473 [D loss: 0.069149, acc: 96.88%, op_acc: 78.12%] [G loss: 0.040543, mse: 0.035532]\n",
      "474 [D loss: 0.022864, acc: 100.00%, op_acc: 81.25%] [G loss: 0.046340, mse: 0.040465]\n",
      "475 [D loss: 0.039220, acc: 100.00%, op_acc: 81.25%] [G loss: 0.039834, mse: 0.033923]\n",
      "476 [D loss: 0.031489, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043420, mse: 0.037866]\n",
      "477 [D loss: 0.022621, acc: 100.00%, op_acc: 78.12%] [G loss: 0.045073, mse: 0.039827]\n",
      "478 [D loss: 0.094591, acc: 93.75%, op_acc: 65.62%] [G loss: 0.043661, mse: 0.038005]\n",
      "479 [D loss: 0.025910, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050333, mse: 0.043661]\n",
      "480 [D loss: 0.033660, acc: 100.00%, op_acc: 81.25%] [G loss: 0.053223, mse: 0.046625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481 [D loss: 0.031099, acc: 100.00%, op_acc: 75.00%] [G loss: 0.042434, mse: 0.036164]\n",
      "482 [D loss: 0.078891, acc: 93.75%, op_acc: 71.88%] [G loss: 0.048927, mse: 0.044492]\n",
      "483 [D loss: 0.052710, acc: 96.88%, op_acc: 65.62%] [G loss: 0.040849, mse: 0.035188]\n",
      "484 [D loss: 0.025394, acc: 100.00%, op_acc: 75.00%] [G loss: 0.040264, mse: 0.034380]\n",
      "485 [D loss: 0.143939, acc: 93.75%, op_acc: 78.12%] [G loss: 0.038994, mse: 0.032779]\n",
      "486 [D loss: 0.020558, acc: 100.00%, op_acc: 81.25%] [G loss: 0.040882, mse: 0.034543]\n",
      "487 [D loss: 0.065803, acc: 96.88%, op_acc: 78.12%] [G loss: 0.039240, mse: 0.034301]\n",
      "488 [D loss: 0.036803, acc: 100.00%, op_acc: 87.50%] [G loss: 0.045328, mse: 0.039063]\n",
      "489 [D loss: 0.079768, acc: 96.88%, op_acc: 59.38%] [G loss: 0.037518, mse: 0.032738]\n",
      "490 [D loss: 0.096419, acc: 90.62%, op_acc: 75.00%] [G loss: 0.036763, mse: 0.032022]\n",
      "491 [D loss: 0.050524, acc: 96.88%, op_acc: 84.38%] [G loss: 0.038472, mse: 0.032670]\n",
      "492 [D loss: 0.035273, acc: 100.00%, op_acc: 75.00%] [G loss: 0.040455, mse: 0.034077]\n",
      "493 [D loss: 0.024240, acc: 100.00%, op_acc: 78.12%] [G loss: 0.044217, mse: 0.037728]\n",
      "494 [D loss: 0.023644, acc: 100.00%, op_acc: 81.25%] [G loss: 0.050287, mse: 0.043741]\n",
      "495 [D loss: 0.068965, acc: 96.88%, op_acc: 78.12%] [G loss: 0.044705, mse: 0.040232]\n",
      "496 [D loss: 0.044420, acc: 100.00%, op_acc: 75.00%] [G loss: 0.044014, mse: 0.038570]\n",
      "497 [D loss: 0.322807, acc: 84.38%, op_acc: 71.88%] [G loss: 0.036446, mse: 0.031444]\n",
      "498 [D loss: 0.029764, acc: 100.00%, op_acc: 81.25%] [G loss: 0.039794, mse: 0.034224]\n",
      "499 [D loss: 0.245546, acc: 87.50%, op_acc: 75.00%] [G loss: 0.038442, mse: 0.036619]\n",
      "500 [D loss: 0.169682, acc: 84.38%, op_acc: 62.50%] [G loss: 0.038575, mse: 0.035001]\n",
      "501 [D loss: 0.027696, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041547, mse: 0.036712]\n",
      "502 [D loss: 0.154918, acc: 87.50%, op_acc: 65.62%] [G loss: 0.037755, mse: 0.035168]\n",
      "503 [D loss: 0.063229, acc: 100.00%, op_acc: 75.00%] [G loss: 0.034846, mse: 0.031685]\n",
      "504 [D loss: 0.080319, acc: 93.75%, op_acc: 65.62%] [G loss: 0.038892, mse: 0.033679]\n",
      "505 [D loss: 0.026668, acc: 100.00%, op_acc: 78.12%] [G loss: 0.036728, mse: 0.030516]\n",
      "506 [D loss: 0.089330, acc: 93.75%, op_acc: 68.75%] [G loss: 0.033740, mse: 0.029000]\n",
      "507 [D loss: 0.043834, acc: 100.00%, op_acc: 78.12%] [G loss: 0.037748, mse: 0.033481]\n",
      "508 [D loss: 0.032965, acc: 100.00%, op_acc: 65.62%] [G loss: 0.041897, mse: 0.037602]\n",
      "509 [D loss: 0.069978, acc: 96.88%, op_acc: 75.00%] [G loss: 0.043357, mse: 0.037502]\n",
      "510 [D loss: 0.039418, acc: 100.00%, op_acc: 81.25%] [G loss: 0.042190, mse: 0.035865]\n",
      "511 [D loss: 0.234591, acc: 90.62%, op_acc: 65.62%] [G loss: 0.043937, mse: 0.040667]\n",
      "512 [D loss: 0.118603, acc: 90.62%, op_acc: 68.75%] [G loss: 0.046460, mse: 0.042906]\n",
      "513 [D loss: 0.029649, acc: 100.00%, op_acc: 78.12%] [G loss: 0.050993, mse: 0.046682]\n",
      "514 [D loss: 0.027998, acc: 100.00%, op_acc: 90.62%] [G loss: 0.043848, mse: 0.038959]\n",
      "515 [D loss: 0.055596, acc: 96.88%, op_acc: 78.12%] [G loss: 0.040659, mse: 0.035435]\n",
      "516 [D loss: 0.097546, acc: 93.75%, op_acc: 75.00%] [G loss: 0.038461, mse: 0.034517]\n",
      "517 [D loss: 0.054426, acc: 100.00%, op_acc: 78.12%] [G loss: 0.038712, mse: 0.034178]\n",
      "518 [D loss: 0.047393, acc: 96.88%, op_acc: 75.00%] [G loss: 0.041950, mse: 0.037269]\n",
      "519 [D loss: 0.029644, acc: 100.00%, op_acc: 87.50%] [G loss: 0.046083, mse: 0.041329]\n",
      "520 [D loss: 0.026426, acc: 100.00%, op_acc: 75.00%] [G loss: 0.037848, mse: 0.032696]\n",
      "521 [D loss: 0.029372, acc: 100.00%, op_acc: 84.38%] [G loss: 0.045373, mse: 0.040568]\n",
      "522 [D loss: 0.022228, acc: 100.00%, op_acc: 93.75%] [G loss: 0.042465, mse: 0.037262]\n",
      "523 [D loss: 0.045182, acc: 96.88%, op_acc: 75.00%] [G loss: 0.047476, mse: 0.043509]\n",
      "524 [D loss: 0.066240, acc: 96.88%, op_acc: 71.88%] [G loss: 0.042439, mse: 0.036555]\n",
      "525 [D loss: 0.031128, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041693, mse: 0.034872]\n",
      "526 [D loss: 0.054565, acc: 93.75%, op_acc: 90.62%] [G loss: 0.040059, mse: 0.034805]\n",
      "527 [D loss: 0.029856, acc: 100.00%, op_acc: 84.38%] [G loss: 0.040200, mse: 0.035647]\n",
      "528 [D loss: 0.029566, acc: 100.00%, op_acc: 71.88%] [G loss: 0.038830, mse: 0.033186]\n",
      "529 [D loss: 0.059949, acc: 96.88%, op_acc: 71.88%] [G loss: 0.038643, mse: 0.031920]\n",
      "530 [D loss: 0.031961, acc: 100.00%, op_acc: 90.62%] [G loss: 0.040517, mse: 0.033154]\n",
      "531 [D loss: 0.036520, acc: 100.00%, op_acc: 71.88%] [G loss: 0.039573, mse: 0.033132]\n",
      "532 [D loss: 0.191274, acc: 87.50%, op_acc: 75.00%] [G loss: 0.038863, mse: 0.033898]\n",
      "533 [D loss: 0.127192, acc: 93.75%, op_acc: 68.75%] [G loss: 0.035053, mse: 0.030373]\n",
      "534 [D loss: 0.039678, acc: 96.88%, op_acc: 81.25%] [G loss: 0.037352, mse: 0.032755]\n",
      "535 [D loss: 0.034425, acc: 100.00%, op_acc: 78.12%] [G loss: 0.040818, mse: 0.036060]\n",
      "536 [D loss: 0.026397, acc: 100.00%, op_acc: 78.12%] [G loss: 0.042326, mse: 0.036893]\n",
      "537 [D loss: 0.068162, acc: 96.88%, op_acc: 71.88%] [G loss: 0.040043, mse: 0.035698]\n",
      "538 [D loss: 0.118101, acc: 93.75%, op_acc: 71.88%] [G loss: 0.042879, mse: 0.038597]\n",
      "539 [D loss: 0.017280, acc: 100.00%, op_acc: 93.75%] [G loss: 0.042390, mse: 0.036769]\n",
      "540 [D loss: 0.062480, acc: 93.75%, op_acc: 84.38%] [G loss: 0.037669, mse: 0.033441]\n",
      "541 [D loss: 0.030390, acc: 100.00%, op_acc: 75.00%] [G loss: 0.040369, mse: 0.036294]\n",
      "542 [D loss: 0.046099, acc: 100.00%, op_acc: 81.25%] [G loss: 0.039161, mse: 0.034654]\n",
      "543 [D loss: 0.056170, acc: 96.88%, op_acc: 65.62%] [G loss: 0.039525, mse: 0.033295]\n",
      "544 [D loss: 0.017709, acc: 100.00%, op_acc: 87.50%] [G loss: 0.045068, mse: 0.037821]\n",
      "545 [D loss: 0.131399, acc: 93.75%, op_acc: 65.62%] [G loss: 0.037035, mse: 0.031825]\n",
      "546 [D loss: 0.051909, acc: 100.00%, op_acc: 75.00%] [G loss: 0.039549, mse: 0.035281]\n",
      "547 [D loss: 0.090377, acc: 93.75%, op_acc: 75.00%] [G loss: 0.041801, mse: 0.035656]\n",
      "548 [D loss: 0.025844, acc: 100.00%, op_acc: 78.12%] [G loss: 0.036998, mse: 0.030220]\n",
      "549 [D loss: 0.137544, acc: 93.75%, op_acc: 78.12%] [G loss: 0.039021, mse: 0.034540]\n",
      "550 [D loss: 0.046436, acc: 100.00%, op_acc: 59.38%] [G loss: 0.040785, mse: 0.036727]\n",
      "551 [D loss: 0.101030, acc: 93.75%, op_acc: 68.75%] [G loss: 0.041264, mse: 0.037232]\n",
      "552 [D loss: 0.024111, acc: 100.00%, op_acc: 81.25%] [G loss: 0.044011, mse: 0.038938]\n",
      "553 [D loss: 0.042929, acc: 100.00%, op_acc: 71.88%] [G loss: 0.040846, mse: 0.036013]\n",
      "554 [D loss: 0.034758, acc: 100.00%, op_acc: 84.38%] [G loss: 0.042497, mse: 0.037782]\n",
      "555 [D loss: 0.075035, acc: 93.75%, op_acc: 65.62%] [G loss: 0.040685, mse: 0.036368]\n",
      "556 [D loss: 0.035750, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041077, mse: 0.036546]\n",
      "557 [D loss: 0.102517, acc: 96.88%, op_acc: 78.12%] [G loss: 0.044523, mse: 0.040816]\n",
      "558 [D loss: 0.113040, acc: 87.50%, op_acc: 65.62%] [G loss: 0.037318, mse: 0.030947]\n",
      "559 [D loss: 0.129016, acc: 90.62%, op_acc: 68.75%] [G loss: 0.035231, mse: 0.030342]\n",
      "560 [D loss: 0.117332, acc: 93.75%, op_acc: 75.00%] [G loss: 0.038964, mse: 0.035339]\n",
      "561 [D loss: 0.112931, acc: 90.62%, op_acc: 71.88%] [G loss: 0.037299, mse: 0.033933]\n",
      "562 [D loss: 0.053095, acc: 96.88%, op_acc: 75.00%] [G loss: 0.037189, mse: 0.033354]\n",
      "563 [D loss: 0.030301, acc: 100.00%, op_acc: 75.00%] [G loss: 0.038433, mse: 0.033998]\n",
      "564 [D loss: 0.030651, acc: 100.00%, op_acc: 68.75%] [G loss: 0.037369, mse: 0.032658]\n",
      "565 [D loss: 0.134420, acc: 93.75%, op_acc: 68.75%] [G loss: 0.040090, mse: 0.036806]\n",
      "566 [D loss: 0.034733, acc: 100.00%, op_acc: 78.12%] [G loss: 0.041675, mse: 0.038096]\n",
      "567 [D loss: 0.026419, acc: 100.00%, op_acc: 87.50%] [G loss: 0.042466, mse: 0.037643]\n",
      "568 [D loss: 0.023877, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041316, mse: 0.035897]\n",
      "569 [D loss: 0.028534, acc: 100.00%, op_acc: 68.75%] [G loss: 0.036665, mse: 0.031015]\n",
      "570 [D loss: 0.082459, acc: 93.75%, op_acc: 75.00%] [G loss: 0.039756, mse: 0.035157]\n",
      "571 [D loss: 0.035013, acc: 100.00%, op_acc: 78.12%] [G loss: 0.032978, mse: 0.028223]\n",
      "572 [D loss: 0.066155, acc: 93.75%, op_acc: 56.25%] [G loss: 0.032523, mse: 0.027345]\n",
      "573 [D loss: 0.026798, acc: 100.00%, op_acc: 78.12%] [G loss: 0.037084, mse: 0.031834]\n",
      "574 [D loss: 0.046450, acc: 96.88%, op_acc: 75.00%] [G loss: 0.040029, mse: 0.033187]\n",
      "575 [D loss: 0.148661, acc: 90.62%, op_acc: 68.75%] [G loss: 0.037423, mse: 0.033169]\n",
      "576 [D loss: 0.091046, acc: 90.62%, op_acc: 68.75%] [G loss: 0.036409, mse: 0.032407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 [D loss: 0.034698, acc: 100.00%, op_acc: 68.75%] [G loss: 0.037153, mse: 0.032350]\n",
      "578 [D loss: 0.054550, acc: 96.88%, op_acc: 71.88%] [G loss: 0.041525, mse: 0.036799]\n",
      "579 [D loss: 0.035864, acc: 100.00%, op_acc: 71.88%] [G loss: 0.039578, mse: 0.034533]\n",
      "580 [D loss: 0.096588, acc: 96.88%, op_acc: 68.75%] [G loss: 0.033517, mse: 0.029506]\n",
      "581 [D loss: 0.058470, acc: 96.88%, op_acc: 78.12%] [G loss: 0.037035, mse: 0.033224]\n",
      "582 [D loss: 0.063327, acc: 96.88%, op_acc: 75.00%] [G loss: 0.039113, mse: 0.034113]\n",
      "583 [D loss: 0.089238, acc: 96.88%, op_acc: 71.88%] [G loss: 0.036630, mse: 0.031633]\n",
      "584 [D loss: 0.024707, acc: 100.00%, op_acc: 75.00%] [G loss: 0.038281, mse: 0.033075]\n",
      "585 [D loss: 0.033385, acc: 100.00%, op_acc: 75.00%] [G loss: 0.040282, mse: 0.035780]\n",
      "586 [D loss: 0.040436, acc: 100.00%, op_acc: 81.25%] [G loss: 0.047007, mse: 0.042369]\n",
      "587 [D loss: 0.020962, acc: 100.00%, op_acc: 84.38%] [G loss: 0.047780, mse: 0.042171]\n",
      "588 [D loss: 0.036076, acc: 100.00%, op_acc: 68.75%] [G loss: 0.043296, mse: 0.037947]\n",
      "589 [D loss: 0.048418, acc: 96.88%, op_acc: 71.88%] [G loss: 0.039112, mse: 0.033223]\n",
      "590 [D loss: 0.020803, acc: 100.00%, op_acc: 78.12%] [G loss: 0.040892, mse: 0.034161]\n",
      "591 [D loss: 0.051322, acc: 96.88%, op_acc: 71.88%] [G loss: 0.035778, mse: 0.031460]\n",
      "592 [D loss: 0.098410, acc: 93.75%, op_acc: 65.62%] [G loss: 0.034046, mse: 0.027813]\n",
      "593 [D loss: 0.031640, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041126, mse: 0.033788]\n",
      "594 [D loss: 0.071682, acc: 96.88%, op_acc: 81.25%] [G loss: 0.039770, mse: 0.033653]\n",
      "595 [D loss: 0.102353, acc: 96.88%, op_acc: 78.12%] [G loss: 0.037915, mse: 0.033700]\n",
      "596 [D loss: 0.060816, acc: 93.75%, op_acc: 78.12%] [G loss: 0.038803, mse: 0.033893]\n",
      "597 [D loss: 0.035685, acc: 100.00%, op_acc: 81.25%] [G loss: 0.037208, mse: 0.031564]\n",
      "598 [D loss: 0.024644, acc: 100.00%, op_acc: 75.00%] [G loss: 0.040423, mse: 0.034488]\n",
      "599 [D loss: 0.035621, acc: 100.00%, op_acc: 65.62%] [G loss: 0.040970, mse: 0.034823]\n",
      "600 [D loss: 0.029774, acc: 100.00%, op_acc: 84.38%] [G loss: 0.040284, mse: 0.034115]\n",
      "601 [D loss: 0.023893, acc: 100.00%, op_acc: 93.75%] [G loss: 0.038369, mse: 0.032760]\n",
      "602 [D loss: 0.031828, acc: 100.00%, op_acc: 81.25%] [G loss: 0.038208, mse: 0.032722]\n",
      "603 [D loss: 0.042015, acc: 100.00%, op_acc: 71.88%] [G loss: 0.035844, mse: 0.030041]\n",
      "604 [D loss: 0.028462, acc: 100.00%, op_acc: 87.50%] [G loss: 0.042413, mse: 0.035929]\n",
      "605 [D loss: 0.030484, acc: 100.00%, op_acc: 84.38%] [G loss: 0.037731, mse: 0.031826]\n",
      "606 [D loss: 0.048083, acc: 96.88%, op_acc: 78.12%] [G loss: 0.046219, mse: 0.038030]\n",
      "607 [D loss: 0.024472, acc: 100.00%, op_acc: 78.12%] [G loss: 0.043109, mse: 0.034592]\n",
      "608 [D loss: 0.791220, acc: 50.00%, op_acc: 21.88%] [G loss: 0.040749, mse: 0.036586]\n",
      "609 [D loss: 0.030482, acc: 100.00%, op_acc: 87.50%] [G loss: 0.037892, mse: 0.033541]\n",
      "610 [D loss: 0.375934, acc: 81.25%, op_acc: 68.75%] [G loss: 0.036017, mse: 0.033769]\n",
      "611 [D loss: 0.151518, acc: 90.62%, op_acc: 68.75%] [G loss: 0.029863, mse: 0.027881]\n",
      "612 [D loss: 0.081532, acc: 100.00%, op_acc: 81.25%] [G loss: 0.036337, mse: 0.034118]\n",
      "613 [D loss: 0.057676, acc: 100.00%, op_acc: 71.88%] [G loss: 0.032624, mse: 0.029829]\n",
      "614 [D loss: 0.136192, acc: 87.50%, op_acc: 81.25%] [G loss: 0.029652, mse: 0.026911]\n",
      "615 [D loss: 0.039065, acc: 100.00%, op_acc: 71.88%] [G loss: 0.037181, mse: 0.034290]\n",
      "616 [D loss: 0.068507, acc: 96.88%, op_acc: 71.88%] [G loss: 0.036539, mse: 0.032863]\n",
      "617 [D loss: 0.032924, acc: 100.00%, op_acc: 78.12%] [G loss: 0.036562, mse: 0.032684]\n",
      "618 [D loss: 0.104082, acc: 93.75%, op_acc: 78.12%] [G loss: 0.033340, mse: 0.030318]\n",
      "619 [D loss: 0.047659, acc: 100.00%, op_acc: 75.00%] [G loss: 0.030439, mse: 0.026842]\n",
      "620 [D loss: 0.038113, acc: 100.00%, op_acc: 68.75%] [G loss: 0.035997, mse: 0.032186]\n",
      "621 [D loss: 0.044319, acc: 100.00%, op_acc: 84.38%] [G loss: 0.035817, mse: 0.031740]\n",
      "622 [D loss: 0.026054, acc: 100.00%, op_acc: 78.12%] [G loss: 0.034793, mse: 0.030390]\n",
      "623 [D loss: 0.085021, acc: 93.75%, op_acc: 78.12%] [G loss: 0.033940, mse: 0.029987]\n",
      "624 [D loss: 0.113494, acc: 87.50%, op_acc: 75.00%] [G loss: 0.032775, mse: 0.028354]\n",
      "625 [D loss: 0.030667, acc: 100.00%, op_acc: 81.25%] [G loss: 0.041695, mse: 0.037012]\n",
      "626 [D loss: 0.065259, acc: 96.88%, op_acc: 81.25%] [G loss: 0.047611, mse: 0.042903]\n",
      "627 [D loss: 0.123201, acc: 93.75%, op_acc: 65.62%] [G loss: 0.051116, mse: 0.047105]\n",
      "628 [D loss: 0.030537, acc: 100.00%, op_acc: 68.75%] [G loss: 0.050856, mse: 0.046675]\n",
      "629 [D loss: 0.102201, acc: 90.62%, op_acc: 84.38%] [G loss: 0.041379, mse: 0.036790]\n",
      "630 [D loss: 0.032585, acc: 100.00%, op_acc: 75.00%] [G loss: 0.038754, mse: 0.033824]\n",
      "631 [D loss: 0.122706, acc: 93.75%, op_acc: 71.88%] [G loss: 0.039384, mse: 0.035456]\n",
      "632 [D loss: 0.088176, acc: 93.75%, op_acc: 68.75%] [G loss: 0.035810, mse: 0.032571]\n",
      "633 [D loss: 0.034743, acc: 100.00%, op_acc: 71.88%] [G loss: 0.045998, mse: 0.041944]\n",
      "634 [D loss: 0.042687, acc: 96.88%, op_acc: 78.12%] [G loss: 0.040233, mse: 0.035706]\n",
      "635 [D loss: 0.033531, acc: 100.00%, op_acc: 87.50%] [G loss: 0.044670, mse: 0.039974]\n",
      "636 [D loss: 0.025868, acc: 100.00%, op_acc: 84.38%] [G loss: 0.042919, mse: 0.037786]\n",
      "637 [D loss: 0.040158, acc: 100.00%, op_acc: 65.62%] [G loss: 0.036205, mse: 0.030351]\n",
      "638 [D loss: 0.119340, acc: 93.75%, op_acc: 59.38%] [G loss: 0.035378, mse: 0.030998]\n",
      "639 [D loss: 0.031191, acc: 100.00%, op_acc: 75.00%] [G loss: 0.038377, mse: 0.034157]\n",
      "640 [D loss: 0.025837, acc: 100.00%, op_acc: 78.12%] [G loss: 0.035650, mse: 0.030764]\n",
      "641 [D loss: 0.021152, acc: 100.00%, op_acc: 90.62%] [G loss: 0.036172, mse: 0.031695]\n",
      "642 [D loss: 0.248063, acc: 87.50%, op_acc: 68.75%] [G loss: 0.035406, mse: 0.032767]\n",
      "643 [D loss: 0.117432, acc: 90.62%, op_acc: 68.75%] [G loss: 0.036445, mse: 0.032095]\n",
      "644 [D loss: 0.134671, acc: 90.62%, op_acc: 68.75%] [G loss: 0.034663, mse: 0.031203]\n",
      "645 [D loss: 0.037748, acc: 100.00%, op_acc: 81.25%] [G loss: 0.037841, mse: 0.034426]\n",
      "646 [D loss: 0.040882, acc: 100.00%, op_acc: 75.00%] [G loss: 0.034767, mse: 0.030895]\n",
      "647 [D loss: 0.033383, acc: 100.00%, op_acc: 78.12%] [G loss: 0.034456, mse: 0.030228]\n",
      "648 [D loss: 0.028311, acc: 100.00%, op_acc: 81.25%] [G loss: 0.036048, mse: 0.031509]\n",
      "649 [D loss: 0.043377, acc: 100.00%, op_acc: 75.00%] [G loss: 0.038722, mse: 0.033888]\n",
      "650 [D loss: 0.083253, acc: 96.88%, op_acc: 71.88%] [G loss: 0.034571, mse: 0.029943]\n",
      "651 [D loss: 0.046455, acc: 100.00%, op_acc: 68.75%] [G loss: 0.037363, mse: 0.032796]\n",
      "652 [D loss: 0.029778, acc: 100.00%, op_acc: 78.12%] [G loss: 0.038976, mse: 0.034069]\n",
      "653 [D loss: 0.023691, acc: 100.00%, op_acc: 81.25%] [G loss: 0.034820, mse: 0.029820]\n",
      "654 [D loss: 0.042266, acc: 96.88%, op_acc: 75.00%] [G loss: 0.036127, mse: 0.030773]\n",
      "655 [D loss: 0.039907, acc: 100.00%, op_acc: 75.00%] [G loss: 0.034277, mse: 0.028651]\n",
      "656 [D loss: 0.025612, acc: 100.00%, op_acc: 81.25%] [G loss: 0.039611, mse: 0.034067]\n",
      "657 [D loss: 0.051712, acc: 100.00%, op_acc: 71.88%] [G loss: 0.032803, mse: 0.027663]\n",
      "658 [D loss: 0.024380, acc: 100.00%, op_acc: 84.38%] [G loss: 0.036923, mse: 0.032272]\n",
      "659 [D loss: 0.135091, acc: 96.88%, op_acc: 78.12%] [G loss: 0.036110, mse: 0.031023]\n",
      "660 [D loss: 0.020267, acc: 100.00%, op_acc: 81.25%] [G loss: 0.036195, mse: 0.030618]\n",
      "661 [D loss: 0.046409, acc: 96.88%, op_acc: 81.25%] [G loss: 0.040456, mse: 0.035755]\n",
      "662 [D loss: 0.050359, acc: 96.88%, op_acc: 75.00%] [G loss: 0.039989, mse: 0.034545]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-962f70c90b08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mccgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCCGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mccgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-953104e75b61>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;31m# Train the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# Plot the progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ccgan = CCGAN()\n",
    "ccgan.train(epochs=20000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
