{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6147287220424757222\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1687760896\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17736817575887873542\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 660, pci bus id: 0000:01:00.0, compute capability: 3.0\"\n",
      "]\n",
      "Default GPU Device: /device:GPU:0\n",
      "Modules imported.\n"
     ]
    }
   ],
   "source": [
    "# math\n",
    "import numpy as np\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# aux\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "# device check\n",
    "from tensorflow.python.client import device_lib\n",
    "print('Devices:', device_lib.list_local_devices())\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# GPU check\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "print('Modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs dims since we are working with MNIST dataset\n",
    "img_rows = 28 \n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = [img_rows, img_cols, channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Rescale -1 to 1\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_routing(inputs, input_num_capsule=32, input_dim_vector=8, num_capsule=10, dim_vector=16):\n",
    "    x_inp = Input(shape=img_shape)\n",
    "    x = Dense(input_num_capsule*input_dim_vector, activation=squash)(x_inp)\n",
    "    x = Dense(num_capsule*dim_vector, activation=squash)(x)\n",
    "    out_digs = Reshape(num_capsule, dim_vector)\n",
    "    \n",
    "    return Model(x_inp, out_digs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num_capsule=32\n",
    "input_dim_vector=8\n",
    "num_capsule=10\n",
    "dim_vector=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator structure\n",
    "def build_discriminator():\n",
    "\n",
    "        img_shape = (img_rows, img_cols, channels)\n",
    "        \n",
    "        #model = Sequential()\n",
    "\n",
    "        #model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        #model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        #model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        #model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        #model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(Dropout(0.25))\n",
    "\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        #model.summary()\n",
    "\n",
    "        #img = Input(shape=img_shape)\n",
    "        #validity = model(img)\n",
    "        \n",
    "        \n",
    "        img = Input(shape=img_shape)\n",
    "        \n",
    "        # Layer 1: Just a conventional Conv2D layer\n",
    "        conv1 = Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation=squash, name='conv1')(img)\n",
    "        conv2 = Conv2D(filters=4*64, kernel_size=3, strides=2, padding='same', name='primarycap_conv2d_2')(conv1)\n",
    "        conv2 = Lambda(squash, name='primarycap_squash_2')(conv2)\n",
    "        x = Dropout(0.25)(conv2)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Reshape((8, 8, -1))(x)\n",
    "        conv3 = Conv2D(filters=8*128, kernel_size=3, strides=2, padding='same', name='primarycap_conv2d_3')(x)\n",
    "        conv3 = Lambda(squash, name='primarycap_squash_3')(conv3)\n",
    "        x = Dropout(0.25)(conv3)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Reshape((8, 8, -1))(x)\n",
    "        conv4 = Conv2D(filters=16*256, kernel_size=3, strides=1, padding='same', name='primarycap_conv2d_4')(x)\n",
    "        conv4 = Lambda(squash, name='primarycap_squash_4')(conv4)\n",
    "        x = Dropout(0.25)(conv4)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        #digitcaps = CapsuleLayer(num_capsule=10, dim_vector=16, num_routing=3, name='digitcaps')(primarycaps)\n",
    "        x = Flatten()(x)\n",
    "        validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "        return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator structure\n",
    "def build_generator():\n",
    "\n",
    "        noise_shape = (100,)\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=noise_shape))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8)) \n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining an optimizer\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d_2 (Conv2D) (None, 7, 7, 256)         73984     \n",
      "_________________________________________________________________\n",
      "primarycap_squash_2 (Lambda) (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "reshape_19 (Reshape)         (None, 8, 8, 196)         0         \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d_3 (Conv2D) (None, 4, 4, 1024)        1807360   \n",
      "_________________________________________________________________\n",
      "primarycap_squash_3 (Lambda) (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "reshape_20 (Reshape)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "primarycap_conv2d_4 (Conv2D) (None, 8, 8, 4096)        9441280   \n",
      "_________________________________________________________________\n",
      "primarycap_squash_4 (Lambda) (None, 8, 8, 4096)        0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 8, 8, 4096)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 8, 8, 4096)        16384     \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 262144)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 262145    \n",
      "=================================================================\n",
      "Total params: 11,606,593\n",
      "Trainable params: 11,595,841\n",
      "Non-trainable params: 10,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_21 (Reshape)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,705\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the generator\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model_19/sequential_4/activation_12/Tanh:0\", shape=(?, 28, 28, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# feeding noise to generator\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the combined model we will only train the generator\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to discriminate generated images\n",
    "valid = discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_19 (Model)             (None, 28, 28, 1)         856705    \n",
      "_________________________________________________________________\n",
      "model_18 (Model)             (None, 1)                 11606593  \n",
      "=================================================================\n",
      "Total params: 12,463,298\n",
      "Trainable params: 856,065\n",
      "Non-trainable params: 11,607,233\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The combined model  (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "print('COMBINED:')\n",
    "combined.summary()\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size=32, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # The generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * 32)\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "        directory = \"images\"\n",
    "        \n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.839026, acc.: 9.38%] [G loss: 1.379021]\n",
      "1 [D loss: 0.348902, acc.: 53.12%] [G loss: 2.377234]\n",
      "2 [D loss: 0.476814, acc.: 65.62%] [G loss: 2.562446]\n",
      "3 [D loss: 6.636912, acc.: 50.00%] [G loss: 1.445375]\n",
      "4 [D loss: 1.040923, acc.: 81.25%] [G loss: 1.777034]\n",
      "5 [D loss: 0.498869, acc.: 90.62%] [G loss: 0.640130]\n",
      "6 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.593138]\n",
      "7 [D loss: 0.294135, acc.: 96.88%] [G loss: 0.013981]\n",
      "8 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.127763]\n",
      "9 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.015772]\n",
      "10 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.139718]\n",
      "11 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.173629]\n",
      "12 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.005854]\n",
      "13 [D loss: 0.010384, acc.: 100.00%] [G loss: 0.172729]\n",
      "14 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000015]\n",
      "15 [D loss: 0.047752, acc.: 96.88%] [G loss: 0.457836]\n",
      "16 [D loss: 0.110405, acc.: 96.88%] [G loss: 0.028939]\n",
      "17 [D loss: 0.064535, acc.: 96.88%] [G loss: 0.001133]\n",
      "18 [D loss: 1.524699, acc.: 78.12%] [G loss: 0.003902]\n",
      "19 [D loss: 0.001230, acc.: 100.00%] [G loss: 0.323884]\n",
      "20 [D loss: 9.868963, acc.: 21.88%] [G loss: 0.170131]\n",
      "21 [D loss: 6.179371, acc.: 59.38%] [G loss: 0.517876]\n",
      "22 [D loss: 5.719346, acc.: 62.50%] [G loss: 0.000296]\n",
      "23 [D loss: 6.544974, acc.: 56.25%] [G loss: 1.191361]\n",
      "24 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.976669]\n",
      "25 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.437352]\n",
      "26 [D loss: 7.473000, acc.: 53.12%] [G loss: 1.556429]\n",
      "27 [D loss: 6.624735, acc.: 56.25%] [G loss: 0.082801]\n",
      "28 [D loss: 7.407163, acc.: 53.12%] [G loss: 1.511072]\n",
      "29 [D loss: 6.979546, acc.: 56.25%] [G loss: 0.006289]\n",
      "30 [D loss: 6.343153, acc.: 53.12%] [G loss: 3.713937]\n",
      "31 [D loss: 6.154748, acc.: 59.38%] [G loss: 3.132704]\n",
      "32 [D loss: 6.136197, acc.: 59.38%] [G loss: 2.518662]\n",
      "33 [D loss: 6.299697, acc.: 59.38%] [G loss: 6.307971]\n",
      "34 [D loss: 5.887297, acc.: 56.25%] [G loss: 4.355096]\n",
      "35 [D loss: 5.966909, acc.: 56.25%] [G loss: 5.487329]\n",
      "36 [D loss: 5.265454, acc.: 62.50%] [G loss: 6.926184]\n",
      "37 [D loss: 1.992798, acc.: 87.50%] [G loss: 7.740967]\n",
      "38 [D loss: 4.548719, acc.: 68.75%] [G loss: 8.309628]\n",
      "39 [D loss: 5.945553, acc.: 59.38%] [G loss: 9.296320]\n",
      "40 [D loss: 4.475777, acc.: 65.62%] [G loss: 8.045414]\n",
      "41 [D loss: 3.337819, acc.: 71.88%] [G loss: 6.575118]\n",
      "42 [D loss: 4.189901, acc.: 71.88%] [G loss: 7.052073]\n",
      "43 [D loss: 3.487397, acc.: 78.12%] [G loss: 7.576285]\n",
      "44 [D loss: 4.601507, acc.: 68.75%] [G loss: 5.104353]\n",
      "45 [D loss: 5.679914, acc.: 62.50%] [G loss: 10.618444]\n",
      "46 [D loss: 7.211517, acc.: 46.88%] [G loss: 9.217169]\n",
      "47 [D loss: 2.491023, acc.: 84.38%] [G loss: 6.098686]\n",
      "48 [D loss: 5.176046, acc.: 62.50%] [G loss: 5.694119]\n",
      "49 [D loss: 3.985596, acc.: 75.00%] [G loss: 7.122682]\n",
      "50 [D loss: 3.998504, acc.: 75.00%] [G loss: 6.609811]\n",
      "51 [D loss: 4.645704, acc.: 68.75%] [G loss: 4.868016]\n",
      "52 [D loss: 4.336449, acc.: 68.75%] [G loss: 4.641716]\n",
      "53 [D loss: 3.663787, acc.: 75.00%] [G loss: 6.527642]\n",
      "54 [D loss: 2.996813, acc.: 81.25%] [G loss: 3.760334]\n",
      "55 [D loss: 4.697896, acc.: 65.62%] [G loss: 4.555963]\n",
      "56 [D loss: 2.723047, acc.: 81.25%] [G loss: 6.129812]\n",
      "57 [D loss: 4.966295, acc.: 65.62%] [G loss: 10.313784]\n",
      "58 [D loss: 4.857597, acc.: 59.38%] [G loss: 7.897787]\n",
      "59 [D loss: 5.087926, acc.: 65.62%] [G loss: 7.460603]\n",
      "60 [D loss: 2.490998, acc.: 84.38%] [G loss: 7.436617]\n",
      "61 [D loss: 2.857800, acc.: 81.25%] [G loss: 4.370400]\n",
      "62 [D loss: 3.985596, acc.: 75.00%] [G loss: 3.812292]\n",
      "63 [D loss: 3.109625, acc.: 78.12%] [G loss: 7.178577]\n",
      "64 [D loss: 3.487397, acc.: 78.12%] [G loss: 5.341005]\n",
      "65 [D loss: 5.480195, acc.: 65.62%] [G loss: 6.262764]\n",
      "66 [D loss: 4.981996, acc.: 68.75%] [G loss: 4.560065]\n",
      "67 [D loss: 4.340199, acc.: 71.88%] [G loss: 7.053477]\n",
      "68 [D loss: 3.985623, acc.: 75.00%] [G loss: 1.870212]\n",
      "69 [D loss: 5.480489, acc.: 65.62%] [G loss: 4.151462]\n",
      "70 [D loss: 5.499661, acc.: 62.50%] [G loss: 5.887885]\n",
      "71 [D loss: 4.177856, acc.: 71.88%] [G loss: 6.653521]\n",
      "72 [D loss: 4.982174, acc.: 68.75%] [G loss: 3.033394]\n",
      "73 [D loss: 5.503806, acc.: 65.62%] [G loss: 2.561618]\n",
      "74 [D loss: 5.882071, acc.: 62.50%] [G loss: 8.206228]\n",
      "75 [D loss: 6.974793, acc.: 56.25%] [G loss: 4.536065]\n",
      "76 [D loss: 6.969839, acc.: 56.25%] [G loss: 2.871961]\n",
      "77 [D loss: 6.974793, acc.: 56.25%] [G loss: 2.745205]\n",
      "78 [D loss: 5.738471, acc.: 62.50%] [G loss: 3.820521]\n",
      "79 [D loss: 5.961462, acc.: 62.50%] [G loss: 5.637443]\n",
      "80 [D loss: 6.069947, acc.: 59.38%] [G loss: 5.007813]\n",
      "81 [D loss: 4.483878, acc.: 71.88%] [G loss: 3.996614]\n",
      "82 [D loss: 4.930269, acc.: 68.75%] [G loss: 5.292797]\n",
      "83 [D loss: 4.424265, acc.: 68.75%] [G loss: 7.352367]\n",
      "84 [D loss: 2.537815, acc.: 81.25%] [G loss: 6.667941]\n",
      "85 [D loss: 2.006474, acc.: 87.50%] [G loss: 9.570119]\n",
      "86 [D loss: 3.085900, acc.: 78.12%] [G loss: 8.382626]\n",
      "87 [D loss: 2.996718, acc.: 81.25%] [G loss: 9.441532]\n",
      "88 [D loss: 4.483796, acc.: 71.88%] [G loss: 9.621222]\n",
      "89 [D loss: 4.168091, acc.: 71.88%] [G loss: 5.036905]\n",
      "90 [D loss: 3.058760, acc.: 78.12%] [G loss: 8.627236]\n",
      "91 [D loss: 3.444115, acc.: 78.12%] [G loss: 8.902641]\n",
      "92 [D loss: 5.359220, acc.: 65.62%] [G loss: 7.371210]\n",
      "93 [D loss: 4.144499, acc.: 71.88%] [G loss: 7.459731]\n",
      "94 [D loss: 5.978395, acc.: 62.50%] [G loss: 7.611473]\n",
      "95 [D loss: 5.217129, acc.: 65.62%] [G loss: 6.803477]\n",
      "96 [D loss: 4.523581, acc.: 68.75%] [G loss: 7.211502]\n",
      "97 [D loss: 5.324888, acc.: 65.62%] [G loss: 8.562738]\n",
      "98 [D loss: 4.981996, acc.: 68.75%] [G loss: 10.556786]\n",
      "99 [D loss: 2.352909, acc.: 84.38%] [G loss: 8.285804]\n",
      "100 [D loss: 3.630250, acc.: 75.00%] [G loss: 10.138151]\n",
      "101 [D loss: 2.490998, acc.: 84.38%] [G loss: 11.241189]\n",
      "102 [D loss: 3.991152, acc.: 75.00%] [G loss: 8.563172]\n",
      "103 [D loss: 2.989197, acc.: 81.25%] [G loss: 8.562738]\n",
      "104 [D loss: 1.494599, acc.: 90.62%] [G loss: 9.570119]\n",
      "105 [D loss: 2.758540, acc.: 81.25%] [G loss: 8.059376]\n",
      "106 [D loss: 4.982002, acc.: 68.75%] [G loss: 11.849996]\n",
      "107 [D loss: 2.109224, acc.: 84.38%] [G loss: 9.966413]\n",
      "108 [D loss: 1.992826, acc.: 87.50%] [G loss: 9.332458]\n",
      "109 [D loss: 2.775543, acc.: 81.25%] [G loss: 8.562084]\n",
      "110 [D loss: 5.491177, acc.: 65.62%] [G loss: 10.635513]\n",
      "111 [D loss: 5.188875, acc.: 65.62%] [G loss: 9.138258]\n",
      "112 [D loss: 3.467948, acc.: 75.00%] [G loss: 5.165838]\n",
      "113 [D loss: 3.985612, acc.: 75.00%] [G loss: 7.900683]\n",
      "114 [D loss: 4.489287, acc.: 71.88%] [G loss: 8.302696]\n",
      "115 [D loss: 4.423701, acc.: 71.88%] [G loss: 8.126898]\n",
      "116 [D loss: 2.989494, acc.: 81.25%] [G loss: 7.579531]\n",
      "117 [D loss: 4.548910, acc.: 62.50%] [G loss: 8.562738]\n",
      "118 [D loss: 2.765971, acc.: 78.12%] [G loss: 8.931971]\n",
      "119 [D loss: 2.578676, acc.: 78.12%] [G loss: 12.770147]\n",
      "120 [D loss: 2.491013, acc.: 84.38%] [G loss: 11.459119]\n",
      "121 [D loss: 0.996400, acc.: 93.75%] [G loss: 10.902896]\n",
      "122 [D loss: 1.584972, acc.: 87.50%] [G loss: 10.319252]\n",
      "123 [D loss: 3.768836, acc.: 75.00%] [G loss: 11.575830]\n",
      "124 [D loss: 2.394403, acc.: 84.38%] [G loss: 8.873821]\n",
      "125 [D loss: 1.992927, acc.: 87.50%] [G loss: 9.809945]\n",
      "126 [D loss: 3.116619, acc.: 78.12%] [G loss: 12.606211]\n",
      "127 [D loss: 2.320409, acc.: 81.25%] [G loss: 11.354410]\n",
      "128 [D loss: 0.498200, acc.: 96.88%] [G loss: 10.685642]\n",
      "129 [D loss: 0.996399, acc.: 93.75%] [G loss: 11.770977]\n",
      "130 [D loss: 1.341309, acc.: 90.62%] [G loss: 11.404181]\n",
      "131 [D loss: 0.996400, acc.: 93.75%] [G loss: 11.497639]\n",
      "132 [D loss: 3.314602, acc.: 78.12%] [G loss: 11.287783]\n",
      "133 [D loss: 1.494599, acc.: 90.62%] [G loss: 10.964401]\n",
      "134 [D loss: 2.529794, acc.: 81.25%] [G loss: 10.146446]\n",
      "135 [D loss: 2.068490, acc.: 81.25%] [G loss: 11.838184]\n",
      "136 [D loss: 2.297644, acc.: 84.38%] [G loss: 12.634147]\n",
      "137 [D loss: 2.736028, acc.: 81.25%] [G loss: 12.794574]\n",
      "138 [D loss: 2.105408, acc.: 81.25%] [G loss: 8.630649]\n",
      "139 [D loss: 2.536745, acc.: 81.25%] [G loss: 8.528576]\n",
      "140 [D loss: 3.404926, acc.: 78.12%] [G loss: 9.341610]\n",
      "141 [D loss: 2.014746, acc.: 84.38%] [G loss: 9.066433]\n",
      "142 [D loss: 1.520957, acc.: 87.50%] [G loss: 11.322601]\n",
      "143 [D loss: 0.000010, acc.: 100.00%] [G loss: 13.934566]\n",
      "144 [D loss: 0.476539, acc.: 96.88%] [G loss: 11.582235]\n",
      "145 [D loss: 0.000000, acc.: 100.00%] [G loss: 12.593683]\n",
      "146 [D loss: 0.726027, acc.: 93.75%] [G loss: 15.264212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 [D loss: 0.000000, acc.: 100.00%] [G loss: 13.638155]\n",
      "148 [D loss: 0.008160, acc.: 100.00%] [G loss: 14.962907]\n",
      "149 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.712296]\n",
      "150 [D loss: 0.996399, acc.: 93.75%] [G loss: 13.504635]\n",
      "151 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.548648]\n",
      "152 [D loss: 0.000000, acc.: 100.00%] [G loss: 13.184855]\n",
      "153 [D loss: 0.440861, acc.: 93.75%] [G loss: 15.104993]\n",
      "154 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.337946]\n",
      "155 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.495682]\n",
      "156 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.738821]\n",
      "157 [D loss: 0.101910, acc.: 96.88%] [G loss: 15.253975]\n",
      "158 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614407]\n",
      "159 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.678848]\n",
      "160 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.607024]\n",
      "161 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "162 [D loss: 0.006849, acc.: 100.00%] [G loss: 14.103333]\n",
      "163 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "164 [D loss: 0.996399, acc.: 93.75%] [G loss: 16.118095]\n",
      "165 [D loss: 0.498200, acc.: 96.88%] [G loss: 14.607023]\n",
      "166 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "167 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "168 [D loss: 0.013550, acc.: 100.00%] [G loss: 15.614405]\n",
      "169 [D loss: 0.665943, acc.: 93.75%] [G loss: 16.100016]\n",
      "170 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "171 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "172 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.302020]\n",
      "173 [D loss: 0.015414, acc.: 100.00%] [G loss: 16.118095]\n",
      "174 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "175 [D loss: 0.547822, acc.: 93.75%] [G loss: 16.118095]\n",
      "176 [D loss: 0.529605, acc.: 93.75%] [G loss: 9.066871]\n",
      "177 [D loss: 6.476594, acc.: 59.38%] [G loss: 7.555357]\n",
      "178 [D loss: 3.437102, acc.: 78.12%] [G loss: 8.805630]\n",
      "179 [D loss: 3.487397, acc.: 78.12%] [G loss: 7.555441]\n",
      "180 [D loss: 5.034061, acc.: 65.62%] [G loss: 5.058770]\n",
      "181 [D loss: 3.487397, acc.: 78.12%] [G loss: 6.148528]\n",
      "182 [D loss: 4.483803, acc.: 71.88%] [G loss: 6.547976]\n",
      "183 [D loss: 5.978396, acc.: 62.50%] [G loss: 5.608088]\n",
      "184 [D loss: 4.399169, acc.: 71.88%] [G loss: 7.051667]\n",
      "185 [D loss: 2.456712, acc.: 84.38%] [G loss: 6.567702]\n",
      "186 [D loss: 2.898879, acc.: 81.25%] [G loss: 14.103432]\n",
      "187 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.549555]\n",
      "188 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "189 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "190 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "191 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "192 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "193 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.447925]\n",
      "194 [D loss: 1.108246, acc.: 90.62%] [G loss: 16.118095]\n",
      "195 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.614405]\n",
      "196 [D loss: 1.007381, acc.: 93.75%] [G loss: 15.110693]\n",
      "197 [D loss: 0.077249, acc.: 96.88%] [G loss: 16.118095]\n",
      "198 [D loss: 1.665364, acc.: 87.50%] [G loss: 14.078936]\n",
      "199 [D loss: 4.536550, acc.: 68.75%] [G loss: 15.248773]\n",
      "200 [D loss: 0.000133, acc.: 100.00%] [G loss: 14.963981]\n",
      "201 [D loss: 0.111721, acc.: 96.88%] [G loss: 15.510030]\n",
      "202 [D loss: 0.146105, acc.: 96.88%] [G loss: 13.371054]\n",
      "203 [D loss: 0.000000, acc.: 100.00%] [G loss: 13.927177]\n",
      "204 [D loss: 1.495126, acc.: 90.62%] [G loss: 13.705644]\n",
      "205 [D loss: 3.487397, acc.: 78.12%] [G loss: 10.806311]\n",
      "206 [D loss: 4.982003, acc.: 68.75%] [G loss: 11.092043]\n",
      "207 [D loss: 3.981671, acc.: 71.88%] [G loss: 13.253362]\n",
      "208 [D loss: 0.113625, acc.: 96.88%] [G loss: 14.607024]\n",
      "209 [D loss: 1.907792, acc.: 87.50%] [G loss: 13.625665]\n",
      "210 [D loss: 0.631346, acc.: 93.75%] [G loss: 15.252180]\n",
      "211 [D loss: 1.061770, acc.: 90.62%] [G loss: 15.317663]\n",
      "212 [D loss: 0.747270, acc.: 93.75%] [G loss: 16.088173]\n",
      "213 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.876498]\n",
      "214 [D loss: 0.036403, acc.: 96.88%] [G loss: 16.027271]\n",
      "215 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "216 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "217 [D loss: 0.996399, acc.: 93.75%] [G loss: 15.154890]\n",
      "218 [D loss: 0.230184, acc.: 96.88%] [G loss: 16.118095]\n",
      "219 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "220 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.650783]\n",
      "221 [D loss: 0.086923, acc.: 96.88%] [G loss: 16.118095]\n",
      "222 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "223 [D loss: 0.000114, acc.: 100.00%] [G loss: 16.118095]\n",
      "224 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "225 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "226 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "227 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "228 [D loss: 5.609724, acc.: 62.50%] [G loss: 12.818084]\n",
      "229 [D loss: 4.981996, acc.: 68.75%] [G loss: 10.868842]\n",
      "230 [D loss: 1.600035, acc.: 87.50%] [G loss: 13.989163]\n",
      "231 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.312330]\n",
      "232 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.609301]\n",
      "233 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "234 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.022018]\n",
      "235 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614418]\n",
      "236 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.677409]\n",
      "237 [D loss: 0.223086, acc.: 96.88%] [G loss: 16.118095]\n",
      "238 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "239 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "240 [D loss: 2.076849, acc.: 81.25%] [G loss: 15.538324]\n",
      "241 [D loss: 3.385748, acc.: 75.00%] [G loss: 16.118095]\n",
      "242 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.649486]\n",
      "243 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.630722]\n",
      "244 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "245 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "246 [D loss: 1.007381, acc.: 93.75%] [G loss: 16.118095]\n",
      "247 [D loss: 6.271985, acc.: 53.12%] [G loss: 3.022143]\n",
      "248 [D loss: 6.979080, acc.: 56.25%] [G loss: 2.014762]\n",
      "249 [D loss: 5.480195, acc.: 65.62%] [G loss: 4.533215]\n",
      "250 [D loss: 7.681314, acc.: 50.00%] [G loss: 4.029524]\n",
      "251 [D loss: 6.320865, acc.: 59.38%] [G loss: 6.256761]\n",
      "252 [D loss: 5.876579, acc.: 62.50%] [G loss: 8.669941]\n",
      "253 [D loss: 4.483813, acc.: 71.88%] [G loss: 6.044394]\n",
      "254 [D loss: 4.168442, acc.: 71.88%] [G loss: 6.989692]\n",
      "255 [D loss: 3.985596, acc.: 75.00%] [G loss: 6.694066]\n",
      "256 [D loss: 3.700164, acc.: 75.00%] [G loss: 7.030560]\n",
      "257 [D loss: 4.061069, acc.: 71.88%] [G loss: 7.585601]\n",
      "258 [D loss: 4.305538, acc.: 71.88%] [G loss: 7.564382]\n",
      "259 [D loss: 2.101852, acc.: 84.38%] [G loss: 10.258991]\n",
      "260 [D loss: 3.985596, acc.: 75.00%] [G loss: 8.783546]\n",
      "261 [D loss: 3.248081, acc.: 78.12%] [G loss: 10.335274]\n",
      "262 [D loss: 1.992799, acc.: 87.50%] [G loss: 6.892340]\n",
      "263 [D loss: 2.490998, acc.: 84.38%] [G loss: 7.174658]\n",
      "264 [D loss: 3.985596, acc.: 75.00%] [G loss: 10.565763]\n",
      "265 [D loss: 4.483796, acc.: 71.88%] [G loss: 8.621918]\n",
      "266 [D loss: 4.810987, acc.: 68.75%] [G loss: 5.178210]\n",
      "267 [D loss: 7.456150, acc.: 53.12%] [G loss: 4.533327]\n",
      "268 [D loss: 4.483796, acc.: 71.88%] [G loss: 8.068016]\n",
      "269 [D loss: 4.871797, acc.: 68.75%] [G loss: 7.019669]\n",
      "270 [D loss: 3.555949, acc.: 75.00%] [G loss: 9.342784]\n",
      "271 [D loss: 3.988184, acc.: 75.00%] [G loss: 8.228018]\n",
      "272 [D loss: 6.505495, acc.: 56.25%] [G loss: 9.837587]\n",
      "273 [D loss: 5.004048, acc.: 68.75%] [G loss: 9.647550]\n",
      "274 [D loss: 7.316096, acc.: 50.00%] [G loss: 8.301622]\n",
      "275 [D loss: 3.504845, acc.: 78.12%] [G loss: 8.562738]\n",
      "276 [D loss: 4.998469, acc.: 68.75%] [G loss: 7.052009]\n",
      "277 [D loss: 6.532276, acc.: 56.25%] [G loss: 9.973535]\n",
      "278 [D loss: 5.432632, acc.: 62.50%] [G loss: 10.577499]\n",
      "279 [D loss: 4.981246, acc.: 68.75%] [G loss: 7.832347]\n",
      "280 [D loss: 3.669698, acc.: 75.00%] [G loss: 8.972670]\n",
      "281 [D loss: 5.422793, acc.: 62.50%] [G loss: 7.917112]\n",
      "282 [D loss: 3.831748, acc.: 71.88%] [G loss: 7.275876]\n",
      "283 [D loss: 3.974250, acc.: 75.00%] [G loss: 8.058348]\n",
      "284 [D loss: 3.619178, acc.: 71.88%] [G loss: 9.570402]\n",
      "285 [D loss: 4.494778, acc.: 71.88%] [G loss: 9.066437]\n",
      "286 [D loss: 3.775518, acc.: 68.75%] [G loss: 11.503065]\n",
      "287 [D loss: 2.598639, acc.: 81.25%] [G loss: 10.081075]\n",
      "288 [D loss: 2.122665, acc.: 84.38%] [G loss: 7.955581]\n",
      "289 [D loss: 3.297985, acc.: 75.00%] [G loss: 11.609430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290 [D loss: 1.494599, acc.: 90.62%] [G loss: 13.205391]\n",
      "291 [D loss: 0.608623, acc.: 93.75%] [G loss: 12.594759]\n",
      "292 [D loss: 0.164858, acc.: 96.88%] [G loss: 14.661697]\n",
      "293 [D loss: 1.058335, acc.: 90.62%] [G loss: 16.118095]\n",
      "294 [D loss: 0.219072, acc.: 96.88%] [G loss: 16.118095]\n",
      "295 [D loss: 0.116462, acc.: 96.88%] [G loss: 16.099258]\n",
      "296 [D loss: 0.000081, acc.: 100.00%] [G loss: 16.118095]\n",
      "297 [D loss: 0.503691, acc.: 96.88%] [G loss: 16.118095]\n",
      "298 [D loss: 0.503709, acc.: 96.88%] [G loss: 16.118095]\n",
      "299 [D loss: 1.008345, acc.: 93.75%] [G loss: 16.118095]\n",
      "300 [D loss: 0.029096, acc.: 96.88%] [G loss: 16.118095]\n",
      "301 [D loss: 0.498280, acc.: 96.88%] [G loss: 15.614405]\n",
      "302 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "303 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.874058]\n",
      "304 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "305 [D loss: 0.000001, acc.: 100.00%] [G loss: 15.638876]\n",
      "306 [D loss: 0.008047, acc.: 100.00%] [G loss: 16.118095]\n",
      "307 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.785379]\n",
      "308 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "309 [D loss: 0.011455, acc.: 100.00%] [G loss: 16.072081]\n",
      "310 [D loss: 0.996399, acc.: 93.75%] [G loss: 15.110770]\n",
      "311 [D loss: 0.000000, acc.: 100.00%] [G loss: 14.627636]\n",
      "312 [D loss: 0.000006, acc.: 100.00%] [G loss: 14.697651]\n",
      "313 [D loss: 0.667751, acc.: 93.75%] [G loss: 16.118095]\n",
      "314 [D loss: 0.048088, acc.: 96.88%] [G loss: 15.614405]\n",
      "315 [D loss: 2.538113, acc.: 84.38%] [G loss: 15.137090]\n",
      "316 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "317 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110714]\n",
      "318 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.614405]\n",
      "319 [D loss: 1.494599, acc.: 90.62%] [G loss: 15.842209]\n",
      "320 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.226657]\n",
      "321 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.110765]\n",
      "322 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "323 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "324 [D loss: 0.000240, acc.: 100.00%] [G loss: 16.118095]\n",
      "325 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "326 [D loss: 0.498200, acc.: 96.88%] [G loss: 16.118095]\n",
      "327 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "328 [D loss: 0.000000, acc.: 100.00%] [G loss: 16.118095]\n",
      "329 [D loss: 1.268356, acc.: 90.62%] [G loss: 14.836813]\n",
      "330 [D loss: 1.133734, acc.: 90.62%] [G loss: 16.118095]\n",
      "331 [D loss: 1.007409, acc.: 93.75%] [G loss: 16.118095]\n",
      "332 [D loss: 5.492851, acc.: 62.50%] [G loss: 13.578587]\n",
      "333 [D loss: 1.992798, acc.: 87.50%] [G loss: 8.838708]\n",
      "334 [D loss: 1.992798, acc.: 87.50%] [G loss: 3.223229]\n",
      "335 [D loss: 1.029082, acc.: 90.62%] [G loss: 6.082443]\n",
      "336 [D loss: 3.622372, acc.: 75.00%] [G loss: 13.096021]\n",
      "337 [D loss: 2.363589, acc.: 84.38%] [G loss: 15.614405]\n",
      "338 [D loss: 0.000000, acc.: 100.00%] [G loss: 15.481337]\n",
      "339 [D loss: 0.815750, acc.: 90.62%] [G loss: 15.614405]\n",
      "340 [D loss: 0.498200, acc.: 96.88%] [G loss: 15.110714]\n",
      "341 [D loss: 0.498204, acc.: 96.88%] [G loss: 16.118095]\n",
      "342 [D loss: 0.010341, acc.: 100.00%] [G loss: 15.677838]\n",
      "343 [D loss: 8.859959, acc.: 37.50%] [G loss: 1.809731]\n",
      "344 [D loss: 5.484099, acc.: 65.62%] [G loss: 2.518466]\n",
      "345 [D loss: 6.974793, acc.: 56.25%] [G loss: 2.814075]\n",
      "346 [D loss: 6.974856, acc.: 56.25%] [G loss: 0.755049]\n",
      "347 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "348 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "349 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "350 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "351 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.511072]\n",
      "352 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "353 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "354 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "355 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "356 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "357 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.511072]\n",
      "358 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "359 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.002339]\n",
      "360 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "361 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.007381]\n",
      "362 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000549]\n",
      "363 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007382]\n",
      "364 [D loss: 7.473244, acc.: 53.12%] [G loss: 1.511071]\n",
      "365 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "366 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.464168]\n",
      "367 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "368 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.511071]\n",
      "369 [D loss: 7.473109, acc.: 53.12%] [G loss: 0.000000]\n",
      "370 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "371 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.571548]\n",
      "372 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.511071]\n",
      "373 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.764749]\n",
      "374 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "375 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "376 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "377 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "378 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "379 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "380 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "381 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "382 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "383 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "384 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "385 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "386 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "387 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.511071]\n",
      "388 [D loss: 6.974793, acc.: 56.25%] [G loss: 1.007381]\n",
      "389 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.000000]\n",
      "390 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "391 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "392 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.770183]\n",
      "393 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "394 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "395 [D loss: 7.491526, acc.: 53.12%] [G loss: 0.000000]\n",
      "396 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.007381]\n",
      "397 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "398 [D loss: 7.971192, acc.: 50.00%] [G loss: 2.014762]\n",
      "399 [D loss: 6.974793, acc.: 56.25%] [G loss: 2.014762]\n",
      "400 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "401 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "402 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.297306]\n",
      "403 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "404 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.020635]\n",
      "405 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.244397]\n",
      "406 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "407 [D loss: 7.472993, acc.: 53.12%] [G loss: 1.511071]\n",
      "408 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "409 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.106363]\n",
      "410 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "411 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "412 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "413 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "414 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "415 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "416 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "417 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "418 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "419 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.503691]\n",
      "420 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "421 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "422 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "423 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.000000]\n",
      "424 [D loss: 6.975220, acc.: 56.25%] [G loss: 1.511071]\n",
      "425 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "426 [D loss: 6.974793, acc.: 56.25%] [G loss: 2.014762]\n",
      "427 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "428 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.511071]\n",
      "429 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.000000]\n",
      "430 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.000000]\n",
      "431 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.339488]\n",
      "432 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "433 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.507532]\n",
      "435 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.503691]\n",
      "436 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "437 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.000000]\n",
      "438 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "439 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "440 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.870756]\n",
      "441 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.011464]\n",
      "442 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "443 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "444 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "445 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "446 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503710]\n",
      "447 [D loss: 7.971192, acc.: 50.00%] [G loss: 2.014762]\n",
      "448 [D loss: 6.476594, acc.: 59.38%] [G loss: 1.007381]\n",
      "449 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.212057]\n",
      "450 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "451 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "452 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.478177]\n",
      "453 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "454 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.007381]\n",
      "455 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000005]\n",
      "456 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "457 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "458 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "459 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "460 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "461 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "462 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "463 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "464 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "465 [D loss: 7.971192, acc.: 50.00%] [G loss: 1.511071]\n",
      "466 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "467 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "468 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "469 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "470 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "471 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "472 [D loss: 6.974793, acc.: 56.25%] [G loss: 0.503691]\n",
      "473 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "474 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "475 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "476 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "477 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.503691]\n",
      "478 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.503691]\n",
      "479 [D loss: 7.105762, acc.: 53.12%] [G loss: 0.000000]\n",
      "480 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000000]\n",
      "481 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "482 [D loss: 7.971192, acc.: 50.00%] [G loss: 0.000040]\n",
      "483 [D loss: 7.472993, acc.: 53.12%] [G loss: 0.000000]\n",
      "484 [D loss: 7.608240, acc.: 50.00%] [G loss: 1.050291]\n",
      "485 [D loss: 6.313315, acc.: 59.38%] [G loss: 3.695566]\n",
      "486 [D loss: 5.248612, acc.: 65.62%] [G loss: 6.044286]\n",
      "487 [D loss: 5.212140, acc.: 65.62%] [G loss: 6.271098]\n",
      "488 [D loss: 5.989377, acc.: 62.50%] [G loss: 8.860325]\n",
      "489 [D loss: 4.368957, acc.: 68.75%] [G loss: 10.259644]\n",
      "490 [D loss: 5.286540, acc.: 62.50%] [G loss: 11.276957]\n",
      "491 [D loss: 6.515031, acc.: 59.38%] [G loss: 10.464344]\n",
      "492 [D loss: 6.510922, acc.: 59.38%] [G loss: 9.038550]\n",
      "493 [D loss: 7.472450, acc.: 50.00%] [G loss: 7.550004]\n",
      "494 [D loss: 5.672832, acc.: 59.38%] [G loss: 5.790588]\n",
      "495 [D loss: 7.996878, acc.: 50.00%] [G loss: 4.605392]\n",
      "496 [D loss: 5.990133, acc.: 62.50%] [G loss: 7.288590]\n",
      "497 [D loss: 5.882330, acc.: 56.25%] [G loss: 4.545254]\n",
      "498 [D loss: 5.978395, acc.: 62.50%] [G loss: 4.236760]\n",
      "499 [D loss: 7.882253, acc.: 46.88%] [G loss: 4.429494]\n",
      "500 [D loss: 6.122606, acc.: 59.38%] [G loss: 4.620976]\n",
      "501 [D loss: 5.989376, acc.: 62.50%] [G loss: 3.089647]\n",
      "502 [D loss: 6.000980, acc.: 59.38%] [G loss: 5.036913]\n",
      "503 [D loss: 6.529389, acc.: 53.12%] [G loss: 9.554428]\n",
      "504 [D loss: 5.174046, acc.: 65.62%] [G loss: 8.643917]\n",
      "505 [D loss: 5.573922, acc.: 62.50%] [G loss: 9.822053]\n",
      "506 [D loss: 5.009633, acc.: 68.75%] [G loss: 10.624070]\n",
      "507 [D loss: 6.628521, acc.: 56.25%] [G loss: 11.081191]\n",
      "508 [D loss: 6.339547, acc.: 53.12%] [G loss: 10.073830]\n",
      "509 [D loss: 6.066666, acc.: 59.38%] [G loss: 9.570191]\n",
      "510 [D loss: 4.209868, acc.: 71.88%] [G loss: 6.044309]\n",
      "511 [D loss: 3.492888, acc.: 78.12%] [G loss: 8.857936]\n",
      "512 [D loss: 5.226574, acc.: 65.62%] [G loss: 6.547978]\n",
      "513 [D loss: 3.986072, acc.: 75.00%] [G loss: 4.533215]\n",
      "514 [D loss: 4.483800, acc.: 71.88%] [G loss: 6.516438]\n",
      "515 [D loss: 6.482093, acc.: 59.38%] [G loss: 6.415401]\n",
      "516 [D loss: 4.960335, acc.: 68.75%] [G loss: 5.974835]\n",
      "517 [D loss: 4.993420, acc.: 68.75%] [G loss: 9.069670]\n",
      "518 [D loss: 6.272339, acc.: 59.38%] [G loss: 7.555357]\n",
      "519 [D loss: 3.991089, acc.: 75.00%] [G loss: 6.388506]\n",
      "520 [D loss: 5.989377, acc.: 62.50%] [G loss: 9.078150]\n",
      "521 [D loss: 4.993509, acc.: 68.75%] [G loss: 8.059048]\n",
      "522 [D loss: 5.829490, acc.: 62.50%] [G loss: 5.949341]\n",
      "523 [D loss: 5.485686, acc.: 65.62%] [G loss: 6.902586]\n",
      "524 [D loss: 5.836789, acc.: 62.50%] [G loss: 9.577529]\n",
      "525 [D loss: 5.022477, acc.: 65.62%] [G loss: 5.540595]\n",
      "526 [D loss: 4.948486, acc.: 65.62%] [G loss: 9.134199]\n",
      "527 [D loss: 5.326996, acc.: 65.62%] [G loss: 9.066441]\n",
      "528 [D loss: 6.865499, acc.: 56.25%] [G loss: 10.400987]\n",
      "529 [D loss: 4.516751, acc.: 71.88%] [G loss: 10.686649]\n",
      "530 [D loss: 6.515403, acc.: 59.38%] [G loss: 12.697841]\n",
      "531 [D loss: 7.937156, acc.: 46.88%] [G loss: 7.373763]\n",
      "532 [D loss: 5.040125, acc.: 65.62%] [G loss: 8.730824]\n",
      "533 [D loss: 4.880255, acc.: 65.62%] [G loss: 7.016817]\n",
      "534 [D loss: 6.266849, acc.: 59.38%] [G loss: 6.064347]\n",
      "535 [D loss: 5.866803, acc.: 62.50%] [G loss: 9.674637]\n",
      "536 [D loss: 4.088556, acc.: 71.88%] [G loss: 6.578883]\n",
      "537 [D loss: 7.009573, acc.: 56.25%] [G loss: 6.656193]\n",
      "538 [D loss: 5.485686, acc.: 65.62%] [G loss: 6.547976]\n",
      "539 [D loss: 7.850378, acc.: 50.00%] [G loss: 6.939091]\n",
      "540 [D loss: 6.092409, acc.: 59.38%] [G loss: 4.533216]\n",
      "541 [D loss: 5.983886, acc.: 62.50%] [G loss: 5.310320]\n",
      "542 [D loss: 5.523274, acc.: 62.50%] [G loss: 5.540595]\n",
      "543 [D loss: 5.506871, acc.: 62.50%] [G loss: 6.044286]\n",
      "544 [D loss: 4.057155, acc.: 71.88%] [G loss: 8.562950]\n",
      "545 [D loss: 6.793326, acc.: 56.25%] [G loss: 9.870234]\n",
      "546 [D loss: 5.861279, acc.: 62.50%] [G loss: 4.547289]\n",
      "547 [D loss: 3.697231, acc.: 75.00%] [G loss: 7.492421]\n",
      "548 [D loss: 4.489287, acc.: 71.88%] [G loss: 8.071211]\n",
      "549 [D loss: 4.489287, acc.: 71.88%] [G loss: 8.563170]\n",
      "550 [D loss: 3.635041, acc.: 71.88%] [G loss: 7.555357]\n",
      "551 [D loss: 4.500269, acc.: 71.88%] [G loss: 8.562845]\n",
      "552 [D loss: 3.568653, acc.: 75.00%] [G loss: 7.097413]\n",
      "553 [D loss: 5.491177, acc.: 65.62%] [G loss: 7.555357]\n",
      "554 [D loss: 3.487432, acc.: 78.12%] [G loss: 6.547976]\n",
      "555 [D loss: 2.989197, acc.: 81.25%] [G loss: 6.897009]\n",
      "556 [D loss: 4.483840, acc.: 71.88%] [G loss: 7.051667]\n",
      "557 [D loss: 4.981996, acc.: 68.75%] [G loss: 7.762323]\n",
      "558 [D loss: 5.983886, acc.: 62.50%] [G loss: 6.547976]\n",
      "559 [D loss: 5.488360, acc.: 65.62%] [G loss: 7.051667]\n",
      "560 [D loss: 4.560743, acc.: 68.75%] [G loss: 8.059048]\n",
      "561 [D loss: 3.492890, acc.: 78.12%] [G loss: 9.066488]\n",
      "562 [D loss: 5.513190, acc.: 65.62%] [G loss: 10.305309]\n",
      "563 [D loss: 7.648556, acc.: 50.00%] [G loss: 8.562738]\n",
      "564 [D loss: 5.364021, acc.: 62.50%] [G loss: 10.604233]\n",
      "565 [D loss: 5.014941, acc.: 68.75%] [G loss: 12.502213]\n",
      "566 [D loss: 4.007569, acc.: 75.00%] [G loss: 12.863607]\n",
      "567 [D loss: 9.285213, acc.: 40.62%] [G loss: 2.342364]\n",
      "568 [D loss: 5.696932, acc.: 62.50%] [G loss: 4.176527]\n",
      "569 [D loss: 5.978395, acc.: 62.50%] [G loss: 3.022143]\n",
      "570 [D loss: 5.491177, acc.: 65.62%] [G loss: 7.051667]\n",
      "571 [D loss: 4.708190, acc.: 68.75%] [G loss: 6.291106]\n",
      "572 [D loss: 3.985596, acc.: 75.00%] [G loss: 6.547976]\n",
      "573 [D loss: 3.100322, acc.: 78.12%] [G loss: 6.416619]\n",
      "574 [D loss: 4.987487, acc.: 68.75%] [G loss: 8.379002]\n",
      "575 [D loss: 3.803739, acc.: 75.00%] [G loss: 9.713976]\n",
      "576 [D loss: 3.736733, acc.: 75.00%] [G loss: 12.089820]\n",
      "577 [D loss: 4.070033, acc.: 71.88%] [G loss: 14.103334]\n",
      "578 [D loss: 6.135053, acc.: 59.38%] [G loss: 6.965545]\n",
      "579 [D loss: 5.757910, acc.: 62.50%] [G loss: 7.480568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 [D loss: 4.489287, acc.: 71.88%] [G loss: 5.959420]\n",
      "581 [D loss: 5.520773, acc.: 62.50%] [G loss: 9.559195]\n",
      "582 [D loss: 5.413846, acc.: 59.38%] [G loss: 11.927180]\n",
      "583 [D loss: 5.507875, acc.: 65.62%] [G loss: 12.608532]\n",
      "584 [D loss: 7.076441, acc.: 53.12%] [G loss: 8.059080]\n",
      "585 [D loss: 5.024969, acc.: 65.62%] [G loss: 7.744893]\n",
      "586 [D loss: 4.303747, acc.: 71.88%] [G loss: 12.088031]\n",
      "587 [D loss: 7.943032, acc.: 50.00%] [G loss: 11.283926]\n",
      "588 [D loss: 6.782081, acc.: 56.25%] [G loss: 10.073810]\n",
      "589 [D loss: 5.839043, acc.: 62.50%] [G loss: 7.051667]\n",
      "590 [D loss: 5.485750, acc.: 65.62%] [G loss: 6.777018]\n",
      "591 [D loss: 5.491205, acc.: 65.62%] [G loss: 5.540595]\n",
      "592 [D loss: 4.981996, acc.: 68.75%] [G loss: 5.896921]\n",
      "593 [D loss: 4.982167, acc.: 68.75%] [G loss: 5.540595]\n",
      "594 [D loss: 5.480195, acc.: 65.62%] [G loss: 6.547988]\n",
      "595 [D loss: 4.441031, acc.: 71.88%] [G loss: 5.037030]\n",
      "596 [D loss: 6.904419, acc.: 56.25%] [G loss: 5.036905]\n",
      "597 [D loss: 6.476762, acc.: 59.38%] [G loss: 5.326116]\n",
      "598 [D loss: 5.480195, acc.: 65.62%] [G loss: 5.805400]\n",
      "599 [D loss: 7.826652, acc.: 50.00%] [G loss: 6.484890]\n",
      "600 [D loss: 4.981996, acc.: 68.75%] [G loss: 5.540637]\n",
      "601 [D loss: 4.582484, acc.: 68.75%] [G loss: 9.432062]\n",
      "602 [D loss: 4.610841, acc.: 65.62%] [G loss: 10.178276]\n",
      "603 [D loss: 6.183208, acc.: 53.12%] [G loss: 11.482650]\n",
      "604 [D loss: 6.408865, acc.: 53.12%] [G loss: 10.078008]\n",
      "605 [D loss: 5.927419, acc.: 62.50%] [G loss: 8.059048]\n",
      "606 [D loss: 8.401249, acc.: 43.75%] [G loss: 8.059048]\n",
      "607 [D loss: 4.489287, acc.: 71.88%] [G loss: 7.051667]\n",
      "608 [D loss: 4.987489, acc.: 68.75%] [G loss: 8.307237]\n",
      "609 [D loss: 6.728096, acc.: 56.25%] [G loss: 7.051720]\n",
      "610 [D loss: 7.104328, acc.: 53.12%] [G loss: 5.036905]\n",
      "611 [D loss: 6.142067, acc.: 59.38%] [G loss: 5.088518]\n",
      "612 [D loss: 5.989377, acc.: 62.50%] [G loss: 6.351588]\n",
      "613 [D loss: 7.998648, acc.: 50.00%] [G loss: 7.672000]\n",
      "614 [D loss: 6.545870, acc.: 56.25%] [G loss: 5.541005]\n",
      "615 [D loss: 6.821055, acc.: 56.25%] [G loss: 5.422506]\n",
      "616 [D loss: 6.118404, acc.: 59.38%] [G loss: 8.075060]\n",
      "617 [D loss: 4.494778, acc.: 71.88%] [G loss: 6.875944]\n",
      "618 [D loss: 5.170438, acc.: 65.62%] [G loss: 8.454951]\n",
      "619 [D loss: 5.505116, acc.: 65.62%] [G loss: 7.051667]\n",
      "620 [D loss: 7.505939, acc.: 53.12%] [G loss: 7.555357]\n",
      "621 [D loss: 4.689397, acc.: 68.75%] [G loss: 8.201068]\n",
      "622 [D loss: 4.052608, acc.: 71.88%] [G loss: 6.191593]\n",
      "623 [D loss: 6.493067, acc.: 59.38%] [G loss: 7.555716]\n",
      "624 [D loss: 6.504051, acc.: 59.38%] [G loss: 7.051667]\n",
      "625 [D loss: 5.492990, acc.: 65.62%] [G loss: 7.535441]\n",
      "626 [D loss: 5.496668, acc.: 65.62%] [G loss: 7.443547]\n",
      "627 [D loss: 5.491177, acc.: 65.62%] [G loss: 6.547976]\n",
      "628 [D loss: 6.910257, acc.: 56.25%] [G loss: 6.092356]\n",
      "629 [D loss: 5.983886, acc.: 62.50%] [G loss: 4.539967]\n",
      "630 [D loss: 6.810361, acc.: 56.25%] [G loss: 3.525835]\n",
      "631 [D loss: 7.478484, acc.: 53.12%] [G loss: 6.895277]\n",
      "632 [D loss: 6.915154, acc.: 56.25%] [G loss: 4.533215]\n",
      "633 [D loss: 5.983886, acc.: 62.50%] [G loss: 4.533215]\n",
      "634 [D loss: 5.988793, acc.: 62.50%] [G loss: 2.850819]\n",
      "635 [D loss: 5.480195, acc.: 65.62%] [G loss: 3.022245]\n",
      "636 [D loss: 6.476594, acc.: 59.38%] [G loss: 3.635021]\n",
      "637 [D loss: 5.485686, acc.: 65.62%] [G loss: 5.445666]\n",
      "638 [D loss: 6.808910, acc.: 56.25%] [G loss: 2.525809]\n",
      "639 [D loss: 5.784063, acc.: 62.50%] [G loss: 4.064828]\n",
      "640 [D loss: 4.981996, acc.: 68.75%] [G loss: 4.029524]\n",
      "641 [D loss: 3.991087, acc.: 75.00%] [G loss: 6.479382]\n",
      "642 [D loss: 4.987491, acc.: 68.75%] [G loss: 5.540595]\n",
      "643 [D loss: 7.269959, acc.: 53.12%] [G loss: 5.108627]\n",
      "644 [D loss: 6.476594, acc.: 59.38%] [G loss: 3.184615]\n",
      "645 [D loss: 5.480195, acc.: 65.62%] [G loss: 3.525833]\n",
      "646 [D loss: 5.480195, acc.: 65.62%] [G loss: 4.375233]\n",
      "647 [D loss: 5.978395, acc.: 62.50%] [G loss: 5.540596]\n",
      "648 [D loss: 6.469347, acc.: 59.38%] [G loss: 4.565437]\n",
      "649 [D loss: 6.882125, acc.: 56.25%] [G loss: 3.907346]\n",
      "650 [D loss: 6.533976, acc.: 56.25%] [G loss: 4.029524]\n",
      "651 [D loss: 5.983886, acc.: 62.50%] [G loss: 3.022143]\n",
      "652 [D loss: 5.981676, acc.: 62.50%] [G loss: 4.093639]\n",
      "653 [D loss: 5.553105, acc.: 62.50%] [G loss: 5.036937]\n",
      "654 [D loss: 6.018991, acc.: 59.38%] [G loss: 4.533215]\n",
      "655 [D loss: 5.435370, acc.: 62.50%] [G loss: 7.555934]\n",
      "656 [D loss: 7.392178, acc.: 53.12%] [G loss: 6.916198]\n",
      "657 [D loss: 7.014518, acc.: 56.25%] [G loss: 7.103036]\n",
      "658 [D loss: 8.267815, acc.: 46.88%] [G loss: 6.327847]\n",
      "659 [D loss: 4.489287, acc.: 71.88%] [G loss: 6.547976]\n",
      "660 [D loss: 7.505947, acc.: 53.12%] [G loss: 7.251429]\n",
      "661 [D loss: 4.259936, acc.: 71.88%] [G loss: 7.290994]\n",
      "662 [D loss: 6.498591, acc.: 59.38%] [G loss: 7.053196]\n",
      "663 [D loss: 6.615313, acc.: 53.12%] [G loss: 6.980269]\n",
      "664 [D loss: 6.559624, acc.: 56.25%] [G loss: 12.148895]\n",
      "665 [D loss: 6.524726, acc.: 56.25%] [G loss: 14.697650]\n",
      "666 [D loss: 8.062201, acc.: 50.00%] [G loss: 14.607712]\n",
      "667 [D loss: 8.059193, acc.: 50.00%] [G loss: 14.272196]\n",
      "668 [D loss: 8.564560, acc.: 46.88%] [G loss: 15.110714]\n",
      "669 [D loss: 8.059048, acc.: 50.00%] [G loss: 13.540663]\n",
      "670 [D loss: 9.055447, acc.: 43.75%] [G loss: 15.019509]\n",
      "671 [D loss: 9.055447, acc.: 43.75%] [G loss: 12.771210]\n",
      "672 [D loss: 8.059048, acc.: 50.00%] [G loss: 14.170197]\n",
      "673 [D loss: 8.546414, acc.: 46.88%] [G loss: 13.069862]\n",
      "674 [D loss: 8.276729, acc.: 46.88%] [G loss: 12.087468]\n",
      "675 [D loss: 9.355462, acc.: 37.50%] [G loss: 12.788031]\n",
      "676 [D loss: 8.808104, acc.: 40.62%] [G loss: 14.781237]\n",
      "677 [D loss: 7.555357, acc.: 53.12%] [G loss: 13.599644]\n",
      "678 [D loss: 9.410273, acc.: 37.50%] [G loss: 14.614989]\n",
      "679 [D loss: 8.557247, acc.: 46.88%] [G loss: 14.103773]\n",
      "680 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.395085]\n",
      "681 [D loss: 8.557247, acc.: 46.88%] [G loss: 14.607023]\n",
      "682 [D loss: 8.053562, acc.: 50.00%] [G loss: 15.614405]\n",
      "683 [D loss: 9.537422, acc.: 37.50%] [G loss: 16.118095]\n",
      "684 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "685 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "686 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.110714]\n",
      "687 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "688 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "689 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "690 [D loss: 9.055447, acc.: 43.75%] [G loss: 16.118095]\n",
      "691 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "692 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.110782]\n",
      "693 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "694 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "695 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.110717]\n",
      "696 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.470709]\n",
      "697 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "698 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614501]\n",
      "699 [D loss: 8.059048, acc.: 50.00%] [G loss: 14.828159]\n",
      "700 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.812887]\n",
      "701 [D loss: 8.059048, acc.: 50.00%] [G loss: 14.627304]\n",
      "702 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.614405]\n",
      "703 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.614405]\n",
      "704 [D loss: 9.055447, acc.: 43.75%] [G loss: 15.614408]\n",
      "705 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.614405]\n",
      "706 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.110714]\n",
      "707 [D loss: 9.055447, acc.: 43.75%] [G loss: 15.110714]\n",
      "708 [D loss: 8.557247, acc.: 46.88%] [G loss: 14.607031]\n",
      "709 [D loss: 8.255704, acc.: 46.88%] [G loss: 15.005316]\n",
      "710 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.110714]\n",
      "711 [D loss: 8.059050, acc.: 50.00%] [G loss: 15.210299]\n",
      "712 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "713 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.267741]\n",
      "714 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.798697]\n",
      "715 [D loss: 9.055447, acc.: 43.75%] [G loss: 15.719829]\n",
      "716 [D loss: 9.732738, acc.: 37.50%] [G loss: 16.118095]\n",
      "717 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "718 [D loss: 8.070736, acc.: 50.00%] [G loss: 15.514598]\n",
      "719 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.614445]\n",
      "720 [D loss: 8.059048, acc.: 50.00%] [G loss: 14.597157]\n",
      "721 [D loss: 8.557247, acc.: 46.88%] [G loss: 13.614084]\n",
      "722 [D loss: 9.055447, acc.: 43.75%] [G loss: 13.139102]\n",
      "723 [D loss: 9.055447, acc.: 43.75%] [G loss: 14.539829]\n",
      "724 [D loss: 8.557247, acc.: 46.88%] [G loss: 14.103333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725 [D loss: 9.096541, acc.: 40.62%] [G loss: 15.110714]\n",
      "726 [D loss: 9.055447, acc.: 43.75%] [G loss: 15.110716]\n",
      "727 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.111976]\n",
      "728 [D loss: 8.784017, acc.: 43.75%] [G loss: 15.628025]\n",
      "729 [D loss: 8.557247, acc.: 46.88%] [G loss: 14.924149]\n",
      "730 [D loss: 9.553646, acc.: 40.62%] [G loss: 15.614405]\n",
      "731 [D loss: 8.238914, acc.: 46.88%] [G loss: 16.118095]\n",
      "732 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.105194]\n",
      "733 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "734 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "735 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "736 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "737 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.657887]\n",
      "738 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "739 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.113022]\n",
      "740 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "741 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.062578]\n",
      "742 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "743 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "744 [D loss: 8.557247, acc.: 46.88%] [G loss: 15.614405]\n",
      "745 [D loss: 9.055447, acc.: 43.75%] [G loss: 15.110794]\n",
      "746 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.523632]\n",
      "747 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "748 [D loss: 8.101912, acc.: 46.88%] [G loss: 15.614405]\n",
      "749 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.411877]\n",
      "750 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "751 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.758520]\n",
      "752 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "753 [D loss: 8.073645, acc.: 50.00%] [G loss: 16.118095]\n",
      "754 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "755 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.664722]\n",
      "756 [D loss: 8.059076, acc.: 50.00%] [G loss: 15.964259]\n",
      "757 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.355094]\n",
      "758 [D loss: 8.059117, acc.: 50.00%] [G loss: 16.118095]\n",
      "759 [D loss: 8.557247, acc.: 46.88%] [G loss: 16.118095]\n",
      "760 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "761 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.090834]\n",
      "762 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.049202]\n",
      "763 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.614405]\n",
      "764 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "765 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "766 [D loss: 8.059048, acc.: 50.00%] [G loss: 15.435074]\n",
      "767 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "768 [D loss: 8.245598, acc.: 46.88%] [G loss: 16.118095]\n",
      "769 [D loss: 8.252665, acc.: 46.88%] [G loss: 16.118095]\n",
      "770 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "771 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "772 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "773 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "774 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "775 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "776 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "777 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "778 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "779 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "780 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "781 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "782 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "783 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "784 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "785 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "786 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "787 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "788 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "789 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "790 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "791 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "792 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "793 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "794 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "795 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "796 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "797 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "798 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "799 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "800 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "801 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "802 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "803 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "804 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "805 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "806 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "807 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "808 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "809 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "810 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "811 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "812 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "813 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "814 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "815 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "816 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "817 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "818 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "819 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "820 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "821 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "822 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "823 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "824 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "825 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "826 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "827 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "828 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "829 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "830 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "831 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "832 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "833 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "834 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "835 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "836 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "837 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "838 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "839 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "840 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "841 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "842 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "843 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "844 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "845 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "846 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "847 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "848 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "849 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "850 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "851 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "852 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "853 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "854 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "855 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "856 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "857 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "858 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "859 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "860 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "861 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "862 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "863 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "864 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "865 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "866 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "867 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "869 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "870 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "871 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "872 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "873 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "874 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "875 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "876 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "877 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "878 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "879 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "880 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "881 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "882 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "883 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "884 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "885 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "886 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "887 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "888 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "889 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "890 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "891 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "892 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "893 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "894 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "895 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "896 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "897 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "898 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "899 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "900 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "901 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "902 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "903 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "904 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "905 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "906 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "907 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "908 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "909 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "910 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "911 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "912 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "913 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "914 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "915 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "916 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "917 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "918 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "919 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "920 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "921 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "922 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "923 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "924 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "925 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "926 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "927 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "928 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "929 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "930 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "931 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "932 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "933 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "934 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "935 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "936 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "937 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "938 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "939 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "940 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "941 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "942 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "943 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "944 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "945 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "946 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "947 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "948 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "949 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "950 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "951 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "952 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "953 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "954 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "955 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "956 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "957 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "958 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "959 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "960 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "961 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "962 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "963 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "964 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "965 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "966 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "967 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "968 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "969 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "970 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "971 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "972 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "973 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "974 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "975 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "976 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "977 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "978 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "979 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "980 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "981 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "982 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "983 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "984 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "985 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "986 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "987 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "988 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "989 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "990 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "991 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "992 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "993 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "994 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "995 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "996 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "997 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "998 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "999 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1000 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1001 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1002 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1003 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1004 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1005 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1006 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1007 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1008 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1009 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1010 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1012 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1013 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1014 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1015 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1016 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1017 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1018 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1019 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1020 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1021 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1022 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1023 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1024 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1025 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1026 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1027 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1028 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1029 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1030 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1031 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1032 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n",
      "1033 [D loss: 8.059048, acc.: 50.00%] [G loss: 16.118095]\n"
     ]
    }
   ],
   "source": [
    "history = train(epochs=30000, batch_size=32, save_interval=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
