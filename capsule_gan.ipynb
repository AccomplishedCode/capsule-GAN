{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules imported.\n",
      "Devices: [name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15153974355859996784\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 140865536\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15625452983944590388\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 660, pci bus id: 0000:01:00.0, compute capability: 3.0\"\n",
      "]\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# math libraries\n",
    "import numpy as np\n",
    "\n",
    "# ml libraries\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda, Concatenate, Multiply\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# sys and helpers\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "print('Modules imported.')\n",
    "\n",
    "\n",
    "\n",
    "# device check\n",
    "from tensorflow.python.client import device_lib\n",
    "print('Devices:', device_lib.list_local_devices())\n",
    "\n",
    "# GPU check\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset, width, height, channels):\n",
    "    \n",
    "    if dataset == 'mnist':\n",
    "        # load MNIST data\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "        # rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        \n",
    "    if dataset == 'cifar10':\n",
    "        # load MNIST data\n",
    "        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        # rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        \n",
    "    # defining input dims\n",
    "    img_rows = width\n",
    "    img_cols = height\n",
    "    channels = channels\n",
    "    img_shape = [img_rows, img_cols, channels]\n",
    "    \n",
    "    return [X_train, img_shape]\n",
    "    \n",
    "\n",
    "# if MNIST ('mnist', 28, 28, 1) if CIFAR10 ('cifar10', 32, 32, 3)\n",
    "dataset = load_dataset('cifar10', 32, 32, 3)\n",
    "print(dataset[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for Capsule layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squash function of capsule layers, borrowed from Xifeng Guo's implementation of Keras CapsNet `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator structure\n",
    "def build_discriminator():\n",
    "\n",
    "    \"\"\"\n",
    "    This is the part my 'Capsule Layer as a Discriminator in Generative Adversarial Networks' paper focuses on\n",
    "    as it introduces a new structure to the discriminator of DCGAN by using Capsule Network architecture of original\n",
    "    'Dynamic Routing Between Capsules' paper by S. Sabour, N. Frosst and G. Hinton.\n",
    "    \n",
    "    Discriminator takes real/generated images and outputs its prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # depending on dataset we define input shape for our network\n",
    "    img = Input(shape=dataset[1])\n",
    "\n",
    "    # first typical convlayer outputs a 20x20x256 matrix\n",
    "    x = Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    # original 'Dynamic Routing Between Capsules' paper does not include the batch norm layer after the first conv group\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Capsule architecture starts from here.\n",
    "    \"\"\"\n",
    "    #\n",
    "    # primarycaps coming first\n",
    "    #\n",
    "    \n",
    "    # filters 256 (n_vectors=8 * channels=32)\n",
    "    x = Conv2D(filters=8 * 32, kernel_size=9, strides=2, padding='valid', name='primarycap_conv2')(x)\n",
    "    \n",
    "    # reshape into the 8D vector for all 32 feature maps combined\n",
    "    # (primary capsule has collections of activations which denote the orientation of digit\n",
    "    # while intensity of the vector which denotes the presence of the digit)\n",
    "    x = Reshape(target_shape=[-1, 8], name='primarycap_reshape')(x)\n",
    "    \n",
    "    # the purpose is to output a number between 0 and 1 for each capsule where the length of the input decides the amount\n",
    "    x = Lambda(squash, name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "\n",
    "\n",
    "    #\n",
    "    # digitcaps are here\n",
    "    #\n",
    "    \"\"\"\n",
    "    NOTE: My approach is a simplified version of digitcaps i.e. without expanding the dimensions into\n",
    "    [None, 1, input_n_vectors, input_dim_capsule (feature maps)]\n",
    "    and tiling it into [None, num_capsule, input_n_vectors, input_dim_capsule (feature maps)].\n",
    "    Instead I replace it with ordinary Keras Dense layers as weight holders in the following lines.\n",
    "    \n",
    "    ANY CORRECTIONS ARE APPRECIATED IN THIS PART, PLEASE SUBMIT PULL REQUESTS!\n",
    "    \"\"\"\n",
    "    x = Flatten()(x)\n",
    "    # capsule (i) in a lower-level layer needs to decide how to send its output vector to higher-level capsules (j)\n",
    "    # it makes this decision by changing scalar weight (c=coupling coefficient) that will multiply its output vector and then be treated as input to a higher-level capsule\n",
    "    #\n",
    "    # uhat = prediction vector, w = weight matrix but will act as a dense layer, u = output from a previous layer\n",
    "    # uhat = u * w\n",
    "    # neurons 160 (num_capsules=10 * num_vectors=16)\n",
    "    uhat = Dense(160, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    # c = coupling coefficient (softmax over the bias weights, log prior) | \"the coupling coefficients between capsule (i) and all the capsules in the layer above sum to 1\"\n",
    "    # we treat the coupling coefficiant as a softmax over bias weights from the previous dense layer\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    # s_j (output of the current capsule level) = uhat * c (coupling coefficient)\n",
    "    c = Dense(160)(c) #apply a final squashing function\n",
    "    x = Multiply()([uhat, c])\n",
    "    \"\"\"\n",
    "    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n",
    "    \"\"\"\n",
    "    v_j = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "    #\n",
    "    # we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n",
    "    #\n",
    "    c = Activation('softmax', name='softmax_digitcaps2')(v_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(160)(c) #apply a final squashing function\n",
    "    x = Multiply()([uhat, c])\n",
    "    v_j = LeakyReLU()(x)\n",
    "\n",
    "    c = Activation('softmax', name='softmax_digitcaps3')(v_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(160)(c) #apply a final squashing function\n",
    "    x = Multiply()([uhat, c])\n",
    "    v_j = LeakyReLU()(x)\n",
    "\n",
    "    pred = Dense(1, activation='sigmoid')(v_j)\n",
    "\n",
    "    \n",
    "    return Model(img, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISCRIMINATOR:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_23 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 24, 24, 256)  62464       input_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 24, 24, 256)  0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 24, 24, 256)  1024        leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2 (Conv2D)       (None, 8, 8, 256)    5308672     batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 2048, 8)      0           primarycap_conv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 2048, 8)      0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 2048, 8)      32          primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 16384)        0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "uhat_digitcaps (Dense)          (None, 160)          2621600     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_digitcaps1 (Activation) (None, 160)          0           uhat_digitcaps[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 160)          25760       softmax_digitcaps1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 160)          0           uhat_digitcaps[0][0]             \n",
      "                                                                 dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 160)          0           multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "softmax_digitcaps2 (Activation) (None, 160)          0           leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 160)          25760       softmax_digitcaps2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 160)          0           uhat_digitcaps[0][0]             \n",
      "                                                                 dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 160)          0           multiply_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "softmax_digitcaps3 (Activation) (None, 160)          0           leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 160)          25760       softmax_digitcaps3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 160)          0           uhat_digitcaps[0][0]             \n",
      "                                                                 dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 160)          0           multiply_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            161         leaky_re_lu_28[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 8,071,233\n",
      "Trainable params: 8,070,705\n",
      "Non-trainable params: 528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "print('DISCRIMINATOR:')\n",
    "discriminator.summary()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator structure\n",
    "def build_generator():\n",
    "\n",
    "    \"\"\"\n",
    "    Generator follows the DCGAN architecture and creates generated image representations through learning.\n",
    "    \"\"\"\n",
    "\n",
    "    noise_shape = (100,)\n",
    "    x_noise = Input(shape=noise_shape)\n",
    "\n",
    "    if (dataset[0].shape[1] == 28 and dataset[0].shape[2] == 28):\n",
    "        x = Dense(128 * 7 * 7, activation=\"relu\")(x_noise)\n",
    "        x = Reshape((7, 7, 128))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(128, kernel_size=3, padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n",
    "        gen_out = Activation(\"tanh\")(x)\n",
    "        \n",
    "        return Model(x_noise, gen_out)\n",
    "\n",
    "    if (dataset[0].shape[1] == 32 and dataset[0].shape[2] == 32):\n",
    "        x = Dense(128 * 8 * 8, activation=\"relu\")(x_noise)\n",
    "        x = Reshape((8, 8, 128))(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(128, kernel_size=3, padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = UpSampling2D()(x)\n",
    "        x = Conv2D(64, kernel_size=3, padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(momentum=0.8)(x)\n",
    "        x = Conv2D(3, kernel_size=3, padding=\"same\")(x)\n",
    "        gen_out = Activation(\"tanh\")(x)\n",
    "        \n",
    "        return Model(x_noise, gen_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATOR:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 32, 32, 3)         1731      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,051,779\n",
      "Trainable params: 1,051,139\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build and compile the generator\n",
    "generator = build_generator()\n",
    "print('GENERATOR:')\n",
    "generator.summary()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding noise to generator\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the combined model we will only train the generator\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to discriminate generated images\n",
    "valid = discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_21 (Model)             (None, 32, 32, 3)         1051779   \n",
      "_________________________________________________________________\n",
      "model_20 (Model)             (None, 1)                 8071233   \n",
      "=================================================================\n",
      "Total params: 9,123,012\n",
      "Trainable params: 1,051,139\n",
      "Non-trainable params: 8,071,873\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the combined model (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "print('COMBINED:')\n",
    "combined.summary()\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss values for further plotting\n",
    "D_L_REAL = []\n",
    "D_L_FAKE = []\n",
    "D_L = []\n",
    "D_ACC = []\n",
    "G_L = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_title, epochs, batch_size=32, save_interval=50):\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # select a random half batch of images\n",
    "            idx = np.random.randint(0, dataset[0].shape[0], half_batch)\n",
    "            imgs = dataset[0][idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "\n",
    "            # generate a half batch of new images\n",
    "            gen_imgs = generator.predict(noise)\n",
    "\n",
    "            # train the discriminator by feeding both real and fake (generated) images one by one\n",
    "            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1))*0.9)\n",
    "            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # the generator wants the discriminator to label the generated samples\n",
    "            # as valid (ones)\n",
    "            valid_y = np.array([1] * 32)\n",
    "\n",
    "            # train the generator\n",
    "            g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "            D_L_REAL.append(d_loss_real)\n",
    "            D_L_FAKE.append(d_loss_fake)\n",
    "            D_L.append(d_loss)\n",
    "            D_ACC.append(d_loss[1])\n",
    "            G_L.append(g_loss)\n",
    "\n",
    "            # if at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                save_imgs(dataset_title, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(dataset_title, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        \n",
    "        # iterate in order to create a subplot\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                if dataset_title == 'mnist':\n",
    "                    axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                    axs[i,j].axis('off')\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    axs[i,j].imshow(gen_imgs[cnt, :,:,:])\n",
    "                    axs[i,j].axis('off')\n",
    "                    cnt += 1\n",
    "        \n",
    "        if not os.path.exists('images_{0}'.format(dataset_title)):\n",
    "            os.makedirs('images_{0}'.format(dataset_title))\n",
    "        \n",
    "        fig.savefig(\"images_{0}/{1}.png\".format(dataset_title, epoch))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husey_000\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.497025, acc.: 40.62%] [G loss: 2.438772]\n",
      "1 [D loss: 0.636038, acc.: 34.38%] [G loss: 2.666818]\n",
      "2 [D loss: 0.593567, acc.: 43.75%] [G loss: 2.773867]\n",
      "3 [D loss: 0.510709, acc.: 46.88%] [G loss: 2.553872]\n",
      "4 [D loss: 0.574744, acc.: 43.75%] [G loss: 2.130646]\n",
      "5 [D loss: 0.300679, acc.: 50.00%] [G loss: 2.806822]\n",
      "6 [D loss: 0.322257, acc.: 46.88%] [G loss: 2.919825]\n",
      "7 [D loss: 0.422623, acc.: 46.88%] [G loss: 2.932542]\n",
      "8 [D loss: 0.335914, acc.: 46.88%] [G loss: 3.067499]\n",
      "9 [D loss: 0.336702, acc.: 50.00%] [G loss: 3.509737]\n",
      "10 [D loss: 0.255629, acc.: 46.88%] [G loss: 3.008981]\n",
      "11 [D loss: 0.404564, acc.: 50.00%] [G loss: 3.606637]\n",
      "12 [D loss: 0.453569, acc.: 40.62%] [G loss: 3.589422]\n",
      "13 [D loss: 0.389363, acc.: 43.75%] [G loss: 3.719497]\n",
      "14 [D loss: 0.326614, acc.: 50.00%] [G loss: 3.384321]\n",
      "15 [D loss: 0.661039, acc.: 46.88%] [G loss: 2.624905]\n",
      "16 [D loss: 0.347018, acc.: 43.75%] [G loss: 1.927030]\n",
      "17 [D loss: 0.475677, acc.: 37.50%] [G loss: 2.681320]\n",
      "18 [D loss: 0.453822, acc.: 40.62%] [G loss: 2.147118]\n",
      "19 [D loss: 0.364427, acc.: 46.88%] [G loss: 1.596512]\n",
      "20 [D loss: 0.646024, acc.: 40.62%] [G loss: 1.475868]\n",
      "21 [D loss: 0.553129, acc.: 34.38%] [G loss: 1.810756]\n",
      "22 [D loss: 0.440885, acc.: 31.25%] [G loss: 2.298565]\n",
      "23 [D loss: 0.746068, acc.: 34.38%] [G loss: 1.967915]\n",
      "24 [D loss: 1.000391, acc.: 34.38%] [G loss: 2.393623]\n",
      "25 [D loss: 0.400954, acc.: 50.00%] [G loss: 2.565679]\n",
      "26 [D loss: 0.460426, acc.: 40.62%] [G loss: 2.075987]\n",
      "27 [D loss: 0.593868, acc.: 50.00%] [G loss: 2.434138]\n",
      "28 [D loss: 0.499377, acc.: 43.75%] [G loss: 2.908983]\n",
      "29 [D loss: 0.253572, acc.: 50.00%] [G loss: 2.851310]\n",
      "30 [D loss: 0.674851, acc.: 31.25%] [G loss: 2.852493]\n",
      "31 [D loss: 0.462336, acc.: 40.62%] [G loss: 2.512202]\n",
      "32 [D loss: 0.571140, acc.: 43.75%] [G loss: 2.102859]\n",
      "33 [D loss: 0.396371, acc.: 46.88%] [G loss: 2.871307]\n",
      "34 [D loss: 0.263548, acc.: 50.00%] [G loss: 3.201455]\n",
      "35 [D loss: 0.242634, acc.: 50.00%] [G loss: 2.670413]\n",
      "36 [D loss: 0.245577, acc.: 50.00%] [G loss: 2.540263]\n",
      "37 [D loss: 0.251411, acc.: 50.00%] [G loss: 2.886940]\n",
      "38 [D loss: 0.428628, acc.: 46.88%] [G loss: 2.062133]\n",
      "39 [D loss: 0.721078, acc.: 34.38%] [G loss: 1.613655]\n",
      "40 [D loss: 0.625976, acc.: 37.50%] [G loss: 2.187530]\n",
      "41 [D loss: 0.331168, acc.: 43.75%] [G loss: 2.545004]\n",
      "42 [D loss: 0.485456, acc.: 46.88%] [G loss: 2.278392]\n",
      "43 [D loss: 0.551604, acc.: 28.12%] [G loss: 2.391145]\n",
      "44 [D loss: 0.367901, acc.: 43.75%] [G loss: 2.813069]\n",
      "45 [D loss: 0.462510, acc.: 43.75%] [G loss: 2.375679]\n",
      "46 [D loss: 0.692406, acc.: 46.88%] [G loss: 2.381375]\n",
      "47 [D loss: 0.425429, acc.: 46.88%] [G loss: 1.790570]\n",
      "48 [D loss: 0.558897, acc.: 43.75%] [G loss: 2.504516]\n",
      "49 [D loss: 0.430293, acc.: 46.88%] [G loss: 2.196546]\n",
      "50 [D loss: 0.430586, acc.: 37.50%] [G loss: 1.769773]\n",
      "51 [D loss: 0.696264, acc.: 43.75%] [G loss: 1.916065]\n",
      "52 [D loss: 0.314106, acc.: 46.88%] [G loss: 1.906652]\n",
      "53 [D loss: 0.345775, acc.: 46.88%] [G loss: 1.943610]\n",
      "54 [D loss: 0.381498, acc.: 43.75%] [G loss: 2.632547]\n",
      "55 [D loss: 0.258693, acc.: 50.00%] [G loss: 3.311278]\n",
      "56 [D loss: 0.556041, acc.: 46.88%] [G loss: 1.975012]\n",
      "57 [D loss: 0.431815, acc.: 40.62%] [G loss: 2.161243]\n",
      "58 [D loss: 0.537658, acc.: 31.25%] [G loss: 2.095673]\n",
      "59 [D loss: 1.233350, acc.: 34.38%] [G loss: 2.184525]\n",
      "60 [D loss: 0.364622, acc.: 43.75%] [G loss: 2.142449]\n",
      "61 [D loss: 0.297996, acc.: 50.00%] [G loss: 2.306415]\n",
      "62 [D loss: 0.387648, acc.: 40.62%] [G loss: 2.591074]\n",
      "63 [D loss: 0.266434, acc.: 50.00%] [G loss: 2.941725]\n",
      "64 [D loss: 0.495819, acc.: 46.88%] [G loss: 2.688104]\n",
      "65 [D loss: 0.930753, acc.: 34.38%] [G loss: 2.851217]\n",
      "66 [D loss: 0.769942, acc.: 43.75%] [G loss: 1.924170]\n",
      "67 [D loss: 0.774498, acc.: 34.38%] [G loss: 1.273903]\n",
      "68 [D loss: 0.594964, acc.: 25.00%] [G loss: 1.705647]\n",
      "69 [D loss: 0.516674, acc.: 37.50%] [G loss: 1.780914]\n",
      "70 [D loss: 0.435198, acc.: 40.62%] [G loss: 2.558258]\n",
      "71 [D loss: 0.405349, acc.: 50.00%] [G loss: 2.220511]\n",
      "72 [D loss: 0.316773, acc.: 50.00%] [G loss: 2.075603]\n",
      "73 [D loss: 0.327496, acc.: 50.00%] [G loss: 2.498692]\n",
      "74 [D loss: 0.397874, acc.: 50.00%] [G loss: 2.018695]\n",
      "75 [D loss: 0.561638, acc.: 43.75%] [G loss: 2.038072]\n",
      "76 [D loss: 0.299417, acc.: 50.00%] [G loss: 2.643710]\n",
      "77 [D loss: 0.332978, acc.: 50.00%] [G loss: 2.525578]\n",
      "78 [D loss: 0.385174, acc.: 50.00%] [G loss: 2.250350]\n",
      "79 [D loss: 0.467812, acc.: 43.75%] [G loss: 2.037252]\n",
      "80 [D loss: 0.578410, acc.: 40.62%] [G loss: 2.093526]\n",
      "81 [D loss: 0.693750, acc.: 40.62%] [G loss: 1.959370]\n",
      "82 [D loss: 0.642882, acc.: 43.75%] [G loss: 2.100397]\n",
      "83 [D loss: 0.558622, acc.: 37.50%] [G loss: 1.642181]\n",
      "84 [D loss: 0.385936, acc.: 50.00%] [G loss: 2.499119]\n",
      "85 [D loss: 0.342127, acc.: 40.62%] [G loss: 2.367819]\n",
      "86 [D loss: 0.649723, acc.: 34.38%] [G loss: 1.440241]\n",
      "87 [D loss: 0.590287, acc.: 34.38%] [G loss: 1.752660]\n",
      "88 [D loss: 0.657706, acc.: 31.25%] [G loss: 0.990306]\n",
      "89 [D loss: 0.509283, acc.: 40.62%] [G loss: 1.010546]\n",
      "90 [D loss: 0.736400, acc.: 18.75%] [G loss: 1.558277]\n",
      "91 [D loss: 0.613606, acc.: 43.75%] [G loss: 1.830753]\n",
      "92 [D loss: 0.752346, acc.: 34.38%] [G loss: 1.158840]\n",
      "93 [D loss: 0.564761, acc.: 31.25%] [G loss: 1.501214]\n",
      "94 [D loss: 0.638861, acc.: 40.62%] [G loss: 1.343131]\n",
      "95 [D loss: 0.541223, acc.: 40.62%] [G loss: 1.599337]\n",
      "96 [D loss: 0.652140, acc.: 21.88%] [G loss: 1.851535]\n",
      "97 [D loss: 0.482229, acc.: 43.75%] [G loss: 1.984301]\n",
      "98 [D loss: 0.596674, acc.: 37.50%] [G loss: 1.904888]\n",
      "99 [D loss: 0.714869, acc.: 40.62%] [G loss: 1.877117]\n",
      "100 [D loss: 0.494071, acc.: 46.88%] [G loss: 1.636418]\n",
      "101 [D loss: 0.890004, acc.: 37.50%] [G loss: 2.329028]\n",
      "102 [D loss: 0.606190, acc.: 34.38%] [G loss: 2.480971]\n",
      "103 [D loss: 0.489734, acc.: 43.75%] [G loss: 2.753913]\n",
      "104 [D loss: 0.464667, acc.: 46.88%] [G loss: 2.375468]\n",
      "105 [D loss: 0.442565, acc.: 50.00%] [G loss: 1.570403]\n",
      "106 [D loss: 0.791121, acc.: 28.12%] [G loss: 1.338880]\n",
      "107 [D loss: 0.487333, acc.: 40.62%] [G loss: 1.770864]\n",
      "108 [D loss: 0.421206, acc.: 43.75%] [G loss: 2.007439]\n",
      "109 [D loss: 0.579307, acc.: 37.50%] [G loss: 2.695931]\n",
      "110 [D loss: 0.722446, acc.: 21.88%] [G loss: 2.414918]\n",
      "111 [D loss: 0.634040, acc.: 46.88%] [G loss: 2.057761]\n",
      "112 [D loss: 0.595088, acc.: 40.62%] [G loss: 2.226800]\n",
      "113 [D loss: 0.794871, acc.: 37.50%] [G loss: 1.826604]\n",
      "114 [D loss: 0.516309, acc.: 34.38%] [G loss: 1.507797]\n",
      "115 [D loss: 0.560688, acc.: 37.50%] [G loss: 1.877541]\n",
      "116 [D loss: 0.514918, acc.: 40.62%] [G loss: 2.650748]\n",
      "117 [D loss: 0.336133, acc.: 43.75%] [G loss: 3.055414]\n",
      "118 [D loss: 0.641778, acc.: 40.62%] [G loss: 2.391549]\n",
      "119 [D loss: 0.422808, acc.: 43.75%] [G loss: 2.540091]\n",
      "120 [D loss: 0.579143, acc.: 40.62%] [G loss: 1.872620]\n",
      "121 [D loss: 0.642759, acc.: 34.38%] [G loss: 1.948246]\n",
      "122 [D loss: 0.530927, acc.: 40.62%] [G loss: 1.954753]\n",
      "123 [D loss: 0.563452, acc.: 43.75%] [G loss: 2.243997]\n",
      "124 [D loss: 0.433978, acc.: 50.00%] [G loss: 1.878892]\n",
      "125 [D loss: 0.536617, acc.: 40.62%] [G loss: 2.086979]\n",
      "126 [D loss: 0.436852, acc.: 50.00%] [G loss: 1.928171]\n",
      "127 [D loss: 0.465657, acc.: 43.75%] [G loss: 2.255210]\n",
      "128 [D loss: 0.395700, acc.: 46.88%] [G loss: 2.144685]\n",
      "129 [D loss: 0.514519, acc.: 46.88%] [G loss: 2.274730]\n",
      "130 [D loss: 0.386087, acc.: 46.88%] [G loss: 2.068215]\n",
      "131 [D loss: 0.368888, acc.: 46.88%] [G loss: 1.685800]\n",
      "132 [D loss: 0.320441, acc.: 46.88%] [G loss: 2.050602]\n",
      "133 [D loss: 0.419535, acc.: 46.88%] [G loss: 2.233529]\n",
      "134 [D loss: 0.684208, acc.: 40.62%] [G loss: 1.836028]\n",
      "135 [D loss: 0.586332, acc.: 31.25%] [G loss: 2.294413]\n",
      "136 [D loss: 0.505987, acc.: 43.75%] [G loss: 1.643988]\n",
      "137 [D loss: 0.492472, acc.: 37.50%] [G loss: 1.348948]\n",
      "138 [D loss: 0.519738, acc.: 37.50%] [G loss: 2.191834]\n",
      "139 [D loss: 0.588055, acc.: 37.50%] [G loss: 1.754024]\n",
      "140 [D loss: 0.505350, acc.: 40.62%] [G loss: 2.134608]\n",
      "141 [D loss: 0.592814, acc.: 34.38%] [G loss: 3.045207]\n",
      "142 [D loss: 0.663669, acc.: 40.62%] [G loss: 1.388499]\n",
      "143 [D loss: 0.522178, acc.: 37.50%] [G loss: 2.187326]\n",
      "144 [D loss: 0.529945, acc.: 34.38%] [G loss: 1.842346]\n",
      "145 [D loss: 0.519042, acc.: 46.88%] [G loss: 1.764258]\n",
      "146 [D loss: 0.591434, acc.: 43.75%] [G loss: 1.820664]\n",
      "147 [D loss: 0.591415, acc.: 43.75%] [G loss: 1.974599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 [D loss: 0.531155, acc.: 43.75%] [G loss: 1.850848]\n",
      "149 [D loss: 0.400140, acc.: 46.88%] [G loss: 1.906374]\n",
      "150 [D loss: 0.473799, acc.: 43.75%] [G loss: 2.105398]\n",
      "151 [D loss: 0.487664, acc.: 40.62%] [G loss: 2.304368]\n",
      "152 [D loss: 0.608333, acc.: 43.75%] [G loss: 1.811450]\n",
      "153 [D loss: 0.504007, acc.: 43.75%] [G loss: 1.896896]\n",
      "154 [D loss: 0.504671, acc.: 43.75%] [G loss: 2.245822]\n",
      "155 [D loss: 0.444461, acc.: 43.75%] [G loss: 2.371263]\n",
      "156 [D loss: 0.608195, acc.: 37.50%] [G loss: 2.330111]\n",
      "157 [D loss: 0.485199, acc.: 43.75%] [G loss: 2.300364]\n",
      "158 [D loss: 0.699948, acc.: 31.25%] [G loss: 2.467576]\n",
      "159 [D loss: 0.510742, acc.: 46.88%] [G loss: 2.430659]\n",
      "160 [D loss: 0.464950, acc.: 37.50%] [G loss: 2.052294]\n",
      "161 [D loss: 0.520120, acc.: 43.75%] [G loss: 2.203372]\n",
      "162 [D loss: 0.500091, acc.: 43.75%] [G loss: 2.206497]\n",
      "163 [D loss: 0.556682, acc.: 46.88%] [G loss: 1.929286]\n",
      "164 [D loss: 0.447033, acc.: 40.62%] [G loss: 2.194455]\n",
      "165 [D loss: 0.603259, acc.: 46.88%] [G loss: 2.006396]\n",
      "166 [D loss: 0.448922, acc.: 46.88%] [G loss: 1.939286]\n",
      "167 [D loss: 0.392860, acc.: 50.00%] [G loss: 2.334236]\n",
      "168 [D loss: 0.562705, acc.: 46.88%] [G loss: 2.079334]\n",
      "169 [D loss: 0.626191, acc.: 34.38%] [G loss: 2.054770]\n",
      "170 [D loss: 0.594877, acc.: 43.75%] [G loss: 1.937358]\n",
      "171 [D loss: 0.648781, acc.: 40.62%] [G loss: 1.747818]\n",
      "172 [D loss: 0.483627, acc.: 46.88%] [G loss: 1.675728]\n",
      "173 [D loss: 0.441917, acc.: 50.00%] [G loss: 2.229298]\n",
      "174 [D loss: 0.510904, acc.: 46.88%] [G loss: 1.842575]\n",
      "175 [D loss: 0.394508, acc.: 43.75%] [G loss: 2.072391]\n",
      "176 [D loss: 0.495799, acc.: 46.88%] [G loss: 2.096807]\n",
      "177 [D loss: 0.389944, acc.: 50.00%] [G loss: 2.568729]\n",
      "178 [D loss: 0.415288, acc.: 43.75%] [G loss: 2.047518]\n",
      "179 [D loss: 0.408664, acc.: 40.62%] [G loss: 2.326161]\n",
      "180 [D loss: 0.327802, acc.: 50.00%] [G loss: 2.251553]\n",
      "181 [D loss: 0.408098, acc.: 46.88%] [G loss: 2.103353]\n",
      "182 [D loss: 0.474752, acc.: 43.75%] [G loss: 1.936503]\n",
      "183 [D loss: 0.402420, acc.: 46.88%] [G loss: 2.615636]\n",
      "184 [D loss: 0.382688, acc.: 50.00%] [G loss: 1.930132]\n",
      "185 [D loss: 0.339916, acc.: 46.88%] [G loss: 2.175325]\n",
      "186 [D loss: 0.362558, acc.: 46.88%] [G loss: 1.699627]\n",
      "187 [D loss: 0.478124, acc.: 46.88%] [G loss: 1.511493]\n",
      "188 [D loss: 0.421494, acc.: 43.75%] [G loss: 1.597376]\n",
      "189 [D loss: 0.439446, acc.: 40.62%] [G loss: 2.432351]\n",
      "190 [D loss: 0.605146, acc.: 46.88%] [G loss: 2.009601]\n",
      "191 [D loss: 0.579040, acc.: 43.75%] [G loss: 2.119607]\n",
      "192 [D loss: 0.556793, acc.: 50.00%] [G loss: 2.233577]\n",
      "193 [D loss: 0.367260, acc.: 50.00%] [G loss: 2.699538]\n",
      "194 [D loss: 0.332309, acc.: 50.00%] [G loss: 2.231817]\n",
      "195 [D loss: 0.443305, acc.: 46.88%] [G loss: 2.225299]\n",
      "196 [D loss: 0.511295, acc.: 43.75%] [G loss: 2.380214]\n",
      "197 [D loss: 0.362583, acc.: 46.88%] [G loss: 2.217237]\n",
      "198 [D loss: 0.461358, acc.: 46.88%] [G loss: 2.622725]\n",
      "199 [D loss: 0.377933, acc.: 50.00%] [G loss: 2.218875]\n",
      "200 [D loss: 0.504719, acc.: 43.75%] [G loss: 1.817625]\n",
      "201 [D loss: 0.411029, acc.: 46.88%] [G loss: 1.847990]\n",
      "202 [D loss: 0.635738, acc.: 46.88%] [G loss: 1.683891]\n",
      "203 [D loss: 0.409958, acc.: 46.88%] [G loss: 2.157236]\n",
      "204 [D loss: 0.486538, acc.: 46.88%] [G loss: 1.881782]\n",
      "205 [D loss: 0.421544, acc.: 46.88%] [G loss: 2.354958]\n",
      "206 [D loss: 0.504004, acc.: 43.75%] [G loss: 2.517563]\n",
      "207 [D loss: 0.316821, acc.: 43.75%] [G loss: 2.508438]\n",
      "208 [D loss: 0.425842, acc.: 50.00%] [G loss: 2.036461]\n",
      "209 [D loss: 0.638552, acc.: 31.25%] [G loss: 2.084911]\n",
      "210 [D loss: 0.390031, acc.: 50.00%] [G loss: 2.530125]\n",
      "211 [D loss: 0.480467, acc.: 40.62%] [G loss: 2.069870]\n",
      "212 [D loss: 0.392240, acc.: 50.00%] [G loss: 1.937988]\n",
      "213 [D loss: 0.494369, acc.: 43.75%] [G loss: 2.016993]\n",
      "214 [D loss: 0.752537, acc.: 40.62%] [G loss: 1.857676]\n",
      "215 [D loss: 0.312809, acc.: 50.00%] [G loss: 2.108313]\n",
      "216 [D loss: 0.394911, acc.: 46.88%] [G loss: 2.150471]\n",
      "217 [D loss: 0.568296, acc.: 34.38%] [G loss: 1.757386]\n",
      "218 [D loss: 0.470981, acc.: 50.00%] [G loss: 2.077574]\n",
      "219 [D loss: 0.685918, acc.: 34.38%] [G loss: 2.005975]\n",
      "220 [D loss: 0.552613, acc.: 31.25%] [G loss: 1.755320]\n",
      "221 [D loss: 0.389260, acc.: 50.00%] [G loss: 1.685509]\n",
      "222 [D loss: 0.323961, acc.: 50.00%] [G loss: 2.137675]\n",
      "223 [D loss: 0.731634, acc.: 37.50%] [G loss: 1.859192]\n",
      "224 [D loss: 0.493416, acc.: 46.88%] [G loss: 1.755034]\n",
      "225 [D loss: 0.464544, acc.: 37.50%] [G loss: 1.637393]\n",
      "226 [D loss: 0.354540, acc.: 46.88%] [G loss: 2.028496]\n",
      "227 [D loss: 0.326633, acc.: 50.00%] [G loss: 2.273899]\n",
      "228 [D loss: 0.478001, acc.: 43.75%] [G loss: 2.377576]\n",
      "229 [D loss: 0.364482, acc.: 50.00%] [G loss: 2.199517]\n",
      "230 [D loss: 0.255423, acc.: 50.00%] [G loss: 2.129052]\n",
      "231 [D loss: 0.337133, acc.: 50.00%] [G loss: 2.075948]\n",
      "232 [D loss: 0.488007, acc.: 50.00%] [G loss: 2.422619]\n",
      "233 [D loss: 0.372089, acc.: 46.88%] [G loss: 2.559379]\n",
      "234 [D loss: 0.658424, acc.: 40.62%] [G loss: 1.856375]\n",
      "235 [D loss: 0.459115, acc.: 43.75%] [G loss: 1.841994]\n",
      "236 [D loss: 0.430573, acc.: 40.62%] [G loss: 1.460035]\n",
      "237 [D loss: 0.380929, acc.: 46.88%] [G loss: 1.823871]\n",
      "238 [D loss: 0.530398, acc.: 37.50%] [G loss: 1.665087]\n",
      "239 [D loss: 0.456867, acc.: 37.50%] [G loss: 2.312512]\n",
      "240 [D loss: 0.412803, acc.: 46.88%] [G loss: 2.053825]\n",
      "241 [D loss: 0.400256, acc.: 50.00%] [G loss: 2.371769]\n",
      "242 [D loss: 0.484823, acc.: 46.88%] [G loss: 2.056581]\n",
      "243 [D loss: 0.404605, acc.: 46.88%] [G loss: 1.990331]\n",
      "244 [D loss: 0.476336, acc.: 43.75%] [G loss: 2.356815]\n",
      "245 [D loss: 0.498078, acc.: 37.50%] [G loss: 3.408011]\n",
      "246 [D loss: 0.562437, acc.: 50.00%] [G loss: 2.023432]\n",
      "247 [D loss: 0.578779, acc.: 43.75%] [G loss: 2.661268]\n",
      "248 [D loss: 0.283572, acc.: 50.00%] [G loss: 2.598678]\n",
      "249 [D loss: 0.328199, acc.: 46.88%] [G loss: 2.388447]\n",
      "250 [D loss: 0.779286, acc.: 37.50%] [G loss: 1.745707]\n",
      "251 [D loss: 0.672286, acc.: 25.00%] [G loss: 1.679697]\n",
      "252 [D loss: 0.467964, acc.: 46.88%] [G loss: 1.693820]\n",
      "253 [D loss: 0.445468, acc.: 34.38%] [G loss: 1.938660]\n",
      "254 [D loss: 0.429822, acc.: 37.50%] [G loss: 1.878556]\n",
      "255 [D loss: 0.515474, acc.: 43.75%] [G loss: 2.140029]\n",
      "256 [D loss: 0.383439, acc.: 50.00%] [G loss: 2.154119]\n",
      "257 [D loss: 0.580905, acc.: 34.38%] [G loss: 2.224243]\n",
      "258 [D loss: 0.416685, acc.: 50.00%] [G loss: 2.514278]\n",
      "259 [D loss: 0.516251, acc.: 50.00%] [G loss: 2.332057]\n",
      "260 [D loss: 0.471386, acc.: 46.88%] [G loss: 1.758520]\n",
      "261 [D loss: 0.429040, acc.: 46.88%] [G loss: 2.124587]\n",
      "262 [D loss: 0.347037, acc.: 43.75%] [G loss: 2.094505]\n",
      "263 [D loss: 0.556813, acc.: 34.38%] [G loss: 2.320310]\n",
      "264 [D loss: 0.453498, acc.: 46.88%] [G loss: 2.295504]\n",
      "265 [D loss: 0.437900, acc.: 46.88%] [G loss: 1.911402]\n",
      "266 [D loss: 0.438789, acc.: 46.88%] [G loss: 1.773194]\n",
      "267 [D loss: 0.301336, acc.: 46.88%] [G loss: 1.667520]\n",
      "268 [D loss: 0.364526, acc.: 50.00%] [G loss: 2.057822]\n",
      "269 [D loss: 0.660972, acc.: 40.62%] [G loss: 1.747660]\n",
      "270 [D loss: 0.332127, acc.: 50.00%] [G loss: 2.220922]\n",
      "271 [D loss: 0.408535, acc.: 43.75%] [G loss: 2.544838]\n",
      "272 [D loss: 0.455532, acc.: 40.62%] [G loss: 2.082178]\n",
      "273 [D loss: 0.380243, acc.: 50.00%] [G loss: 2.837165]\n",
      "274 [D loss: 0.451026, acc.: 43.75%] [G loss: 2.329919]\n",
      "275 [D loss: 0.391750, acc.: 43.75%] [G loss: 2.705928]\n",
      "276 [D loss: 0.504964, acc.: 43.75%] [G loss: 2.430854]\n",
      "277 [D loss: 0.372365, acc.: 40.62%] [G loss: 2.197107]\n",
      "278 [D loss: 0.443181, acc.: 40.62%] [G loss: 2.682524]\n",
      "279 [D loss: 0.435374, acc.: 46.88%] [G loss: 2.272794]\n",
      "280 [D loss: 0.315435, acc.: 46.88%] [G loss: 2.298783]\n",
      "281 [D loss: 0.578214, acc.: 37.50%] [G loss: 1.751127]\n",
      "282 [D loss: 0.438433, acc.: 43.75%] [G loss: 2.246043]\n",
      "283 [D loss: 0.794164, acc.: 21.88%] [G loss: 2.336842]\n",
      "284 [D loss: 0.463746, acc.: 46.88%] [G loss: 2.065674]\n",
      "285 [D loss: 0.486818, acc.: 46.88%] [G loss: 2.029074]\n",
      "286 [D loss: 0.455774, acc.: 50.00%] [G loss: 2.566995]\n",
      "287 [D loss: 0.681209, acc.: 37.50%] [G loss: 2.592806]\n",
      "288 [D loss: 0.591970, acc.: 40.62%] [G loss: 1.951807]\n",
      "289 [D loss: 0.626814, acc.: 28.12%] [G loss: 2.415519]\n",
      "290 [D loss: 0.365935, acc.: 50.00%] [G loss: 2.572388]\n",
      "291 [D loss: 0.487454, acc.: 40.62%] [G loss: 1.847438]\n",
      "292 [D loss: 0.509466, acc.: 40.62%] [G loss: 1.897292]\n",
      "293 [D loss: 0.573087, acc.: 40.62%] [G loss: 1.633381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 [D loss: 0.371070, acc.: 46.88%] [G loss: 2.332871]\n",
      "295 [D loss: 0.715445, acc.: 25.00%] [G loss: 1.985316]\n",
      "296 [D loss: 0.502620, acc.: 40.62%] [G loss: 1.633167]\n",
      "297 [D loss: 0.463156, acc.: 37.50%] [G loss: 2.012724]\n",
      "298 [D loss: 0.436931, acc.: 37.50%] [G loss: 2.430060]\n",
      "299 [D loss: 0.504942, acc.: 40.62%] [G loss: 2.064031]\n",
      "300 [D loss: 0.508203, acc.: 40.62%] [G loss: 1.871064]\n",
      "301 [D loss: 0.519068, acc.: 50.00%] [G loss: 2.315828]\n",
      "302 [D loss: 0.714489, acc.: 28.12%] [G loss: 1.869509]\n",
      "303 [D loss: 0.472985, acc.: 37.50%] [G loss: 2.510338]\n",
      "304 [D loss: 0.605399, acc.: 40.62%] [G loss: 1.738403]\n",
      "305 [D loss: 0.416470, acc.: 46.88%] [G loss: 1.930120]\n",
      "306 [D loss: 0.340962, acc.: 50.00%] [G loss: 2.450740]\n",
      "307 [D loss: 0.509267, acc.: 46.88%] [G loss: 2.227955]\n",
      "308 [D loss: 0.758886, acc.: 34.38%] [G loss: 1.575459]\n",
      "309 [D loss: 0.540586, acc.: 40.62%] [G loss: 2.171665]\n",
      "310 [D loss: 0.518966, acc.: 37.50%] [G loss: 1.651529]\n",
      "311 [D loss: 0.421998, acc.: 46.88%] [G loss: 1.571061]\n",
      "312 [D loss: 0.463724, acc.: 37.50%] [G loss: 1.780842]\n",
      "313 [D loss: 0.473757, acc.: 46.88%] [G loss: 2.040071]\n",
      "314 [D loss: 0.437547, acc.: 37.50%] [G loss: 1.916807]\n",
      "315 [D loss: 0.354584, acc.: 46.88%] [G loss: 1.706238]\n",
      "316 [D loss: 0.486786, acc.: 43.75%] [G loss: 1.690375]\n",
      "317 [D loss: 0.798782, acc.: 37.50%] [G loss: 1.322312]\n",
      "318 [D loss: 0.347407, acc.: 37.50%] [G loss: 1.617894]\n",
      "319 [D loss: 0.498970, acc.: 37.50%] [G loss: 1.822032]\n",
      "320 [D loss: 0.480496, acc.: 46.88%] [G loss: 1.780793]\n",
      "321 [D loss: 0.762891, acc.: 37.50%] [G loss: 1.927112]\n",
      "322 [D loss: 0.482609, acc.: 43.75%] [G loss: 2.025040]\n",
      "323 [D loss: 0.467411, acc.: 43.75%] [G loss: 2.046700]\n",
      "324 [D loss: 0.391219, acc.: 46.88%] [G loss: 1.459051]\n",
      "325 [D loss: 0.724114, acc.: 37.50%] [G loss: 1.818829]\n",
      "326 [D loss: 0.787138, acc.: 34.38%] [G loss: 1.523279]\n",
      "327 [D loss: 0.428901, acc.: 37.50%] [G loss: 2.489022]\n",
      "328 [D loss: 0.329875, acc.: 50.00%] [G loss: 2.170198]\n",
      "329 [D loss: 0.599120, acc.: 43.75%] [G loss: 1.764190]\n",
      "330 [D loss: 0.422762, acc.: 40.62%] [G loss: 1.720856]\n",
      "331 [D loss: 0.400567, acc.: 37.50%] [G loss: 2.098581]\n",
      "332 [D loss: 0.342433, acc.: 50.00%] [G loss: 2.416447]\n",
      "333 [D loss: 0.374201, acc.: 50.00%] [G loss: 2.380368]\n",
      "334 [D loss: 0.520496, acc.: 37.50%] [G loss: 2.160771]\n",
      "335 [D loss: 0.338727, acc.: 50.00%] [G loss: 1.729101]\n",
      "336 [D loss: 0.524828, acc.: 43.75%] [G loss: 2.232125]\n",
      "337 [D loss: 0.533873, acc.: 37.50%] [G loss: 1.691766]\n",
      "338 [D loss: 0.459606, acc.: 37.50%] [G loss: 1.645433]\n",
      "339 [D loss: 0.690756, acc.: 37.50%] [G loss: 1.751526]\n",
      "340 [D loss: 0.353070, acc.: 46.88%] [G loss: 2.099634]\n",
      "341 [D loss: 0.265629, acc.: 50.00%] [G loss: 1.551381]\n",
      "342 [D loss: 0.382653, acc.: 46.88%] [G loss: 1.224724]\n",
      "343 [D loss: 0.746116, acc.: 37.50%] [G loss: 1.786659]\n",
      "344 [D loss: 0.352394, acc.: 50.00%] [G loss: 1.526480]\n",
      "345 [D loss: 0.479126, acc.: 37.50%] [G loss: 1.664261]\n",
      "346 [D loss: 0.405652, acc.: 50.00%] [G loss: 1.303871]\n",
      "347 [D loss: 0.473069, acc.: 50.00%] [G loss: 1.615009]\n",
      "348 [D loss: 0.528803, acc.: 43.75%] [G loss: 1.344716]\n",
      "349 [D loss: 0.650712, acc.: 21.88%] [G loss: 2.221148]\n",
      "350 [D loss: 0.411073, acc.: 43.75%] [G loss: 1.901183]\n",
      "351 [D loss: 0.534745, acc.: 43.75%] [G loss: 2.025962]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-a2fbe2e5c0c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cifar10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mnist_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#generator.save('cifar10_model.h5')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-e3753bebc31c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset_title, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# train the discriminator by feeding both real and fake (generated) images one by one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhalf_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\capsule-gans\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train('cifar10', epochs=30000, batch_size=32, save_interval=50)\n",
    "generator.save('mnist_model.h5')\n",
    "#generator.save('cifar10_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eb2819710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(D_L)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Discriminator Loss (blue), Discriminator Accuracy (orange)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eb286f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(G_L)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Generator Loss (blue)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
